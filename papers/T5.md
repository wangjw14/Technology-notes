# T5



- T5使用了简化的相对位置embeding，即每个位置对应一个数值而不是向量，将相对位置的数值加在attention softmax之前的logits上，每个head的有自己的PE，所有的层共享一套PE。个人认为这种方式更好一点，直接在计算attention weight的时候加入位置信息，而且每一层都加一次，让模型对位置更加敏感。



- [T5 模型：NLP Text-to-Text 预训练模型超大规模探索](https://zhuanlan.zhihu.com/p/88438851) 逻辑很清晰
- [Google T5速读 by 李rumor](https://zhuanlan.zhihu.com/p/88363572)
- 

