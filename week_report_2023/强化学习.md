# 强化学习











## 一、强化学习基础概念

- 强化学习一般包含三个要素：Actor、Env 和 Reward Function

![RL_basic_conpect](pics/RL_basic_conpect.png)

- Actor
  - Policy $\pi$ 一般是指参数为 $\theta$ 的网络，输入是Env的一个state，输出是一个action的分布。
  - 比如，对于一个游戏而言，输入是显示器的画面，输出是要采取的行动，如“左移”、“右移”等。
  - 这几个名称是等价的。Actor $=$ policy $=\pi=$ network $=$ function
- Env
  - 一般是游戏的规则等。

- Actor 和 Env 之间的互动中，一条trajectory的概率

![RL-process](pics/RL-process.jpeg)

- Reward
  - 一条trajectory 的 total reward：$R(\tau)=\sum_{t=1}^T r_t$ 
  - 穷举所有的 trajectory，得到reward的期望值

$$
\bar{R}_\theta=\sum_\tau R(\tau) p_\theta(\tau)=E_{\tau \sim p_\theta(\tau)}[R(\tau)] \\\quad
$$



## 二、Policy Gradient

- 为了求的 $\bar{R}_\theta$ 的最大值，需要对其进行求导，然后进行梯度上升求解。

$$
\begin{aligned}
\nabla \bar{R}_\theta &=\sum_\tau R(\tau) \nabla p_\theta(\tau)=\sum_\tau R(\tau) p_\theta(\tau) \frac{\nabla p_\theta(\tau)}{p_\theta(\tau)} \\

& =\sum_\tau R(\tau) p_\theta(\tau) \nabla \log p_\theta(\tau) \\ 
& =E_{\tau \sim p_\theta(\tau)}\left[R(\tau) \nabla \log p_\theta(\tau)\right] \\

& \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R\left(\tau^n\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)

\end{aligned}
$$

​		其中 $R(\tau)$ 不一定需要可微，可以是一个黑盒。第一行到第二行，用到了公式：$ \nabla f(x)= f(x) \nabla \log f(x)$

- 得到梯度之后，一次迭代过程如下：首先使用 policy $\pi _\theta$ ，采样很多的 trajectory；然后使用这些数据，更新参数 $\theta$ ，之后再重新采用数据，不断进行迭代。

  <img src="pics/policy_gradient.png" alt="policy_gradient" style="zoom:30%;" />

- RL 和一般的supervised learning的区别在于，一般的训练，模型参数更新之后，不会影响数据的标签。而在RL中，模型参数更新之后，Actor 和 Env互动的结果会随之改变，从而影响 label（或者 reward），使得每次更新参数之后，需要重新对数据进行采样。这样也训练成本很高。

- Policy Gradient的改进

  1. 增加一个baseline
     $$
     \nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}\left(R\left(\tau^n\right)-{b}\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right) \\\quad b \approx E[R(\tau)]
     $$
     有些场景下reward可能一直是正数，且采样有一定的随机性，导致优化效果不太好。增加一个基准b，使得reward可以有正有负，更容易优化。

  2. 对reward进行加权
     $$
     \nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}\left(\sum_{t^{\prime}=t}^{T_n} \gamma^{t^{\prime}-t} r_{t^{\prime}}^n-{b}\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)
     $$
     reward从当前时刻t开始计算，t之前的reward可当前时刻t的action并没有关系。同时，增加了时间衰减系数，使得间隔较远的reward影响较小。

## 三、On-policy vs Off-policy

- 定义
  - On-policy：和环境交互的agent和要学习的agent是一个agent。
  - Off-policy：和环境交互的agent和要学习的agent不是一个agent。
- 前面的 policy gradient 的做法是 on-policy。当update参数以后，从 $\theta$ 变成 $\theta^{\prime}$，那么之前采样的数据就不能用了。Update一次参数，只能做一次gradient acent，而后再去 collect data，非常耗时，所以需要 off-policy。off-policy可以拿一批数据用好几次。

- 重要性采样

  - 假设 $x$ 服从 $p$ 分布，想要计算 $f(x)$ 的期望，但是 $p$ 分布很难计算（采样）。于是引入一个比较容易计算（采样）的 $q$ 分布，于是在分子分母同时乘以 $q(x)$，于是就变成了服从 $q$ 分布的 $x$ 来求期望。
    $$
    E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]
    $$

  - 根据上式可以知道，通过分布变换之后，期望值是相同的。但方差是否一致呢？根据公式 $\begin{aligned} & \operatorname{VAR}[X]   =E\left[X^2\right]-(E[X])^2\end{aligned}$ 分别计算两者的方差如下：
    $$
    \begin{aligned}  \operatorname{Var}_{x \sim p}[f(x)]& =E_{x \sim p}\left[f(x)^2\right]-\left(E_{x \sim p}[f(x)]\right)^2 \\ \operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right] & =E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^2\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^2 \\ & =E_{x \sim p}\left[f(x)^2 \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^2\end{aligned}
    $$
    可以看到，第二项是一致，区别主要在第一项，也就是 $\frac{p(x)}{q(x)}$ 。要使得在采样不充分的情况下，不出现较大的误差，则需要要求 ${p(x)}$ 和 $ {q(x)}$ 是两个比较接近的分布。

- 从On-policy到Off-policy
  $$
  \begin{aligned} 
  \text{On-policy:} \qquad & \nabla \bar{R}_\theta=E_{{\tau \sim p_\theta(\tau)}}\left[R(\tau) \nabla \log p_\theta(\tau)\right]  \\
  \text{Off-policy:}\qquad &\nabla \bar{R}_\theta=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\frac{p_\theta(\tau)}{p_{\theta^{\prime}}(\tau)} R(\tau) \nabla \log p_\theta(\tau)\right]
  \end{aligned}
  $$
  其中， $\theta^{\prime}$ 是固定的，从中采样一次数据, 可以给 $\theta$ 更新很多次，完了以后再去采样数据。

- Off-policy的梯度更新
  $$
  \begin{aligned}  \nabla \bar{R}_\theta& =E_{\left(s_t, a_t\right) \sim \pi_\theta}\left[A^\theta\left(s_t, a_t\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)\right] \\ & =E_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{P_\theta\left(s_t, a_t\right)}{P_{\theta^{\prime}}\left(s_t, a_t\right)} A^\theta\left(s_t, a_t\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)\right]\end{aligned}
  $$
  其中 $A^\theta\left(s_t, a_t\right)$ 代表reward，等价于 $R(\tau)$。经过推导，最终得到优化目标
  $$
  J^{\theta^{\prime}}(\theta)=E_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right)\right]
  $$

## 四、PPO/TRPO

- 在重要性采样中，提到替换的分布和替换之前的分布不能差别太大，因此需要增加一个限制条件，保证 $\theta$ 和 $\theta^\prime$  是比较接近的。衡量两个分布是否接近的指标，就是KL散度。

- TRPO

  - TRPO 在损失函数之外，增加了一个限制条件，保证两个分布的相似性

  $$
  J_{T R P O}^{\theta^{\prime}}(\theta)=E_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right)\right]  \\
  K L\left(\theta, \theta^{\prime}\right)<\delta
  $$

- PPO

  - TRPO可以限制两个分布的相似性，但是因为限制条件是单独存在，不好求解。因此，PPO对其进行了改进，变成了损失函数的一部分。
    $$
    J_{P P O}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta K L\left(\theta, \theta^{\prime}\right)
    $$

  - PPO的整体流程

    ![ppo](pics/ppo.png)

  - PPO2：针对不同大小的KL散度，自动调节 $\beta$ 值的大小。
    $$
    \begin{aligned} J_{P P O 2}^{\theta^k}(\theta) \approx & \sum_{\left(s_t, a_t\right)} \min \left(\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^k}\left(a_t \mid s_t\right)} A^{\theta^k}\left(s_t, a_t\right)\right. \\ & \left.\operatorname{clip}\left(\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^k}\left(a_t \mid s_t\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^k}\left(s_t, a_t\right)\right)\end{aligned}
    $$
    

## 五、Q-Learning

- Q-learning是一种value based方法，此时学习的并不是policy，而是一个critic

- Critic
  
  - critic并不会直接决定当前状态下要采取的action，对于给定的policy $\pi$ ，critic可以给出这个策略的分数。
  - 状态价值函数 $V^\pi(s)$ ：使用policy $\pi$ 时，在状态 $s$ 之后，可以累积获得的reward的期望
  -  $V^\pi(s)$ 的值，和policy $\pi$ 有很大的关系，不同的 $\pi$ ，在共同的状态 $s$ 之下，可以有不同的值
  
-  $V^\pi(s)$ 的估计方法
  - 蒙特卡洛方法（MC）：从状态 $s$ 开始，一直到实验结束，不断重复这个过程。将所有过程的reward值，取均值，作为 $V^\pi(s)$ 的估计值。
  - 时间差分算法（TD）：根据状态 $s_t$ 和 $s_{t+1}$ 之间的差值，进行估计。$V^\pi\left(s_t\right)-V^\pi\left(s_{t+1}\right) \leftrightarrow r_t$ 
  - MC方差更大，但更精确。TD方差小，但没有MC精确。TD更常用。
  
- 另一种critic：状态 - 行为价值函数 $Q^\pi(s, a)$ 
  
- 使用policy $\pi$ 时，在状态 $s$ 并采取行动 $a$ 之后，可以累积获得的reward的期望
  - 只适用于离散的actor，连续的话没法使用
  
- Given $Q^\pi(s, a)$, find a new actor $\pi^{\prime}$ "better" than $\pi$
  - "Better": $V^{\pi^{\prime}}(s) \geq V_0^\pi(s)$, for all state $\mathrm{s}$

  $$
  \pi^{\prime}(s)=\arg \max _a Q^\pi(s, a)
  $$

  <img src="pics/q-learning.png" alt="q-learning" style="zoom:40%;" />

- 
- 



## 参考资料 

- https://blog.51cto.com/u_15721703/5575736
- 详解大模型RLHF过程（配代码解读） - 战士金的文章 - 知乎
  https://zhuanlan.zhihu.com/p/624589622
- 李弘毅深度强化学习笔记【1 Policy Gradient 】 - 残血的三井寿的文章 - 知乎
  https://zhuanlan.zhihu.com/p/66291401
- 【李弘毅深度强化学习】2，Proximal Policy Optimization (PPO) - 残血的三井寿的文章 - 知乎
  https://zhuanlan.zhihu.com/p/66302483





