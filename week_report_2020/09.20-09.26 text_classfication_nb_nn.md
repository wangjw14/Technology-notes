# 09.20-09.26 回顾

## 朴素贝叶斯

- MLE vs MAP 

  - Maximum Likelihood Estimation（最大似然估计）
    $$
    \begin{aligned}
    \hat{\theta}_{\mathrm{MLE}} &=\arg \max P(X ; \theta) \\
    &=\arg \max P\left(x_{1} ; \theta\right) P\left(x_{2} ; \theta\right) \cdots P\left(x_{n} ; \theta\right) \\
    &=\arg \max \log \prod_{i=1}^{n} P\left(x_{i} ; \theta\right) \\
    &=\arg \max \sum_{i=1}^{n} \log P\left(x_{i} ; \theta\right) \\
    &=\arg \min -\sum_{i=1}^{n} \log P\left(x_{i} ; \theta\right)
    \end{aligned}
    $$
    最后一行所优化的函数被称为Negative Log Likelihood（NLL）

    

  - Maximum A Posteriori（最大后验估计）
    $$
    \begin{aligned}
    \hat{\theta}_{\mathrm{MAP}} &=\arg \max P(\theta \mid X) \\
    &=\arg \min -\log P(\theta \mid X) \\
    &=\arg \min -\log P(X \mid \theta)-\log P(\theta)+\log P(X) \\
    &=\arg \min -\log P(X \mid \theta)-\log P(\theta)
    \end{aligned}
    $$
    其中，$-\log P(X \mid \theta)$ 其实就是NLL，所以MLE和MAP在优化时的不同就是在于先验项 $-\log P(\theta)$ 。假定先验是一个高斯分布，即
    $$
    P(\theta)=\text{constant}\times e^{-\frac{\theta^{2}}{2 \sigma^{2}}}
    $$
    那么， $-\log P(\theta)=\operatorname{constant}+\frac{\theta^{2}}{2 \sigma^{2}}$ 。至此，一件神奇的事情发生了—— 在MAP中使用一个高斯分布的先验等价于在MLE中采用L2的regularizaton！

    

  - 参考资料：聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计 - 夏飞的文章 - 知乎 https://zhuanlan.zhihu.com/p/32480810 

    

- **朴素贝叶斯 = 贝叶斯公式 + 条件独立假设 + 平滑(拉普拉斯平滑)** 

- 根据贝叶斯公式和条件独立假设，有如下公式：
  $$
  \begin{align}
  P\left(y \mid x_{1}, \ldots, x_{n}\right)&=\frac{P(y) P\left(x_{1}, \ldots, x_{n} \mid y\right)}{P\left(x_{1}, \ldots, x_{n}\right)} \\
  & = \frac{P(y) \prod_{i=1}^{n} P\left(x_{i} \mid y\right)}{P\left(x_{1}, \ldots, x_{n}\right)} \\
  
  & \propto P(y) \prod_{i=1}^{n} P\left(x_{i} \mid y\right) \\
  \end{align}
  $$
  从而有：
  $$
  \hat{y}=\arg \max _{y} P(y) \prod_{i=1}^{n} P\left(x_{i} \mid y\right)
  $$

- 处理重复词语的三种方式

  - 多项式模型：如果我们考虑重复词语的情况，也就是说，**重复的词语我们视为其出现多次**，出现n词的词语，概率也是n次方。
  - 伯努利模型：**将重复的词语都视为其只出现1次**  
  - 混合模型：训练时考虑重复，测试时不考虑重复

- 朴素贝叶斯的工程tricks：

  - 取对数：将概率相乘转化为log概率相加。对数操作在训练时完成，inference时直接查表即可。
  - 转换为权重：对于二分类，可以得到正类和负类概率比值的对数，判断最后值是否大于0
  - 取topk关键词：设置阈值，取关键词，更加速inference
  - 分割样本：对于不同篇幅的文本，得到的值不同。对于篇幅小的应该阈值也小。或者将长文本分割为固定长度的文本。
  - 位置权重：对标题、段首等位置的词加大权重
  - 蜜罐：获取垃圾邮件的方法

- 先验概率问题：

  - 当先验概率相同（各个类别的样本数相同）时，贝叶斯方法等价于最大似然法。
  - 在我们对于先验概率一无所知时，只能假设每种猜测的先验概率是均等的（其实这也是人类经验的结果），这个时候就只有用最大似然了
  - 只有当我们对于先验分布有比较大的把握时，先验概率才能更好发挥作用。否则，可以舍弃先验概率，使用最大似然。



## 语种分类

- 一个朴素贝叶斯分类代码（作为参考，实际运行还需要一些）

  ```python
  import re
  from sklearn.feature_extraction.text import CountVectorizer
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.model_selection import train_test_split
  from sklearn.naive_bayes import MultinomialNB
  from joblib import dump, load
  
  
  class LanguageDetector():
      # 成员函数
      def __init__(self, classifier=MultinomialNB()):
          self.classifier = classifier
          self.vectorizer = TfidfVectorizer(lowercase=True, 
  #                                           ngram_range=(1,3),
                                            decode_error='ignore', 
  #                                           analyzer="char_wb",
  #                                           max_features=1000,
  #                                           binary=True,
                                            preprocessor=self._remove_noise)
  
      # 私有函数，数据清洗
      def _remove_noise(self, document):
          noise_pattern = re.compile("|".join(["http\S+", "\@\w+", "\#\w+"]))
          clean_text = re.sub(noise_pattern, "", document)
          return clean_text
  
      # 特征构建
      def features(self, X):
          return self.vectorizer.transform(X)
  
      # 拟合数据
      def fit(self, X, y):
          self.vectorizer.fit(X)
          self.classifier.fit(self.features(X), y)
  
      # 预估类别
      def predict(self, x):
          return self.classifier.predict(self.features([x]))
  
      # 测试集评分
      def score(self, X, y):
          return self.classifier.score(self.features(X), y)
      
      # 模型持久化存储
      def save_model(self, path):
          dump((self.classifier, self.vectorizer), path)
      
      # 模型加载
      def load_model(self, path):
          self.classifier, self.vectorizer = load(path)
          
  in_f = open('data.csv')
  lines = in_f.readlines()
  in_f.close()
  dataset = [(line.strip()[:-3], line.strip()[-2:]) for line in lines]
  x, y = zip(*dataset)
  x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2019)
  
  language_detector = LanguageDetector()
  language_detector.fit(x_train, y_train)
  print(language_detector.predict('This is an English sentence'))
  print(language_detector.score(x_test, y_test))
  ```

  

| 特征提取器      | analyzer | ngram_range | max_features | binary | accuracy |
| --------------- | -------- | ----------- | ------------ | ------ | -------- |
| CountVectorizer | word     | (1, 1)      | None (22898) | False  | 0.99779  |
| TfidfVectorizer | word     | (1, 1)      | None (22898) | False  | 0.99779  |
|                 |          |             |              |        |          |
| CountVectorizer | word     | (1, 2)      | None(85810)  | False  | 0.99735  |
| TfidfVectorizer | word     | (1, 2)      | None(85810)  | False  | 0.99779  |
|                 |          |             |              |        |          |
| CountVectorizer | char_wb  | (1,1)       | None(59)     | False  | 0.87207  |
| CountVectorizer | char_wb  | (1,2)       | None(1438)   | False  | 0.97397  |
| CountVectorizer | char_wb  | (1,2)       | 1000         | False  | 0.97353  |
| CountVectorizer | char_wb  | (1,2)       | 1000         | True   | 0.97529  |
|                 |          |             |              |        |          |
| CountVectorizer | char_wb  | (1,3)       | 1000         | False  | 0.98808  |
| CountVectorizer | char_wb  | (1,3)       | None(11467)  | False  | 0.99029  |
| CountVectorizer | char_wb  | (1,3)       | None(11467)  | True   | 0.99294  |
|                 |          |             |              |        |          |
| TfidfVectorizer | char_wb  | (1,1)       | None(59)     | False  | 0.82223  |
| TfidfVectorizer | char_wb  | (1,2)       | None(1438)   | False  | 0.97397  |
| TfidfVectorizer | char_wb  | (1,2)       | 1000         | False  | 0.97397  |
| TfidfVectorizer | char_wb  | (1,2)       | 1000         | True   | 0.96779  |
|                 |          |             |              |        |          |
| TfidfVectorizer | char_wb  | (1,3)       | 1000         | False  | 0.99073  |
| TfidfVectorizer | char_wb  | (1,3)       | None(11467)  | False  | 0.99205  |
| TfidfVectorizer | char_wb  | (1,3)       | None(11467)  | True   | 0.99338  |
|                 |          |             |              |        |          |



综合上述的调参过程，可以有以下的观察：

- 同等参数条件下，TfidfVectorizer比CountVectorizer有更好的效果。
- 同等参数条件下，设置binary=True，有较大概率能得到一点点提升。
- ngram中，n取值越大，效果越好
- max_feature越大，效果越好
- 最优值目前可以达到0.99779，测试集中有5条样本分错了。



## 关键词提取

### 基于 TF-IDF 算法的关键词抽取

- jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())

  - sentence 为待提取的文本
  - topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20
  - withWeight 为是否一并返回关键词权重值，默认值为 False
  - allowPOS 仅包括指定词性的词，默认值为空，即不筛选

  ```python
  import jieba.analyse as analyse
  import pandas as pd
  df = pd.read_csv("./origin_data/technology_news.csv", encoding='utf-8')
  df = df.dropna()
  lines=df.content.values.tolist()
  content = "".join(lines)
  keywords = analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=())
  print(keywords)
  # ['用户', '2016', '互联网', '手机', '平台', '人工智能', '百度', '2017', '智能', '技术', '数据', '360', '服务', '直播', '产品', '企业', '安全', '视频', '移动', '应用', '网络', '行业', '游戏', '机器人', '电商', '内容', '中国', '领域', '通过', '发展']
  ```

  

### 基于 TextRank 算法的关键词抽取

- jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。
- jieba.analyse.TextRank() 新建自定义 TextRank 实例

- 算法论文： [TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)

- 基本思想:
  - 将待抽取关键词的文本进行分词
  - 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图
  - 计算图中节点的PageRank，注意是无向带权图

```python
import jieba.analyse as analyse
import pandas as pd
df = pd.read_csv("./origin_data/military_news.csv", encoding='utf-8')
df = df.dropna()
lines=df.content.values.tolist()
content = "".join(lines)

print("  ".join(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))
print("---------------------我是分割线----------------")
print("  ".join(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n'))))


# 中国  海军  训练  美国  部队  进行  官兵  航母  作战  任务  能力  军事  发展  工作  国家  问题  建设  导弹  编队  记者
# ---------------------我是分割线----------------
# 中国  海军  美国  部队  官兵  航母  军事  国家  任务  能力  导弹  技术  问题  日本  军队  编队  装备  系统  记者  战略
```



### LDA主题模型

- 首先我们要把文本内容处理成固定的格式，一个包含句子的list，list中每个元素是分词后的词list。类似下面这个样子。

  [[第，一，条，新闻，在，这里],[第，二，条，新闻，在，这里],[这，是，在，做， 什么],...]

- 代码

  ```python
  from gensim import corpora, models, similarities
  import gensim
  
  stopwords=pd.read_csv("origin_data/stopwords.txt",index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')
  stopwords=stopwords['stopword'].values
  
  import jieba
  import pandas as pd
  df = pd.read_csv("./origin_data/technology_news.csv", encoding='utf-8')
  df = df.dropna()
  lines=df.content.values.tolist()
  
  sentences=[]
  for line in lines:
      try:
          segs=jieba.lcut(line)
          segs = list(filter(lambda x:len(x)>1, segs))
          segs = list(filter(lambda x:x not in stopwords, segs))
          sentences.append(list(segs))
      except Exception as e:
          print(line)
          continue
          
          
  dictionary = corpora.Dictionary(sentences) # 根据现有数据生成词典
  corpus = [dictionary.doc2bow(sentence) for sentence in sentences] # 对每句话，统计每个词语的频数，组成词袋模型
  
  # lda训练 corpus=转化为词袋的词语列表 id2word=词典 num_topics=主题数量
  lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)
  
  print(lda.print_topic(3, topn=5))
  print(lda.print_topics(num_topics=20, num_words=4))
  
  text5 = ['徐立', '商汤', '科技', 'CEO', '谈起', '本次', '参展', '谈到', '成立', '刚刚', '两年', '创业', '公司', '参展', '展示', '最新', '人工智能', '技术', '产品', '表达', '人工智能', '理解', '人工智能', '特定', '领域', '超越', '人类', '广泛应用', '标志', 'Master', '胜利', '围棋', '世界', '开拓', '局面', '不谋而合']
  bow = dictionary.doc2bow(text5)
  ndarray = lda.inference([bow])[0]
  print(''.join(text5))
  for e, value in enumerate(ndarray[0]):
      print('主题%d推断值%.2f' % (e, value))
  
  print(lda.get_document_topics(bow))
  print(lda[bow])
  ```

  

## 分类模型的评价指标

- 多分类任务中的f1
  $$
  \begin{array}{llll}
  \hline
  \text { average } & \text { Precision } & \text { Recall } & \text { F_beta } \\
  \hline \text { micro } & P(y, \hat{y}) & R(y, \hat{y}) & F_{\beta}(y, \hat{y}) \\
  \hline \text { samples } & \frac{1}{|S|} \sum_{s \in S} P\left(y_{s}, \hat{y}_{s}\right) & \frac{1}{|S|} \sum_{s \in S} R\left(y_{s}, \hat{y}_{s}\right) & \frac{1}{|S|} \sum_{s \in S} F_{\beta}\left(y_{s}, \hat{y}_{s}\right) \\
  \hline \text { macro } & \frac{1}{|L|} \sum_{l \in L} P\left(y_{l}, \hat{y}_{l}\right) & \frac{1}{|L|} \sum_{l \in L} R\left(y_{l}, \hat{y}_{l}\right) & \frac{1}{|L|} \sum_{l \in L} F_{\beta}\left(y_{l}, \hat{y}_{l}\right) \\
  \hline \text { weighted } & \frac{1}{\sum_{l \in L}\left|\hat{y}_{l}\right|} \sum_{l \in L}\left|\hat{y}_{l}\right| P\left(y_{l}, \hat{y}_{l}\right) & \frac{1}{\sum_{l \in L}\left|\hat{y}_{l}\right|} \sum_{l \in L}\left|\hat{y}_{l}\right| R\left(y_{l}, \hat{y}_{l}\right) & \frac{1}{\sum_{l \in L}\left|\hat{y}_{l}\right|} \sum_{l \in L}\left|\hat{y}_{l}\right| F_{\beta}\left(y, \hat{y}_{l}\right) \\
  \hline \text { None } & \left\langle P\left(y_{l}, \hat{y}_{l}\right) \mid l \in L\right\rangle & \left\langle R\left(y_{l}, \hat{y}_{l}\right) \mid l \in L\right\rangle & \left\langle F_{\beta}\left(y_{l}, \hat{y}_{l}\right) \mid l \in L\right\rangle \\
  \hline
  \end{array}
  $$
  简单来说：

  - micro 分别计算每个类的TP、FN、FP，然后将每个类的TP、FN、FP相加得到总体样本的TP、FN、FP，然后使用总体样本的TP、FN、FP 来计算f1
  - macro分别计算每个类别的f1，然后直接取平均
  - weighted分别计算每个类别的f1，然后根据正确类别的数量，对f1进行加权平均

- Jaccard similarity
  $$
  J\left(y_{i}, \hat{y}_{i}\right)=\frac{\left|y_{i} \cap \hat{y}_{i}\right|}{\left|y_{i} \cup \hat{y}_{i}\right|}
  $$
  

-  Receiver operating characteristic (ROC)
  - A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.

- sklearn 中的API

  ```python
  from sklearn.metrics import f1_score
  from sklearn.metrics import precision_score
  from sklearn.metrics import recall_score
  from sklearn.metrics import jaccard_score
  from sklearn.metrics import roc_auc_score
  from sklearn.metrics import accuracy_score
  from sklearn.metrics import classification_report
  from sklearn.metrics import precision_recall_fscore_support
  from sklearn.metrics import confusion_matrix
  ...
  ```



- 参考链接
  - Macro-F1 Score与Micro-F1 Score - 田海山的文章 - 知乎 https://zhuanlan.zhihu.com/p/64315175
  - https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics



## CNN中的常见max pooling

- **Max Pooling Over Time**
  - 优点：特征的位置和旋转不变性、减少参数量、将变长的文本得到固定长度的feature、有一定的健壮性
  - 缺点：丢弃了位置信息、丢弃了特征出现的强度（次数）
- **K-Max Pooling**
- **Chunk-Max Pooling**

- 参考资料
  - https://blog.csdn.net/malefactor/article/details/51078135#0-tsina-1-38411-397232819ff9a47a7b7e80a40613cfe1



## textCNN的调参经验

- 词向量：
  - 无法确定用那种预训练词向量更好，不同的任务结果不同，应该对于你当前的任务进行实验；
  - 对于不在预训练的word2vec中的词，使用均匀分布U[−a,a]随机初始化，并且调整a使得随机初始化的词向量和预训练的词向量保持相近的方差，可以有微弱提升；
  - 微调策略（non-static）的效果比固定词向量（static）的效果要好。一个解释：预训练词向量可能认为‘good’和‘bad’类似（可能它们有许多类似的上下文），但是对于情感分析任务，good和bad应该要有明显的区分，如果使用CNN-static就无法做调整了；

- filter的选择：
  -  每次使用一种类型的filter进行实验，表明filter的窗口大小设置在1到10之间是一个比较合理的选择。
  - 首先在一种类型的filter大小上执行搜索，以找到当前数据集的"最佳"大小，然后探索这个最佳大小附近的多种filter大小的组合。
  - 每种窗口类型的filter对应的"最好"的filter个数(feature map数量)取决于具体数据集;
  - feature map数量100到600是一个比较合理的搜索空间。数量超过600时，performance提高有限，甚至会损害 performance，这可能是过多的feature map数量导致过拟合了;
- 激活函数：
  - 首先考虑ReLU和tanh，也可以考虑Iden（即线性变换，不做非线性变换）
- 池化策略：
  - 对于句子分类任务，1-max pooling往往比其他池化策略要好 ;
  - 这可能是因为上下文的具体位置对于预测Label可能并不是很重要，而句子某个具体的ngram(1-max pooling后filter提取出来的的特征)可能更可以刻画整个句子的某些含义，对于 预测label更有意义；
  -  (但是在其他任务如释义识别，k-max pooling可能更好。)

- 正则化：
  - 0.1到0.5之间的非零dropout rates能够提高一些performance（尽管提升幅度很小），具体的最佳设置取决于具体数据集
  - 对L2 norm加上一个约束往往不会提高performance（除了Opi数据集）;
  - 当feature map的数量大于100时，可能导致过拟合，影响performance，而dropout将减轻这种影响；
  - 在卷积层上进行dropout帮助很小，而且较大的dropout rate对performance有坏的影响。

- 参考资料：
  - [A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1510.03820.pdf) 
  - https://github.com/llhthinker/NLP-Papers/blob/master/text%20classification/2017-10/A%20Sensitivity%20Analysis%20of%20(and%20Practitioners%E2%80%99%20Guide%20to)%20Convolutional/note.md
  - https://github.com/llhthinker/NLP-Papers#distributed-word-representations



## 新闻分类

### 朴素贝叶斯

- ngram也不是越多越好
- 对max_features比较敏感

| 参数                                                         | accuracy |
| ------------------------------------------------------------ | -------- |
| CountVectorizer(analyzer='word', ngram_range=(1,2), max_features=100000) | 0.89857  |
| CountVectorizer(analyzer='word', ngram_range=(1,2), max_features=300000) | 0.90556  |



### fasttext

- dim=300, epoch=2, lr=0.1, wordNgrams=2, loss='softmax')
- accuracy = 0.84437

- fasttext 的test()接口有点奇怪，和手动测试结果不一致，以手动测试为准。
- fasttext相同参数，产生的结果的浮动性也很大。不清楚为什么，先跳过

### textcnn

- vocab_size = 100000

  maxlen = 100

  batch_size = 64

  embedding_dims = 50

  epochs = 8

  filters_size = [3,4,5]

  accuracy = 0.912461

### textrnn

- vocab_size = 100000

  maxlen = 100

  batch_size = 64

  embedding_dims = 50

  epochs = 8

  hidden_size = 128

  accuracy = 0.904516

### bilstm

- vocab_size = 100000

  maxlen = 100

  batch_size = 64

  embedding_dims = 50

  epochs = 8

  hidden_size = 128

  accuracy = 0.9027

### RCNN 

- vocab_size = 100000

  maxlen = 100

  batch_size = 64

  embedding_dims = 50

  epochs = 8

  accuracy = 0.90743

### ATT-biLSTM

0.9016

### HAN

0.9057

### ELMo



### 汇总

| 方法       | accuracy |
| ---------- | -------- |
| 朴素贝叶斯 | 0.9056   |
| fasttext   | 0.8443   |
| textcnn    | 0.9125   |
| textrnn    | 0.9045   |
| bilstm     | 0.9027   |
| RCNN       | 0.9074   |
| att-biLSTM | 0.9016   |
| HAN        | 0.9057   |
| ELMo       |          |





## 模型部署

### 1. 模型导出

```python
import tensorflow as tf
import shutil 

model = tf.keras.models.load_model('./cnn_model.h5')

# 指定路径
if os.path.exists('./Models/CNN/1'):
    shutil.rmtree('./Models/CNN/1')
    
export_path = './Models/CNN/1'
# 导出tensorflow模型以便部署
tf.saved_model.save(model,export_path)
```



### 2. 部署前的检查和测试

```shell
# 检查模型
$ saved_model_cli show --dir ./Models/CNN/1 --all

# 测试模型
$ saved_model_cli run --dir ./Models/CNN/1 --tag_set serve --signature_def serving_default --input_exp 'input_1=np.random.rand(1,100)'
```



### 3. 模型部署

- ubuntu下**tensorflow_serving** 的安装

  ```shell
  $ echo "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list && curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -
  
  $ apt update
  $ apt-get install tensorflow-model-server
  ```

- 启动服务

  ```shell
  %%bash --bg 
  $ nohup tensorflow_model_server \
    --rest_api_port=8505 \
    --model_base_path="${MODEL_DIR}" >server.log 2>&1
  ```

- 服务调用

  ```python
  import json
  import numpy
  import requests
  import jieba
  
  text = "这是该国史上最大的一次军事演习"
  text_seg = encode_sentences([jieba.lcut(text)], word_to_id)
  text_input = sequence.pad_sequences(text_seg, maxlen=maxlen)
  
  data = json.dumps({"signature_name": "serving_default",
                     "instances": text_input.reshape(1,100).tolist()})
  headers = {"content-type": "application/json"}
  json_response = requests.post('http://localhost:8505/v1/models/default:predict',
                                data=data, headers=headers)
  #print(json.loads(json_response.text))
  print(json_response.text)
  ```

  



## 其他

- 中文预训练模型：

  https://github.com/Embedding/Chinese-Word-Vectors

- ELMoForManyLangs

  https://github.com/HIT-SCIR/ELMoForManyLangs

- 调参技巧

  http://freewill.top/2018/02/26/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9ATextCNN%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/ 

  













# Plan of next week

- 


