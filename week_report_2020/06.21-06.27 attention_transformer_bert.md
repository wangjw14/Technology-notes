# 06.21-06.27 回顾

## 1. 深度文本匹配

### 1.1 应用场景

- 搜索场景（Query and results）
- 智能客服 （Q&Q, Q&A）
- 机器翻译
- 推荐（今日头条）

### 1.2 单语义文本匹配

- 基本框架

  <img src="pics/单语义.jpg" alt="单语义" style="zoom: 120%;" />

- 一个baseline，效果不错

  <img src="pics/单语义_baseline.jpg" alt="单语义_baseline" style="zoom:90%;" />

- DSSM （Deep Structured Semantic Models）

  - [Huang P S , He X , Gao J , et al. Learning deep structured semantic models for web search using clickthrough data[C]// Proceedings of the 22nd ACM international conference on Conference on information & knowledge management. ACM, 2013.](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf)  
  - ![dssm](pics/dssm.jpg)

  - Loss Function
    $$
    P(d \mid q) =\frac{\exp (\gamma f(q, d))}{\sum_{d^{\prime} \in D} \exp \left(\gamma f\left(q, d^{\prime}\right)\right)}
    $$

    $$
    L(\Lambda) =-\log \prod_{(q, d^{+})} P\left(d^{+} \mid q\right)
    $$

    其中，$\gamma$ 是 Softmax 函数的平滑参数 $; f(q, d)$ 表示一个查询项 $q$ 与文档 $d$ 之间的匹配度。$D$ 表示所有文档的集合。实际应用中，一般采用若干正例 $d^{+}$ 以及采样若干负例 $d^{-}$, 来取代整个集合 $D$ 。可以看到word2vec的影子。

- CDSSM（Convolutional Deep Structured Semantic Models ）

  - 类似DSSM，但是加入了卷积层

- ARC-I

  - [Hu B , Lu Z , Li H , et al. Convolutional Neural Network Architectures for Matching Natural Language Sentences[J]. 2015.](http://www.hangli-hl.com/uploads/3/1/6/8/3168008/hu-etal-nips2014.pdf) 
  - <img src="pics/arc-I.jpg" alt="arc-I" style="zoom:80%;" />

  - Loss function

    given the following triples $(\mathbf{x}, \mathbf{y}^{+}, \mathbf{y}^{-})$ from the oracle, with $\mathbf x$ matched with $\mathbf{y}^{+}$ better than with $\mathbf{y}^{-}$. 
    $$
    e\left(\mathbf{x}, \mathbf{y}^{+}, \mathbf{y}^{-} ; \Theta\right)=\max \left(0,1+\mathbf{s}\left(\mathbf{x}, \mathbf{y}^{-}\right)-\mathbf{s}\left(\mathbf{x}, \mathbf{y}^{+}\right)\right)
    $$
    where $\mathbf s(\mathbf{x}, \mathbf{y})$ is predicted matching score for $(\mathbf{x}, \mathbf{y})$ , 

### 1.3 多语义文本匹配

- 基本框架 

  ![多语义](pics/多语义.png)

- MV-LSTM 

  - A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations 

    ![MV-LSTM](pics/MV-LSTM.jpg)

- MatchPyramid 

  - Pang L , Lan Y , Guo J , et al. Text Matching as Image Recognition[J]. 2016. 

    ![MatchPyramid](pics/MatchPyramid.jpg)

### 1.4 参考资料

- [深度文本匹配总结](https://blog.csdn.net/melon0014/article/details/82466595) 

  

## 2. Attention

### 2.1 Attention for Image Caption 

<img src="pics/attention_for_image_caption.jpg" alt="attention_for_image_caption" style="zoom:80%;" />

### 2.2 Attention for Machine Translation  

<img src="pics/attention_for_machine_translation.jpg" alt="attention_for_machine_translation" style="zoom:35%;" />

### 2.3 Self-Attention

<img src="pics/self-attention-output.png" alt="self-attention-output" style="zoom:75%;" />

## 3. Transformer

- RNN/LSTM-based models
  - lack of long-term dependency 
  - linear computation
  - shallow model (deep from time perspective) 

- Transformer的几个关键问题

  1. 如何实现long-term dependency 

     每两个word之间都会计算attention，从而可以捕获long-term dependency。

  2. self-attention, encoder-decoder attention, decoder attention 都有哪些区别？

     self-attention和decoder attention都是self-attention。分别是：

     ```python
     MultiHeadAttention(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)
     MultiHeadAttention(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)
     ```

     encoder-decoder attention 是类似于seq2seq的attention

     ```python
     MultiHeadAttention(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)
     ```

  3. 如何编码词的顺序

     position embedding

- transformer的代码实现
  
  - [pytorch implementation](src/transformer.py) 



## 4. BERT

- Pretraining
  - word2vec ( skip-gram, Glove )	
  - Contextualized representation 
    - Language Modeling : LSTM + Unsupervised data (适合NLG，对于embedding而言，只有上文信息，缺少了下文的信息)
    - Bidriectional Language Modeling : EMLo ( 其实是双向的LM的拼接，还不是真正的同时理解上下文 ) 
    - Masked Language Modeling 
      - 其实是在输入中加入了噪声，然后要求模型去掉噪声，还原出输入。类比dropout，denosing encoder。
      - BERT使用transfomer encoder去预测mask，也可以使用别的模型，如LSTM 

- bert的代码实现
  - [pytorch implementation](src/bert.py) 



## 5. Attention 补充

### 5.1 Soft vs Hard Attention

- **Soft attention**：将encoder的所有hidden state都进行attention的计算。
  - Pro：模型是可导的
  - Con：当输入序列很长时，计算量大
- **Hard attention**：是一个随机的过程，不会将encoder的所有输出都作为其输入，而是会依概率对输入端的hidden state进行采样，将采样后的部分hidden state进行计算。
  - Pro：计算量小
  - Con：模型不可导，需要更复杂的技术（variance reduction or reinforcement learning）来训练。

### 5.2 Global vs Local Attention

- **Global attention**：传统的attention，使用encoder的所有hidden state进行计算。
- **Local attention**：介于Soft Attention和Hard Attention之间的一种Attention。是可导的。先预测当前deocder端词，在source端对应的位置 $p_t$ ，然后基于该位置选择一个窗口，用于计算attention。
  - 当encoder句子不是很长时，相对Global Attention，计算量并没有明显减小。
  - 位置向量 $p_t$ 的预测并不非常准确，这就直接计算的到的local Attention的准确率。

<img src="/Users/jingwen/Summary_notes/github/week_report/pics/attention_global_local.png" alt="attention_global_local" style="zoom:50%;" />

### 5.3 为什么自注意力模型（self-Attention model）在长距离序列中如此强大？

- 当使用神经网络来处理一个变长的向量序列时，我们通常可以使用CNN或RNN进行编码来得到一个相同长度的输出向量序列，无论CNN还是RNN其实都是对变长序列的一种“**局部编码**”：卷积神经网络显然是基于N-gram的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。

- 如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一 种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互，另一种方法是使用全连接网络。
- 全连接网络虽然是一种非常直接的建模远距离依赖的模型， 但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（self-attention model）。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。

### 5.4 Reference

- https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html
- [模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用 - lqfarmer的文章 - 知乎](https://zhuanlan.zhihu.com/p/31547842) 
- [目前主流的attention方法都有哪些？ - JayLou娄杰的回答 - 知乎](https://www.zhihu.com/question/68482809/answer/597944559) 



## 

# Plan of next week

- 


