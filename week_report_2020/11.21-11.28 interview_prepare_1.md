# 11.21-11.28 回顾

- 生活不可能像你想象的那么好，但也不会像你想象的那么糟。我觉得人的脆弱和坚强都超乎自己的想像，有时我脆弱得一句话就泪流满面，有时又发现自己咬着呀走了很长的路。

- AUC的物理意义

  - AUC是指ROC曲线下面的面积，物理意义是指，随意取一个正例和一个负例，正例的得分高于负例的概率。

  - 精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ - 东哥起飞的回答 - 知乎 https://www.zhihu.com/question/30643044/answer/510317055

  - AUC的一种计算方法
    $$
    A U C=\frac{\sum_{\text {正样本 }} \operatorname{rank}(s c o r e)-\frac{n_{1} *\left(n_{1}+1\right)}{2}}{n_{0} * n_{1}}
    $$
    假设正样本 $n_1$ 个，负样本 $n_0$ 个。

  - 如何理解机器学习和统计中的AUC？ - 无涯的回答 - 知乎 https://www.zhihu.com/question/39840928/answer/241440370

  - 最简单直观的理解，auc就是正负样本两两交叉（所有的一正一负对），其中正样本预测的概率比负样本大的对数占总对数（所有的一正一负对）的比例。（如何理解机器学习和统计中的AUC？ - 知乎 https://www.zhihu.com/question/39840928/answer/87820375）

  - The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example.（如何理解机器学习和统计中的AUC？ - 马泽锋的回答 - 知乎 https://www.zhihu.com/question/39840928/answer/105922223）

  - 乱弹机器学习评估指标AUC - 吴海波的文章 - 知乎 https://zhuanlan.zhihu.com/p/52930683

  - python实现（AUC曲线计算方法及代码实现 https://blog.csdn.net/zhj_fly/article/details/98987082）

    ```python
    import numpy as np
    from sklearn.metrics import roc_auc_score
     
     
    def calc_auc(y_labels, y_scores):
        f = list(zip(y_scores, y_labels))
        rank = [values2 for values1, values2 in sorted(f, key=lambda x:x[0])]
        rankList = [i+1 for i in range(len(rank)) if rank[i] == 1]
        pos_cnt = np.sum(y_labels == 1)
        neg_cnt = np.sum(y_labels == 0)
        auc = (np.sum(rankList) - pos_cnt*(pos_cnt+1)/2) / (pos_cnt*neg_cnt)
        print(auc)
     
     
    def get_score():
        # 随机生成100组label和score
        y_labels = np.zeros(100)
        y_scores = np.zeros(100)
        for i in range(100):
            y_labels[i] = np.random.choice([0, 1])
            y_scores[i] = np.random.random()
        return y_labels, y_scores
     
     
    if __name__ == '__main__':
        y_labels, y_scores = get_score()
        # 调用sklearn中的方法计算AUC，与后面自己写的方法作对比
        print('sklearn AUC:', roc_auc_score(y_labels, y_scores))
        calc_auc(y_labels, y_scores)
    
    ```

    

- 欧式距离和余弦距离

  - 欧式距离表示**数值**上的**绝对差异**，余弦距离表示**方向**上的**相对差异** 
  - 余弦距离 = 1 - 余弦相似度
  - 归一化之后，欧式距离和余弦距离是等价的（欧氏距离和余弦相似度的区别是什么？ - 雕栏玉砌的回答 - 知乎 https://www.zhihu.com/question/19640394/answer/207795500）
  - 余弦距离满足正定性、对称性，但不满足三角不等式。
  - KL散度满足正定性，但是不满足对称性和三角不等式。
  - 余弦距离与欧式距离 - 竹汐的文章 - 知乎 https://zhuanlan.zhihu.com/p/84643138

- NER的损失函数（分类+CRF）

  - pytorch tutorial 

  $$
  P(y \mid x)=\frac{\exp (\operatorname{Score}(x, y))}{\sum_{y^{\prime}} \exp \left(\operatorname{Score}\left(x, y^{\prime}\right)\right)}
  $$

  $$
  \begin{array}{c}
  \operatorname{Score}(x, y)=\sum_{i} \log \psi_{\mathrm{EMIT}}\left(y_{i} \rightarrow x_{i}\right)+\log \psi_{\mathrm{TRANS}}\left(y_{i-1} \rightarrow y_{i}\right) \\
  =\sum_{i} h_{i}\left[y_{i}\right]+\mathbf{P}_{y_{i} y_{i-1}}
  \end{array}
  $$

  

  - 知乎

  $$
  \begin{aligned}
  &\text { Loss Function }\\
  &=-\log \frac{P_{\text {RealPath}} }{P_{1}+P_{2}+\ldots+P_{N}}\\
  &=-\left(\log \left(e^{S_{\text {RealPath } }}\right)-\log \left(e^{S_{1}}+e^{S_{2}}+\ldots+e^{S_{N}}\right)\right)\\
  &=-\left(S_{\text {RealPath}}-\log \left(e^{S_{1}}+e^{S_{2}}+\ldots+e^{S_{N}}\right)\right)\\
  &=-\left(\sum_{i=1}^{N} x_{i y_{i}}+\sum_{i=1}^{N-1} t_{y_{i} y_{i+1}}-\log \left(e^{S_{1}}+e^{S_{2}}+\ldots+e^{S_{N}}\right)\right)
  \end{aligned}
  $$

  - 参考资料：
    - https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html
    - 最通俗易懂的BiLSTM-CRF模型中的CRF层介绍 - 孙孙的文章 - 知乎 https://zhuanlan.zhihu.com/p/44042528

- 如何处理OOV

  - 直接[UNK]
  - subword model（fasttext，wordPiece，char level，字level）
  - NLP中的subword算法及实现 - 微胖界李现的文章 - 知乎 https://zhuanlan.zhihu.com/p/112444056
  - 英文中做相应的预处理（大小写统一等等）
  - 根据业务经验进行处理，对OOV进行不同的区分（数字OOV、纯符号OOV、中文OOV）如将数字统一为一个token
  - 使用ELMo、BERT这些上下文相关的模型抽特征时会进一步弱化OOV的影响
  - 扩大词表
  - Word Embedding 如何处理未登录词？ - 夕小瑶的回答 - 知乎 https://www.zhihu.com/question/308543084/answer/589302265
  - NLP 研究主流目前如何处理 out of vocabulary words？ - 小莲子的回答 - 知乎 https://www.zhihu.com/question/55172758/answer/579240329
  
- focal loss
  $$
  FL(p_t)=-\alpha_t(1-p_t)^\gamma \log(p_t)
  $$
  或者：
  $$
  FL = -\sum_{i=1}^C \alpha_i y_i(1-\hat{y}_i)^\gamma \log(\hat y_i)
  $$

  - 当一个样本被分错的时候，pt是很小的，那么调制因子（1-Pt）接近1，损失不被影响；当Pt→1，因子（1-Pt）接近0，那么分的比较好的（well-classified）样本的权值就被调低了。
  - 当γ=0的时候，focal loss就是传统的交叉熵损失，**当γ增加的时候，调制系数也会增加。** 
  - 一般而言当γ增加的时候，a需要减小一点（实验中γ=2，a=0.25的效果最好）

  - Focal loss论文详解 - 逍遥王可爱的文章 - 知乎 https://zhuanlan.zhihu.com/p/49981234

- BERT除了分类其他任务

- 在NLP中，layer normalization 和 batch normalization 哪个用的比较多，为什么？

  - 前者用的多
  - NLP中 batch normalization与 layer normalization - 秩法策士的文章 - 知乎 https://zhuanlan.zhihu.com/p/74516930

- Loss 

  - MSE、MAE、Huber Loss
  - CE、Focal Loss、Hinge Loss

# Plan of next week

- 


