# 05.17-05.23 å›é¡¾

# 1. ç‰¹å¾å·¥ç¨‹

- æ•°æ®å’Œç‰¹å¾å†³å®šäº†æœºå™¨å­¦ä¹ ç®—æ³•çš„ä¸Šé™ã€‚æ›´å¤šçš„æ•°æ®èƒœäºæ›´å¥½çš„ç®—æ³•ï¼Œæ›´å¥½çš„æ•°æ®èƒœäºæ›´å¤šçš„æ•°æ®ã€‚
- ç‰¹å¾å·¥ç¨‹å°±æ˜¯å°†åŸå§‹æ•°æ®ç©ºé—´ï¼Œå˜æ¢åˆ°æ–°çš„ç‰¹å¾ç©ºé—´ã€‚
- ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹äºŒè€…æ­¤æ¶ˆå½¼é•¿ï¼Œå¤æ‚çš„æ¨¡å‹ä¸€å®šç¨‹åº¦ä¸Šå‡å°‘äº†ç‰¹å¾å·¥ç¨‹éœ€è¦åšçš„å·¥ä½œã€‚
- ç‰¹å¾å·¥ç¨‹çš„ç¬¬ä¸€æ­¥æ—¶ç†è§£ä¸šåŠ¡æ•°æ®å’Œä¸šåŠ¡é€»è¾‘ã€‚ç‰¹å¾æå–å¯ä»¥çœ‹ä½œæ˜¯ç”¨ç‰¹å¾æè¿°ä¸šåŠ¡é€»è¾‘çš„è¿‡ç¨‹ã€‚
- å¯¹äºé™Œç”Ÿçš„æ•°æ®ï¼Œå¯ä»¥é‡‡ç”¨æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰äº†è§£æ•°æ®ã€‚EDAçš„ç›®çš„æ˜¯å°½å¯èƒ½å¾—æ´å¯Ÿæ•°æ®ï¼Œå‘ç°æ•°æ®å†…éƒ¨çš„ç»“æ„ï¼Œæå–é‡è¦ç‰¹å¾ï¼Œæ£€æµ‹å¼‚å¸¸å€¼ï¼Œæ£€éªŒåŸºæœ¬å‡è®¾ï¼Œå»ºç«‹åˆæ­¥çš„æ¨¡å‹ã€‚EDAæŠ€æœ¯é€šå¸¸å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼š
  - å¯è§†åŒ–æŠ€æœ¯ï¼šå„ç§å›¾ã€‚
  - å®šé‡æŠ€æœ¯ï¼šæ ·æœ¬å‡å€¼ã€æ–¹å·®ã€åˆ†ä½æ•°ã€å³°åº¦ã€ååº¦ç­‰

## 1.1 ç‰¹å¾è¡¨è¾¾

### 1.1.1 æ•°å€¼ç‰¹å¾

- æ•°å€¼ç‰¹å¾åŒ…æ‹¬**ç¦»æ•£å‹**å’Œ**è¿ç»­å‹**ç‰¹å¾ã€‚ç»™å‡º8ç§å¸¸è§çš„æ•°å€¼ç‰¹å¾çš„å¤„ç†æ–¹æ³•ï¼š
  1. **æˆªæ–­**ï¼šä¿ç•™é‡è¦ä¿¡æ¯çš„å‰æä¸‹ï¼Œå»æ‰è¿‡å¤šçš„ç²¾åº¦ã€‚æˆªæ–­åå¯ä»¥çœ‹ä½œç±»åˆ«ç‰¹å¾ã€‚
  2. **äºŒå€¼åŒ–**ï¼šæ ‡è¯†æ˜¯å¦å­˜åœ¨ã€‚
  3. **åˆ†æ¡¶**ï¼šå‡åŒ€åˆ†æ¡¶ã€å¹‚ï¼ˆå¯¹æ•°ï¼‰åˆ†æ¡¶ã€åˆ†ä½æ•°åˆ†æ¡¶ã€ä½¿ç”¨æ¨¡å‹å¯»æ‰¾æœ€ä¼˜åˆ†æ¡¶
  4. **ç¼©æ”¾**ï¼šæ ‡å‡†åŒ–ç¼©æ”¾ï¼ˆZç¼©æ”¾ï¼‰ã€æœ€å¤§æœ€å°å€¼ç¼©æ”¾åŠæœ€å¤§ç»å¤§å€¼ç¼©æ”¾ã€åŸºäºèŒƒæ•°çš„ç¼©æ”¾ã€å¹³æ–¹æ ¹ç¼©æ”¾æˆ–è€…å¯¹æ•°ç¼©æ”¾ã€‚
  5. **ç¼ºå¤±å€¼å¤„ç†**ï¼šè¡¥ä¸€ä¸ªå€¼ï¼ˆå‡å€¼æˆ–ä¸­ä½æ•°ï¼‰ã€ç›´æ¥å¿½ç•¥ï¼ˆå¯¹äºå¯ä»¥å¤„ç†ç¼ºå¤±å€¼çš„æ¨¡å‹ï¼‰
  6. **ç‰¹å¾äº¤å‰**ï¼šå¯¹ä¸¤ä¸ªç‰¹å¾è¿›è¡ŒåŠ ã€å‡ã€ä¹˜ã€é™¤ç­‰æ“ä½œã€‚ï¼ˆFMå’ŒFFMæ¨¡å‹å¯ä»¥è‡ªåŠ¨è¿›è¡Œç‰¹å¾äº¤å‰ç»„åˆï¼‰
  7. **éçº¿æ€§ç¼–ç **ï¼šå¦‚é‡‡ç”¨å¤šé¡¹å¼æ ¸ã€é«˜æ–¯æ ¸ç­‰ã€‚æˆ–è€…å°†éšæœºæ£®æ—çš„å¶èŠ‚ç‚¹è¿›è¡Œç¼–ç å–‚ç»™çº¿æ€§æ¨¡å‹
  8. **è¡Œç»Ÿè®¡å€¼**ï¼šç»Ÿè®¡è¡Œå‘é‡ä¸­çš„ç©ºå€¼ä¸ªæ•°ã€æ­£å€¼æˆ–è´Ÿå€¼ä¸ªæ•°ã€å‡å€¼ã€æ–¹å·®ã€ã€ç­‰ã€‚

- ä¸ºäº†é˜²æ­¢å¼‚å¸¸ç‚¹é€ æˆçš„å½±å“ï¼Œå¢å¼ºå¥å£®æ€§ï¼Œæœ‰æ—¶å€™ä¼šä½¿ç”¨ä¸­ä½æ•°ä»£æ›¿å‡å€¼ï¼Œä½¿ç”¨åˆ†ä½æ•°ä»£æ›¿æ–¹å·®ã€‚

### 1.1.2 ç±»åˆ«ç‰¹å¾

- å¸¸è§çš„ç±»åˆ«ç‰¹å¾å¤„ç†æ–¹æ³•ï¼š
  1. **è‡ªç„¶æ•°ç¼–ç **ï¼šä½¿ç”¨è¾ƒå°‘
  2. **ç‹¬çƒ­ç¼–ç **ï¼šone-hot 
  3. **åˆ†å±‚ç¼–ç **ï¼šå¯¹äºé‚®ç¼–ã€èº«ä»½è¯å·ç ç­‰ç±»åˆ«ç‰¹å¾ï¼Œå¯ä»¥å–ä¸åŒä½æ•°è¿›è¡Œåˆ†å±‚ã€‚ä¸€èˆ¬éœ€è¦ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ã€‚
  4. **æ•£åˆ—ç¼–ç **ï¼šå½“ç‹¬çƒ­ç¼–ç éå¸¸ç¨€ç–æ—¶ï¼Œå¯ä»¥å…ˆè¿›è¡Œæ•£åˆ—ç¼–ç ã€‚é‡å¤å¤šæ¬¡é€‰å–ä¸åŒçš„æ•£åˆ—å‡½æ•°ï¼Œåˆ©ç”¨èåˆæ¥æå‡æ¨¡å‹æ•ˆæœã€‚è‡ªç„¶æ•°ç¼–ç å’Œåˆ†å±‚ç¼–ç æ—¶æ•£åˆ—ç¼–ç çš„ç‰¹ä¾‹ã€‚
  5. **è®¡æ•°ç¼–ç **ï¼šç±»åˆ«ç‰¹å¾ç”¨å…¶å¯¹åº”çš„è®¡æ•°æ¥ä»£æ›¿ï¼ˆå¼‚å¸¸å€¼æ•æ„Ÿï¼‰
  6. **è®¡æ•°æ’åç¼–ç **ï¼šç±»åˆ«ç‰¹å¾ç”¨å…¶å¯¹åº”çš„è®¡æ•°æ’åä»£æ›¿ï¼ˆå¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰
  7. **ç›®æ ‡ç¼–ç **ï¼šå¯¹äºé«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼ˆå¦‚åŸå¸‚åã€è¡—é“ç­‰ï¼‰ï¼Œç‹¬çƒ­ç¼–ç å¤ªç¨€ç–ï¼Œä½¿ç”¨åŸºäºç›®æ ‡å˜é‡å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œç¼–ç ã€‚ï¼ˆæ²¡å¤ªæ‡‚æ€ä¹ˆæ“ä½œï¼‰
  8. **ç±»åˆ«ç‰¹å¾äº¤å‰ç»„åˆ**ï¼šä¸¤ä¸ªç‰¹å¾çš„ç¬›å¡å°”ç§¯ã€å¤šä¸ªç‰¹å¾çš„ç»„åˆã€åŸºäºç»Ÿè®¡çš„ç»„åˆã€‚
  9. **ç±»åˆ«ç‰¹å¾å’Œæ•°å€¼ç‰¹å¾äº¤å‰ç»„åˆ**ï¼š



- **æ ‡ç­¾ç¼–ç ï¼šLabelEncoder or OrdinalEncoder**  

  - sklearnä¸­çš„ç”¨æ³•ï¼š[`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder) is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1.   [`OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)  convert categorical features to integer codes. 

  - ğŸŒ°

    ```python
    from sklearn import preprocessing
    le = preprocessing.LabelEncoder()
    le.fit(["paris", "paris", "tokyo", "amsterdam"])
    
    list(le.classes_)                                 # ['amsterdam', 'paris', 'tokyo']
    le.transform(["tokyo", "tokyo", "paris"])         # array([2, 2, 1])
    list(le.inverse_transform([2, 2, 1]))             # ['tokyo', 'tokyo', 'paris']
    ```

    ```python
    enc = preprocessing.OrdinalEncoder()
    X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox'],['female', 'from Aisa', 'uses Firefox']]
    enc.fit(X)
    
    enc.transform([['female', 'from US', 'uses Safari']]) 
    array([[0., 2., 1.]])
    ```

  -  `OrdinalEncoder` is for converting features, while `LabelEncoder` is for converting target variable.  (`OrdinalEncoder` for 2D data; shape `(n_samples, n_features)`, `LabelEncoder` is for 1D data: for shape `(n_samples,)`)        

  - æœ‰åºçš„éæ•°å€¼ç¦»æ•£ç‰¹å¾æ‰ä¼šç”¨åˆ°æ ‡ç­¾ç¼–ç 

- **ç‹¬çƒ­ç¼–ç ï¼šLabelBinarizer or OneHotEncoder**   

  - å¯ä»¥ç”¨sklearnçš„onehotencoderï¼Œpandasçš„get_dummiesæˆ–è€…è‡ªå·±ç”¨å­—å…¸æ˜ å°„

  - ğŸŒ° 

    ```python
    from sklearn import preprocessing
    lb = preprocessing.LabelBinarizer()
    lb.fit([1, 2, 6, 4, 2])
    
    lb.classes_                  # array([1, 2, 4, 6])
    lb.transform([1, 6])         # array([[1, 0, 0, 0],
           						 # 		  [0, 0, 0, 1]])
    ```

    ```python
    from sklearn import preprocessing
    enc = preprocessing.OneHotEncoder()
    X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
    enc.fit(X)
    
    enc.transform([['female', 'from US', 'uses Safari'],
                   ['male', 'from Europe', 'uses Safari']]).toarray()
    # array([[1., 0., 0., 1., 0., 1.],
    #        [0., 1., 1., 0., 0., 1.]])
    ```

  - å’Œä¸Šé¢ç±»ä¼¼ï¼Œ`LabelBinarizer` æ˜¯å¯¹ label è¿›è¡Œè½¬æ¢ï¼Œè€Œ `OneHotEncoder`  æ˜¯å¯¹ feature è¿›è¡Œè½¬æ¢ã€‚
  - mutli-hot ç±»å‹å¯ä»¥ç”¨  [`MultiLabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer) ã€‚

- **è®¡æ•°ç¼–ç ** 

  - æ¯ä¸€ä¸ªç±»åˆ«ç‰¹å¾çš„ç±»åˆ«å¯¹æ ‡ç­¾è¿›è¡Œsumæ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªç±»åˆ«ä¸­æ ·æœ¬æ ‡ç­¾ä¸º1çš„æ€»æ•°ã€‚

  - ä»£ç ä¸€è¡Œæå®š

    ```python
    df.groupby("category")["feature"].sum()
    ```

  - æ®è¯´è¿™ä¸ªç¼–ç åœ¨ç«èµ›ä¸­å¾ˆå¥½ç”¨ï¼Ÿ

- **ç›´æ–¹å›¾ç¼–ç **  

  - é’ˆå¯¹ **ç±»åˆ«å‹ç‰¹å¾** å’Œ **ç±»åˆ«å‹æ ‡ç­¾** çš„ä¸€ç§ç¼–ç æ–¹æ³•ã€‚ç»Ÿè®¡ç‰¹å¾çš„ä¸åŒå€¼ï¼Œåœ¨ label ä¸‹çš„å‡å€¼ã€‚ä¸¾ä¸ªğŸŒ°ï¼Œè®¾æœ‰ç‰¹å¾ $f = [Aï¼ŒAï¼ŒBï¼ŒBï¼ŒBï¼ŒCï¼ŒC]$ ï¼Œå¯¹åº”çš„ label ä¸º $[0,1,0,1,1,0,0]$ ï¼Œåˆ™ï¼Œ$A,B,C$ çš„ç›´æ–¹å›¾ç¼–ç ï¼Œåˆ†åˆ«ä¸ºï¼š$[0.5, 0.5],[0.33, 0.67],[1, 0]$  

    ```python
    df = pd.DataFrame({"a":["A","A","B","B","B","C","C"],"b":[0,1,0,1,1,0,0]})
    df["l0"] = df["b"].apply(lambda x: 1 if x==0 else 1)
    df["l1"] = df["b"].apply(lambda x: 1 if x==1 else 0)
    df.groupby("a")[["l0","l1"]].mean()
    
    #  	  l0        l1
    # a
    # A  0.500000  0.500000
    # B  0.333333  0.666667
    # C  1.000000  0.000000
    
    ```

  - ç›´æ–¹å›¾ç¼–ç çš„é—®é¢˜ï¼š
    1. å½“ä¸åŒç‰¹å¾å‡ºç°çš„é¢‘ç‡å·®åˆ«å¾ˆå¤§æ—¶ï¼Œè®¡ç®—å‡ºæ¥çš„ç»“æœæ ¹æœ¬ä¸èƒ½ç®—æ˜¯æ˜æ˜¾çš„ç»Ÿè®¡ç‰¹å¾ï¼Œè€Œå¾ˆå¯èƒ½æ˜¯ä¸€ç§å™ªéŸ³ã€‚ä¾‹å¦‚ï¼Œè®¾æœ‰ç‰¹å¾ $f = [Aï¼ŒAï¼ŒAï¼ŒAï¼ŒAï¼ŒAï¼ŒB]$ ï¼Œå¯¹åº”çš„ label ä¸º $[0,0,0,1,1,1,0]$ ï¼Œåˆ™ï¼Œ$A,B$ çš„ç›´æ–¹å›¾ç¼–ç ï¼Œåˆ†åˆ«ä¸ºï¼š$[0.5, 0.5],[1, 0]$  ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚
    2. å½“è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®åˆ†å¸ƒå·®å¼‚è¾ƒå¤§æ—¶ï¼Œæœ‰è¾ƒæ˜æ˜¾çš„è¿‡æ‹Ÿåˆç°è±¡ã€‚

- **ç›®æ ‡ç¼–ç ï¼šTargetEncoder**  

  - åˆ†ç±»é—®é¢˜ï¼šå¯¹äºCåˆ†ç±»é—®é¢˜ï¼Œç›®æ ‡ç¼–ç ï¼ˆtarget encodeï¼‰ååªéœ€è¦å¢åŠ Câˆ’1ä¸ªå±æ€§åˆ—ã€‚ä¸ºäº†è§£å†³ç›´æ–¹å›¾ç¼–ç çš„ç¬¬ä¸€ä¸ªé—®é¢˜ï¼ŒåŠ å…¥äº†å…ˆéªŒæ¦‚ç‡ã€‚å…·ä½“è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
    $$
    f\left(y_{j}, x_{i}\right)=\lambda\left(n_{i}\right) P\left(y=y_{j} \mid x=x_{i}\right)+\left(1-\lambda\left(n_{i}\right)\right) P\left(y=y_{j}\right)
    $$
    å…¶ä¸­ï¼Œ$j\in [0,C)$ ï¼Œ$n_i$ æ˜¯è®­ç»ƒé›†ä¸­ $x_i$ çš„æ ·æœ¬ä¸ªæ•°ï¼Œ$\lambda(n_i)\in [0,1]$ ï¼Œç”¨äºå¹³è¡¡å…ˆéªŒå’ŒåéªŒä¹‹é—´çš„æ¯”é‡ï¼Œä¸€ç§è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š  
    $$
    \lambda (n)=\frac{1}{1+e^{-(n-k) / f}}
    $$
    å…¶ä¸­ï¼Œk å’Œ f æ˜¯ min_sample_leaf å’Œ smoothing å‚æ•°ï¼Œåœ¨ category_encoders çš„ TargetEncoder ä¸­ï¼Œé»˜è®¤å€¼ min_samples_leaf = 1ï¼Œsmoothing = 1.0 

    ```python
    from category_encoders import TargetEncoder
    df = pd.DataFrame({"feat":["A","A","B","B","B","C","C"],"label":[0,1,0,1,1,0,0]})
    encoder = TargetEncoder()
    encoder.fit_transform(df["feat"],df["label"])
    
    #        feat
    # 0  0.480790
    # 1  0.480790
    # 2  0.638285
    # 3  0.638285
    # 4  0.638285
    # 5  0.115261
    # 6  0.115261
    ```

  - å›å½’é—®é¢˜ï¼šéœ€è¦æŠŠæ¦‚ç‡æ¢æˆå‡å€¼
    $$
    f(y,x_i) = \lambda(n_i) \frac{\sum_{x=x_i}y}{n_i} + (1-\lambda(n_i))\frac{\sum y}{N}
    $$

    ```python
    from category_encoders import TargetEncoder
    df = pd.DataFrame({"feat":["A","A","B","B","B","C","C"],"label":[1,2,3,4,5,6,7]})
    encoder = TargetEncoder()
    encoder.fit_transform(df["feat"],df["label"])
    #        feat
    # 0  2.172354
    # 1  2.172354
    # 2  4.000000
    # 3  4.000000
    # 4  4.000000
    # 5  5.827646
    # 6  5.827646
    ```

  - å®ç°è¿‡ç¨‹

    ```python
    def fit_target_encoding(self, X, y):
        mapping = {}
    
        for switch in self.ordinal_encoder.category_mapping:
            col = switch.get('col')
            values = switch.get('mapping')
    
            prior = self._mean = y.mean()
    
            stats = y.groupby(X[col]).agg(['count', 'mean'])
    
            smoove = 1 / (1 + np.exp(-(stats['count'] - self.min_samples_leaf) / self.smoothing))
            smoothing = prior * (1 - smoove) + stats['mean'] * smoove
            smoothing[stats['count'] == 1] = prior  
            # æ³¨æ„ä¸Šé¢çš„ stats, smoove, smoothing éƒ½æ˜¯ pd.DataFrame
    
            if self.handle_unknown == 'return_nan':
                smoothing.loc[-1] = np.nan
            elif self.handle_unknown == 'value':
                smoothing.loc[-1] = prior
    
            if self.handle_missing == 'return_nan':
                smoothing.loc[values.loc[np.nan]] = np.nan
            elif self.handle_missing == 'value':
                smoothing.loc[-2] = prior
    
            mapping[col] = smoothing
    
        return mapping
    
    ```

    å¯ä»¥çœ‹åˆ°ï¼Œåˆ†ç±»é—®é¢˜å’Œå›å½’é—®é¢˜ï¼Œéƒ½æ˜¯é’ˆå¯¹ label æ±‚å‡å€¼ï¼ˆæ¦‚ç‡ä¹Ÿæ˜¯é€šè¿‡å‡å€¼æ±‚å¾—åˆ°ï¼‰ï¼Œå› æ­¤å¯ä»¥æœ‰ç»Ÿä¸€çš„å½¢å¼ã€‚

  - ä¸Šè¿°æ–¹æ³•è§£å†³äº†ç›´æ–¹å›¾ç¼–ç çš„ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œè¿˜éœ€è¦ä½¿ç”¨äº¤å‰éªŒè¯çš„æ–¹æ³•ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚ä¸€èˆ¬è€Œè¨€ï¼Œå¯¹äºåˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨ `StratifiedKFold` è¿›è¡Œåˆ†å±‚æŠ½æ ·äº¤å‰ï¼Œå¯¹äºå›å½’é—®é¢˜ï¼Œä½¿ç”¨ `KFold` è¿›è¡Œæ™®é€šæŠ½æ ·äº¤å‰ã€‚ç½‘ä¸Šçš„ä¸€ä¸ªå®ç°å¦‚ä¸‹ã€‚

    [mean_encoding.py](./src/mean_encoding.py) 

  - å‚è€ƒèµ„æ–™ï¼š

    ç‰¹å¾ç¼–ç æ–¹æ³•æ€»ç»“â€”part1 https://zhuanlan.zhihu.com/p/67475635

    å¹³å‡æ•°ç¼–ç ï¼šé’ˆå¯¹é«˜åŸºæ•°å®šæ€§ç‰¹å¾ï¼ˆç±»åˆ«ç‰¹å¾ï¼‰çš„æ•°æ®é¢„å¤„ç†/ç‰¹å¾å·¥ç¨‹ https://zhuanlan.zhihu.com/p/26308272

- **è´å¶æ–¯ç›®æ ‡ç¼–ç ** 
  
  - ç•¥ï¼Œä»¥åå†è¡¥å…… 



### 1.1.3 æ—¶é—´ç‰¹å¾

- æ—¶é—´ç‰¹å¾çš„å¤„ç†æ–¹å¼ï¼š
  1. **å½“ä½œç±»åˆ«ç‰¹å¾å¤„ç†** 
  2. **è½¬åŒ–ä¸ºè‹¥å¹²ç±»åˆ«ç‰¹å¾**ï¼šè®¡ç®—å¹´ã€æœˆã€æ—¥ã€æ—¶ã€åˆ†ã€ç§’ã€æ˜ŸæœŸã€æ˜¯å¦æœˆåˆã€æ˜¯å¦æœˆæœ«ã€æ˜¯å¦å·¥ä½œæ—¥ã€æ˜¯å¦è¥ä¸šæ—¶é—´ç­‰ã€‚
  3. **è½¬åŒ–ä¸ºæ•°å€¼ç‰¹å¾**ï¼šç”¨è¿ç»­çš„æ—¶é—´å·®å€¼æ³•ï¼Œå³è®¡ç®—å‡ºæ‰€æœ‰æ ·æœ¬çš„æ—¶é—´åˆ°æŸä¸€ä¸ªæœªæ¥æ—¶é—´ä¹‹é—´çš„æ•°å€¼å·®è·ï¼Œè¿™æ ·è¿™ä¸ªå·®è·æ˜¯UTCçš„æ—¶é—´å·®ï¼Œä»è€Œå°†æ—¶é—´ç‰¹å¾è½¬åŒ–ä¸ºè¿ç»­å€¼
  4. **æ—¶é—´åºåˆ—å¤„ç†**ï¼šå¯¹äºç±»ä¼¼è‚¡ç¥¨ä»·æ ¼ã€å¤©æ°”æ¸©åº¦ç­‰æ•°æ®ï¼Œä½¿ç”¨æ»åç‰¹å¾ï¼ˆlagç‰¹å¾ï¼‰ã€‚ä½¿ç”¨æ»‘åŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾ã€‚

### 1.1.4 ç©ºé—´ç‰¹å¾

- ç©ºé—´ç‰¹å¾å¤„ç†æ–¹å¼ï¼š
  1. **å½“æˆæ•°å€¼ç‰¹å¾**ï¼šç»çº¬åº¦
  2. **å½“æˆç±»åˆ«ç‰¹å¾**ï¼šå¯¹ç»çº¬åº¦è¿›è¡Œæ•£åˆ—ï¼Œå¯¹ç©ºé—´åŒºåŸŸè¿›è¡Œåˆ†å—ã€‚è·å–å¯¹åº”çš„è¡Œæ”¿åŒºç‰¹å¾ã€åŸå¸‚ç‰¹å¾ã€è¡—é“ç‰¹å¾ç­‰ç±»åˆ«ç‰¹å¾

### 1.1.5 æ–‡æœ¬ç‰¹å¾

- æ–‡æœ¬ç‰¹å¾æ„å»ºæµç¨‹ï¼š
  - è¯­æ–™æ„å»ºï¼šæ„å»ºè¯è¡¨ã€æ–‡æ¡£ x è¯è¡¨çŸ©é˜µ
  - æ–‡æœ¬æ¸…æ´—ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ã€å»é™¤åœç”¨è¯ã€å¤§å°å†™è½¬æ¢ã€å»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ç¼–ç 
  - åˆ†è¯ï¼šè¯æ€§ç‰¹å¾ã€è¯æ ¹è¿˜åŸã€æ–‡æœ¬ç»Ÿè®¡ç‰¹å¾ï¼ˆæ–‡æœ¬é•¿åº¦ã€æ•°å­—ä¸ªæ•°ã€å¤§å°å†™å•è¯ä¸ªæ•°ã€æ•°å­—å æ¯”ç­‰ï¼‰ã€n-gramç‰¹å¾
  - è¯å‘é‡ï¼šè¯é›†ç‰¹å¾ã€è¯è¢‹ç‰¹å¾ã€TFIDFã€word2vec
  - è®¡ç®—ç›¸ä¼¼åº¦ï¼šä½™å¼¦ç›¸ä¼¼åº¦ã€Jaccardç›¸ä¼¼åº¦ã€ç¼–è¾‘è·ç¦»ã€éšè¯­ä¹‰åˆ†æ



## 1.2 ç‰¹å¾é€‰æ‹©

- ç‰¹å¾é€‰æ‹©çš„å‰ææ˜¯ï¼šè®­ç»ƒæ•°æ®ä¸­åŒ…å«å†—ä½™æˆ–è€…æ— ç”¨çš„ç‰¹å¾ï¼Œç§»é™¤è¿™äº›ç‰¹å¾ä¸ä¼šå¯¼è‡´ä¿¡æ¯çš„ä¸¢å¤±ã€‚
- **ç‰¹å¾é€‰æ‹©**å’Œ**é™ç»´**æ˜¯å¤„ç†é«˜ç»´æ•°æ®çš„ä¸¤å¤§ä¸»æµæŠ€æœ¯ã€‚

### 1.2.1 è¿‡æ»¤æ–¹æ³•

- åŒ…æ‹¬å•å˜é‡è¿‡æ»¤å’Œå¤šå˜é‡è¿‡æ»¤

- å¸¸è§çš„è¿‡æ»¤æ–¹æ³•ï¼š

  1. **è¦†ç›–ç‡**

  2. **çš®å°”æ£®ç›¸å…³ç³»æ•°** : ç›¸å…³ç³»æ•°å¹¶ä¸ä¸€å®šåˆç†ï¼Œ

  3. **Fisherå¾—åˆ†**

  4. **æ–¹å·®**ï¼š

     - æ–¹å·®è¶Šå¤§çš„ç‰¹å¾ï¼Œè¶Šæœ‰ç”¨ã€‚å¦‚æœæ–¹å·®è¾ƒå°ï¼Œæ¯”å¦‚å°äº1ï¼Œè¿™ä¸ªç‰¹å¾å¯èƒ½å¯¹ç®—æ³•ä½œç”¨æ²¡æœ‰é‚£ä¹ˆå¤§ã€‚æœ€æç«¯çš„ï¼Œå¦‚æœæŸä¸ªç‰¹å¾æ–¹å·®ä¸º0ï¼Œå³æ‰€æœ‰çš„æ ·æœ¬è¯¥ç‰¹å¾çš„å–å€¼éƒ½æ˜¯ä¸€æ ·çš„

     - sklearnä¸­çš„VarianceThresholdç±»

  5. **å‡è®¾æ£€éªŒ**ï¼š

     - åœ¨sklearnä¸­ï¼Œå¯ä»¥ä½¿ç”¨chi2è¿™ä¸ªç±»æ¥åšå¡æ–¹æ£€éªŒï¼Œ [å¡æ–¹æ£€éªŒåŸç†åŠåº”ç”¨](https://segmentfault.com/a/1190000003719712) 
     - åœ¨sklearnä¸­ï¼Œæœ‰Fæ£€éªŒçš„å‡½æ•°f_classifå’Œf_regressionã€‚

  6. **äº’ä¿¡æ¯**

     - sklearnä¸­ï¼Œå¯ä»¥ä½¿ç”¨mutual_info_classif(åˆ†ç±»)å’Œmutual_info_regression(å›å½’)æ¥è®¡ç®—å„ä¸ªè¾“å…¥ç‰¹å¾å’Œè¾“å‡ºå€¼ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚

  7. **æœ€å°å†—ä½™æœ€å¤§ç›¸å…³æ€§ ** ï¼ˆMinimum Redundancy Maximum Relevanceï¼ŒmRMRï¼‰

  8. **ç›¸å…³ç‰¹å¾é€‰æ‹©** ï¼ˆCorrelation Feature Selectionï¼ŒCFSï¼‰

  9. **Relief**ï¼ˆRelevant Featureï¼‰
     $$
     \delta^j=\sum_i-\text{diff}(x_i^j,x^j_{i,nh})^2+\text{diff}(x_i^j,x^j_{i,nm})^2
     $$
     å…¶ä¸­ï¼Œ$x^j_{i,nh}$ æ˜¯â€œçŒœä¸­è¿‘é‚»â€ï¼ˆnear hitï¼‰ï¼Œ$x^j_{i,nm}$ æ˜¯â€œçŒœé”™è¿‘é‚»â€ï¼ˆnear missï¼‰ã€‚è¯¥ç»Ÿè®¡é‡è¶Šå¤§ï¼Œè¯¥ç‰¹å¾çš„åˆ†ç±»èƒ½åŠ›è¶Šå¼ºã€‚æ­¤ç®—æ³•æ˜¯é’ˆå¯¹äºŒåˆ†ç±»è®¾è®¡çš„ï¼Œå¯ä»¥æ¨å¹¿åˆ°å¤šåˆ†ç±»ã€‚

### 1.2.2 å°è£…æ–¹æ³•

- ç›´æ¥ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•è¯„ä¼°ç‰¹å¾å­é›†çš„æ•ˆæœï¼Œç›´æ¥çœ‹å“ªäº›ç‰¹å¾çš„ç»„åˆä½¿å¾—ç®—æ³•æ€§èƒ½æœ€å¥½ã€‚
- ç›®æ ‡ï¼šä»åˆå§‹ç‰¹å¾é›†åˆä¸­ï¼Œé€‰å–ä¸€ä¸ªåŒ…å«äº†æ‰€æœ‰é‡è¦ä¿¡æ¯çš„é‡è¦å­é›†ã€‚ç©·ä¸¾æ‰€æœ‰å¯èƒ½æ€§å¤æ‚åº¦å¤ªé«˜ï¼ˆ$O(2^n)$ï¼‰ï¼Œé‡‡ç”¨é€‰å–**å€™é€‰å­é›†**çš„åŠæ³•ã€‚å€™é€‰å­é›†å¯ä»¥ä½¿ç”¨**å‰å‘**å’Œ**åå‘**ç®—æ³•è¿›è¡Œäº§ç”Ÿï¼ˆè´ªå¿ƒç®—æ³•ï¼‰ã€‚
  - **å‰å‘æœç´¢**ï¼šä»ä¸€ä¸ªç‰¹å¾å¼€å§‹ï¼Œå…ˆæ‰¾å‡ºæœ€ä¼˜çš„å•ä¸ªç‰¹å¾ï¼Œæ¯æ¬¡æ·»åŠ ä¸€ä¸ªå…¶ä»–ç‰¹å¾ï¼Œä½¿å¾—å½“å‰ä¸ªæ•°çš„å­é›†æœ€ä¼˜ï¼Œç›´è‡³k+1çš„å­é›†ä¸å¦‚kçš„å­é›†ã€‚
  - **åå‘æœç´¢**ï¼šä»å®Œæ•´ç‰¹å¾é›†åˆå¼€å§‹ï¼Œæ¯æ¬¡å»æ‰ä¸€ä¸ªæœ€æ— å…³çš„ç‰¹å¾ã€‚
  - **åŒå‘æœç´¢** 
- ä¸ºäº†é€‰å‡ºæœ€ä¼˜çš„å­é›†ï¼Œéœ€è¦è¿›è¡Œ**å­é›†è¯„ä»·**ã€‚æ¯”å¦‚ä½¿ç”¨ä¿¡æ¯ç†µç­‰ã€‚ï¼ˆå‰å‘æœç´¢+ä¿¡æ¯ç†µï¼Œä¸å†³ç­–æ ‘ç®—æ³•éå¸¸è¿‘ä¼¼ã€‚ï¼‰

- å¸¸è§çš„å°è£…æ–¹æ³•

  1. Las Vegas Wrapperï¼ˆLVWï¼‰

     - ä¸€ç§éšæœºæœç´¢æ–¹æ³•

     - ç¼ºç‚¹ï¼šå½“ç‰¹å¾æ•°å¾ˆå¤§æ—¶ï¼Œå¯èƒ½å¾ˆéš¾è¾¾åˆ°åœæ­¢æ¡ä»¶

     - ç®—æ³•è¿‡ç¨‹å¦‚ä¸‹ï¼š

       ![lvw](pics/lvw.png)

       

  2. **é€’å½’æ¶ˆé™¤ç‰¹å¾æ³•**(recursive feature eliminationï¼ŒRFE)
     - ä¸€ç§åå‘æœç´¢æ–¹æ³•ï¼Œä»¥ç»å…¸çš„SVM-RFEç®—æ³•ä¸ºä¾‹ï¼Œå½“æ±‚å‡ºSVMçš„åˆ¤åˆ«è¶…å¹³é¢ä¹‹åï¼Œæ‰¾åˆ°æƒé‡çš„å¹³æ–¹æœ€å°çš„index $i= \arg\max_i w_i^2$ ï¼Œç„¶åå°†å…¶åˆ é™¤ã€‚å¾ªç¯è¿­ä»£ã€‚
     - åœ¨sklearnä¸­ï¼Œå¯ä»¥ä½¿ç”¨RFEå‡½æ•°æ¥é€‰æ‹©ç‰¹å¾ã€‚

- å’Œè¿‡æ»¤æ–¹æ³•ç›¸æ¯”ï¼Œå°è£…æ–¹æ³•æ•ˆæœæ›´å¥½ï¼Œä½†è®¡ç®—å¼€é”€æ›´å¤§ã€‚

### 1.2.3 åµŒå…¥æ–¹æ³•

- åˆ©ç”¨L1æ­£åˆ™åšç‰¹å¾é€‰æ‹©ï¼Œæœ€å…¸å‹çš„ä¾‹å­ï¼šLASSO

- LASSOçš„æ±‚è§£ï¼šCoordinate descent

  - æ¯æ¬¡åªå¯¹ä¸€ä¸ªå‚æ•°è¿›è¡Œè¿­ä»£æ±‚è§£

  - å…·ä½“æ¨å¯¼åŠå®ç°ï¼š
    $$
    \begin{align}
    L &= \sum_{i=1}^n (\sum_{j=1}^d w_jx_{ij} +b-y_i) + \lambda\sum_{j=1}^d |w_j| \\
    \frac{\part L}{\part w_l}&=2\sum_{i=1}^n(\sum_{j=1}^dw_jx_{ij}+b+y_i)\cdot x_{il}+\lambda\frac{\part |w_l|}{\part w_l}  \\
    &=2\sum_{i=1}^n(\sum_{j\ne l}w_jx_{i,j}+b+y_i )\cdot x_{il}+2w_l\sum _{i=1}^nx_{il}^2+\lambda\frac{\part |w_l|}{\part w_l}\\
    &= C_l+w_la_l+\lambda\frac{\part |w_l|}{\part w_l}
    \end{align}
    $$
    å…¶ä¸­ï¼Œ
    $$
    \begin{align}
    C_l &=2\sum_{i=1}^n(\sum_{j\ne l}w_jx_{i,j}+b+y_i )\cdot x_{il} \\
    a_l&=2\sum _{i=1}^nx_{il}^2
    \end{align}
    $$
    åˆ™æœ‰
    $$
    \frac{\part L}{\part w_l}=\cases {C_l+w_la_l+\lambda&  $w_l>0$\\
    \left[C_l-\lambda, C_l+\lambda\right]& $w_l=0$\\
    C_l+w_la_l+\lambda & $w_l<0$}
    $$
    å¯¹äºå‚æ•° $w_l$  
    $$
    \hat w_l=\cases{\frac{-C_l-\lambda}{a_l} & if $\ \ C_l<-\lambda$ \\
    0 &if $ -\lambda\le C_l\le \lambda $\\
    \frac{\lambda-C_l}{a_l}& if $\ \ C_l>\lambda$
    }
    $$

  - å‚è€ƒé“¾æ¥

    - https://xavierbourretsicotte.github.io/lasso_derivation.html 
    - https://xavierbourretsicotte.github.io/lasso_implementation.html 

- å…¶ä»–æ±‚è§£æ–¹æ³•ï¼š

  -  æœ€å°è§’å›å½’æ³•(Least Angle Regressionï¼Œ LARS)

## 1.3 ç‰¹å¾é€‰æ‹©çš„å·¥å…·

- sklearnçš„feature_selection
- spark MLlib
- xgboostä¸­ï¼Œxgbfiæä¾›äº†å¤šç§æŒ‡æ ‡å¯¹ç‰¹å¾åŠç‰¹å¾ç»„åˆçš„æ’åº

## 1.4 å‚è€ƒèµ„æ–™

- å‘¨å¿—åã€Šæœºå™¨å­¦ä¹ ã€‹ç¬¬11ç«  ç‰¹å¾é€‰æ‹©ä¸ç¨€ç–å­¦ä¹ 
- ã€Šç¾å›¢æœºå™¨å­¦ä¹ å®è·µã€‹ç¬¬2ç«  ç‰¹å¾å·¥ç¨‹
- åˆ˜å»ºå¹³Pinardçš„åšå®¢ [ç‰¹å¾å·¥ç¨‹ä¹‹ç‰¹å¾é€‰æ‹©](https://www.cnblogs.com/pinard/p/9032759.html)ï¼Œ [ç‰¹å¾å·¥ç¨‹ä¹‹ç‰¹å¾è¡¨è¾¾](https://www.cnblogs.com/pinard/p/9061549.html)ï¼Œ[ç‰¹å¾å·¥ç¨‹ä¹‹ç‰¹å¾é¢„å¤„ç†](https://www.cnblogs.com/pinard/p/9093890.html) ï¼Œ[Lassoå›å½’ç®—æ³•ï¼š åæ ‡è½´ä¸‹é™æ³•ä¸æœ€å°è§’å›å½’æ³•å°ç»“](https://www.cnblogs.com/pinard/p/6018889.html) 
- [ç‰¹å¾é€‰æ‹©ï¼Œç»å…¸ä¸‰åˆ€](https://mp.weixin.qq.com/s/1dF9Dot7eyiVKFcvL06QoA) 

# 2. è¶…å‚é€‰æ‹©å’Œæ­£åˆ™

## 2.1 è¶…å‚é€‰æ‹©

- äº¤å‰éªŒè¯å’Œgrid search

  ```python
  import pandas as pd
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import train_test_split
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.model_selection import GridSearchCV
  
  data = pd.read_csv("xx.csv")
  
  sents = data[1].values.tolist()
  labels = data[0].values.tolist()
  
  X_train, X_test, y_train, y_test = train_test_split(sents, labels, test_size=0.2, random_state=42)
  
  vectorizer = TfidfVectorizer()
  X_train = vectorizer.fit_transform(X_train)
  X_test = vectorizer.transform(X_test)
  
  parameters = {"C":[0.0001, 0.001, 0.01, 0.1, 0.5, 1, 2, 5]}
  lr = LogisticRegression()
  lr.fit(X_train, y_train).score(X_test, y_test)
  
  clf = GridSearchCV(lr, parameters, cv=5)
  clf.fit(X_train, y_train)
  clf.score(X_test, y_test)
  print(clf.best_params_)
  ```

- å…¶ä»–çš„å‚æ•°æœç´¢æ–¹æ³•ï¼š

  - éšæœºæœç´¢
  - é—ä¼ ç®—æ³•
  - è´å¶æ–¯ä¼˜åŒ–

## 2.2 æ­£åˆ™çš„çµæ´»ä½¿ç”¨

- L2æ­£åˆ™æ˜¯çš„å‚æ•°ç»å¯¹å€¼å¾ˆå°ï¼ŒL1æ­£åˆ™äº§ç”Ÿç¨€ç–çš„å‚æ•°ã€‚è¿˜æœ‰ä¸€äº›åˆ«çš„å‚æ•°é™åˆ¶ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ­£åˆ™è¿›è¡Œå®ç°ã€‚

  - å¯¹äºç‰¹å¾åˆ†ä¸ºå¤šç»„çš„æƒ…å†µï¼Œæ¯ä¸€ç»„ä¸­è¦äº§ç”Ÿç¨€ç–çš„å‚æ•°ï¼Œå¯ä»¥å¯¹æ¯ä¸€ä¸ªç»„åˆ†åˆ«ä½¿ç”¨L1æ­£åˆ™

  - å¦‚æœç›¸é‚»çš„ç‰¹å¾ä¹‹é—´çš„å˜åŒ–æ¯”è¾ƒç¼“æ…¢ï¼Œå¯ä»¥åŠ ä¸€ä¸ªç›¸é‚»å…ƒç´ ä¹‹é—´çš„æ­£åˆ™ã€‚å¯ä»¥åº”ç”¨äºéšæ—¶é—´å˜åŒ–çš„æ¨èç³»ç»Ÿï¼ˆTime-Aware Recommendationï¼‰ä¸­ï¼Œç”¨æˆ·çš„åå¥½å¯èƒ½ä¼šéšæ—¶é—´å˜åŒ–ï¼Œä½†æ˜¯å˜åŒ–ä¸ä¼šéå¸¸å‰§çƒˆï¼Œå› æ­¤å¯ä»¥åŠ ä¸€ä¸ªç±»ä¼¼ä¸‹å¼çš„æ­£åˆ™ï¼š
    $$
    \sum_{t=2}^T\sum_{i=1}^n||u_i^{(t)}-u_i^{(t-1)}||_2^2
    $$
    

## 2.3 MLEå’ŒMAP

- MLEï¼šMaximum Likelihood Estimation ï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡
  $$
  \begin{align}
  \hat \theta&=\arg\max P(X|\theta)\\
  &=\arg\max \sum_i \log P(x_i|\theta)
  \end{align}
  $$
  å…¶ä¸­ï¼Œ$X$ æ˜¯æ•°æ®é›†ã€‚

- MAPï¼šMaximum A Posteriori ï¼Œæœ€å¤§åéªŒä¼°è®¡
  $$
  \begin{align}
  \hat \theta&=\arg\max P(\theta|X)\\
  &=\arg\max P(X|\theta)P(\theta) \\
  &=\arg\max \sum_i \log P(x_i|\theta)+\log P(\theta)
  \end{align}
  $$
  å…¶ä¸­ï¼Œ $P(\theta)$ æ˜¯å…ˆéªŒæ¦‚ç‡ã€‚

- å½“ $\theta$ æ»¡è¶³é«˜æ–¯åˆ†å¸ƒæ—¶ï¼Œ $P(\theta)$ ç­‰ä»·äºL2æ­£åˆ™
  $$
  \theta \sim   N(0,\sigma^2)\\ 
   P(\theta)=\frac1{\sqrt{2\pi}\sigma}\exp(-\frac{\theta^2}{2\sigma^2})
  $$
  
- å½“ $\theta$ æ»¡è¶³æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒæ—¶ï¼Œ $P(\theta)$ ç­‰ä»·äºL1æ­£åˆ™ 
  $$
  \theta \sim  Laplace(0,b)\\ 
   P(\theta)=\frac1{2b}\exp(-\frac{|\theta|}{b})
  $$

- å½“æ•°æ®è¶³å¤Ÿå¤šæ—¶ï¼ŒMLEå’ŒMAPçš„è§£è¶‹äºç›¸åŒã€‚

# 3. å‡¸ä¼˜åŒ–

- ä¼˜åŒ–é—®é¢˜çš„åˆ†ç±»
  1. smooth v.s. non-smooth 
     - å¹³æ»‘å‡½æ•°å¯ä»¥ä½¿ç”¨SGDï¼Œéå¹³æ»‘å‡½æ•°ï¼ˆå¦‚Lassoï¼‰è¦ä½¿ç”¨ç‰¹åˆ«çš„æ–¹æ³•
  2. convex v.s. non-convex
     - logisticæ˜¯å‡¸å‡½æ•°ï¼Œç¥ç»ç½‘ç»œæ˜¯éå‡¸å‡½æ•°ã€‚å¯¹äºéå‡¸å‡½æ•°ï¼Œåˆå§‹åŒ–æ˜¾å¾—æ›´é‡è¦ï¼Œthat why we need pretrainingã€‚
  3. discrete v.s. continous 
  4. contrained v.s. non-contrained 
- æ ¹æ®ä¼˜åŒ–é—®é¢˜çš„ä¸åŒåˆ†ç±»å¯ä»¥é€‰æ‹©ä¸åŒçš„ä¼˜åŒ–ç®—æ³•
  - Least-squares problem
  - Linear programming
  - Quadratic programming
  - Interger Programming
  - Geometric programming
  - ...

## 3.1 å‡¸ä¼˜åŒ–åŸºç¡€æ¦‚å¿µ

- å‡¸é›†ï¼šå‡è®¾å¯¹äºä»»æ„ $x,y\in C$ ï¼Œå¹¶ä¸”ä»»æ„å‚æ•° $\alpha \in [0,1]$ ï¼Œæœ‰ $\alpha x+(1-\alpha)y \in C$ ï¼Œåˆ™é›†åˆ $C$ ä¸ºå‡¸é›†ã€‚

- å¸¸è§çš„å‡¸é›†ï¼š

  - æ‰€æœ‰çš„ $R^n$ 
  - æ‰€æœ‰çš„æ­£æ•°é›†åˆ $R_+^n$ 
  - èŒƒæ•° $||x||<1$ 
  - çº¿æ€§æ–¹ç¨‹ç»„ $Ax=b$ çš„è§£ç©ºé—´
  - ä¸ç­‰å¼çš„æ‰€æœ‰è§£ $Ax<b$ 
  - ä¸¤ä¸ªå‡¸é›†çš„äº¤é›†ä¹Ÿæ˜¯å‡¸é›†

- å‡¸å‡½æ•°ï¼šå‡½æ•°çš„**å®šä¹‰åŸŸæ˜¯å‡¸é›†**ï¼Œå¹¶ä¸”å¯¹äºå®šä¹‰åŸŸå†…çš„ä»»æ„ $x,y$ ï¼Œå‡½æ•°æ»¡è¶³
  $$
  f(\theta x+(1-\theta)y)\le \theta f(x)+(1-\theta)f(y) 
  $$
  å…¶ä¸­$\theta\in [0,1]$ 

- å¸¸è§çš„å‡¸å‡½æ•°ï¼š

  - çº¿æ€§å‡½æ•°ä¸ºå‡¸/å‡¹å‡½æ•°
  - $\exp x, -\log x, x\log x$ æ˜¯å‡¸å‡½æ•°
  - èŒƒæ•°æ˜¯å‡¸å‡½æ•°
  - $\frac {x^Tx}{t}$ ä¸ºå‡¸å‡½æ•°ï¼ˆ$x>0$ï¼‰ 

- å‡¸å‡½æ•°çš„åˆ¤æ–­æ–¹æ³•ï¼š

  1. å®šä¹‰æ³•

  2. ä¸€é˜¶åˆ¤æ–­æ³•

     å‡è®¾ $f:R^n\rightarrow R$ æ˜¯å¯å¯¼çš„ï¼Œåˆ™ $f$ æ˜¯å‡¸å‡½æ•°ï¼Œå½“ä¸”ä»…å½“
     $$
     f(y)\ge f(x)+ \nabla(x)^T(y-x)
     $$

  3. äºŒé˜¶åˆ¤æ–­æ³•

     å‡è®¾ $f:R^n\rightarrow R$ æ˜¯äºŒé˜¶å¯å¯¼çš„ï¼Œåˆ™ $f$ æ˜¯å‡¸å‡½æ•°ï¼Œå½“ä¸”ä»…å½“
     $$
     \nabla^2f(x)\ge0
     $$

  4. æ ¹æ®å‡¸å‡½æ•°çš„æ€§è´¨

     - å¦‚æœ ${f}$ å’Œ ${g}$ æ˜¯å‡¸å‡½æ•°ï¼Œé‚£ä¹ˆ${m(x)=\max\{f(x),g(x)\}}$ å’Œ $h(x)=f(x)+g(x)$ ä¹Ÿæ˜¯å‡¸å‡½æ•°ã€‚
     - å¦‚æœ $f$ å’Œ $g$ æ˜¯å‡¸å‡½æ•°ï¼Œä¸” $g$ é€’å¢ï¼Œé‚£ä¹ˆ $h(x)=g(f(x))$ æ˜¯å‡¸å‡½æ•°ã€‚
     - å‡¸æ€§åœ¨ä»¿å°„æ˜ å°„ä¸‹ä¸å˜ï¼šä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœ $f(x)$ æ˜¯å‡¸å‡½æ•°ï¼ˆ${ x\in \mathbb {R} ^{n}}$ï¼‰ï¼Œé‚£ä¹ˆ $g(y)=f(Ay+b)$ ä¹Ÿæ˜¯å‡¸å‡½æ•°ï¼Œå…¶ä¸­ $A\in \mathbb {R} ^{n\times m},\;b\in \mathbb {R} ^{m}$.
     - å¦‚æœ $f(x,y)$ åœ¨ $(x,y)$ å†…æ˜¯å‡¸å‡½æ•°ï¼Œä¸” $C$ æ˜¯ä¸€ä¸ªå‡¸çš„éç©ºé›†ï¼Œé‚£ä¹ˆ $g(x)=\inf _{y\in C}f(x,y)$  åœ¨ $x$ å†…æ˜¯å‡¸å‡½æ•°ï¼Œåªè¦å¯¹äºæŸä¸ª $x$ ï¼Œæœ‰ $g(x)>-\infty$ ã€‚

- å‡¸ä¼˜åŒ–çš„æ ‡å‡†å½¢å¼
  $$
  \begin{align}
  &\text{minimize}_x &f(x) &\\
  &\text{subject to} &g_i(x)\le0\\
  & & h_j(x)=0
  \end{align}
  $$

## 3.2 ä¼˜åŒ–ä¸¾ä¾‹

- è‚¡ç¥¨ç»„åˆä¼˜åŒ–ï¼ˆportfolio optimizationï¼‰ 

  å‡è®¾æœ‰mæ”¯è‚¡ç¥¨ï¼Œæ¯åªè‚¡ç¥¨çš„æœä»åˆ†å¸ƒ $S_i\sim N(r_i,\sigma_i^2)$ ï¼Œé€‰æ‹©ä¸€ä¸ªè‚¡ç¥¨ç»„åˆï¼Œä½¿å¾—æ”¶ç›Šæœ€é«˜ï¼Œé£é™©æœ€å°ã€‚åˆ™ï¼Œæ‰€æœ‰è‚¡ç¥¨çš„ç»„åˆç¬¦åˆåˆ†å¸ƒ 
  $$
  \sum_{i=1}^mw_iS_i\sim N(\sum_{i=1}^m w_ir_i,\sum_j\sum_{j\ne i}w_iw_j\sigma_{ij}   )
  $$
  å…¶ä¸­ï¼Œ$\sigma_{ij}$ æ˜¯åæ–¹å·®çŸ©é˜µçš„å…ƒç´ ã€‚åˆ™é—®é¢˜å¯ä»¥è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜ï¼š
  $$
  minimize -\sum_{i=1}^m w_ir_i +\sum_j\sum_{ i}w_iw_j\sigma_{ij} \\
  s.t. \ \ \ 
   w_i\ge 0 \\
   \ \ \  \  \ \ \ \ \ \ \sum_iw_i=1
  $$

- maximum flow problem ï¼ˆçº¿æ€§è§„åˆ’ï¼Œlinear programmingï¼‰

- set cover problem

## 3.3 æ¢¯åº¦ä¸‹é™æ³•çš„æ”¶æ•›æ€§

- L-Lipschitzæ¡ä»¶ä»¥åŠå®šç†

  - ä¸€ä¸ªå…‰æ»‘å‡½æ•° $f$ æ»¡è¶³L-Lipschitzæ¡ä»¶ï¼Œåˆ™å¯¹äºä»»æ„ $x,y \in R^d$ ï¼Œæœ‰ï¼š

  $$
  ||\nabla f(x)-\nabla f(y) ||\le L||x-y||
  $$

  - ä¸€ä¸ªå‡½æ•° $f$ æ»¡è¶³L-Lipschitzæ¡ä»¶ï¼Œå¹¶ä¸”æ˜¯å‡¸å‡½æ•°ï¼Œåˆ™å¯¹äºä»»æ„ $x,y \in R^d$ ï¼Œæœ‰ï¼š
    $$
    f(y)\le f(x)+\nabla f(x)(y-x) +\frac L2||y-x||^2
    $$

- å‡è®¾å‡½æ•°æ»¡è¶³L-Lipschitzæ¡ä»¶ï¼Œå¹¶ä¸”æ˜¯å‡¸å‡½æ•°ï¼Œè®¾å®š $x^*=\arg\min f(x)$ï¼Œåˆ™å¯¹äºæ­¥é•¿ $\eta_t\le\frac1L$ï¼Œ æ»¡è¶³
  $$
  f(x_k)\le f(x^*)+\frac{||x_0-x^*||^2_2}{2\eta_tk}
  $$
  å½“æˆ‘ä»¬è¿­ä»£ $k=\frac{L||x_0-x^*||^2_2}{\epsilon}$ æ¬¡ä¹‹åï¼Œå¯ä»¥ä¿è¯å¾—åˆ° $\epsilon$ - approximation optimal value 

# 4. SVM

## 4.1 çº¿æ€§åˆ†ç±»å™¨çš„å¯¼å‡º

- å¯¹äºæ•°æ® $D=\{x_i, y_i\}$ ï¼Œ$y_i\in \{1,-1\}$  æœ‰çº¿æ€§åˆ†ç±»å™¨ï¼š$w^Tx+b=0$ æœ‰
  $$
  y_i=\cases{1, & $w^Tx_i+b\ge0$ \\ 
  -1, & $w^Tx_i+b<0$} 
  $$
  ä¸Šå¼ç­‰ä»·äº
  $$
  (w^Tx_i+b)\cdot y_i\ge 0
  $$
å¯¹äºçº¿æ€§å¯åˆ†çš„æ•°æ®ï¼Œè®¾æ­£ç±»çš„æ”¯æŒå‘é‡ä¸º $x_+$ ï¼Œè´Ÿç±»å¯¹åº”çš„æ”¯æŒå‘é‡ä¸º $x_-$ ï¼Œåˆ™æœ‰
  $$
  w^Tx_++b=1\\
  w^Tx_-+b=-1\\
  x_+=x_-+\lambda w
  $$
  å°†ç¬¬3å¼ä»£å…¥ç¬¬1å¼ï¼Œå¹¶åˆ©ç”¨ç¬¬2å¼ï¼Œæœ‰ï¼š
  $$
  \lambda=\frac2{w^Tw}
  $$
  æ­£è´Ÿæ ·æœ¬ä¹‹é—´çš„marginå¯ä»¥è¡¨ç¤ºä¸ºï¼š
  $$
  margin = |x_+-x_-|=\frac2{||w||}
  $$
  
- SVMçš„hard-constraint 
  $$
  \min ||w||^2 \\
  s.t.\  (w^Tx_i+b)\cdot y_i\ge 1
  $$
  å…¶ä¸­ï¼Œæœ€å°åŒ– $||w||^2$ ç­‰ä»·äºæœ€å¤§åŒ–marginã€‚

- SVMçš„soft-constraint
  $$
  \min ||w||^2 +\lambda \sum_{i=1}^n \varepsilon_i\\
  s.t.\ (w^Tx_i+b)\cdot y_i\ge 1-\varepsilon_i,\ \ \text{ where   } \varepsilon_i\ge0
  $$
  å…¶ä¸­ï¼Œ$\varepsilon_i$ æ˜¯æ¾å¼›å˜é‡ï¼ˆslack variableï¼‰

- Hinge loss
  $$
  \min ||w||^2 +\lambda\sum_{i=1}^n\max(0,1-(w^Tx_i+b)\cdot y_i)
  $$

- SGD for Hinge loss

  - $w_0, b_0$ åˆå§‹åŒ–

  - For $i =1,2,...,n$ :

    - if $1-(w^Tx_i+b)\cdot y_i\le0$  
      $$
      w^* = w -\eta_t\cdot 2w
      $$

    - else
      $$
      w^* = w -\eta_t\cdot( 2w+\lambda\frac{\part (1-(w^Tx_i+b)\cdot y_i)}{\part w}) \\
      b^*=b-\eta_t(\lambda\frac{\part (1-(w^Tx_i+b)\cdot y_i)}{\part b})
      $$

## 4.2 æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§

- **å¦‚ä½•ç†è§£æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•** 

  å¯¹äºæœ‰çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼š
  $$
  \min_x f(x)\\
  \text{s.t. }g(x)=0
  $$
  è½¬åŒ–ä¸ºæ— çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼š
  $$
  \min_x f(x)+\lambda g(x)
  $$
  åˆ†åˆ«å¯¹ $x$ å’Œ $\lambda$ æ±‚å¯¼ï¼Œå¹¶å¦å¯¼æ•°ä¸ºé›¶ï¼Œæ±‚è§£ï¼Œæœ€ä¼˜å€¼ã€‚ä¸ºä»€ä¹ˆä¸Šè¿°ä¸¤ä¸ªé—®é¢˜æ˜¯ç­‰ä»·çš„ã€‚ä»å‡ ä½•æ„ä¹‰ä¸Šè¯´ï¼Œæå€¼ç‚¹å¤„æ˜¯ $f(x)$ å’Œ $g(x)$ ç›¸åˆ‡çš„åœ°æ–¹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæå€¼ç‚¹å¤„ï¼Œä¸¤ä¸ªçš„**å¯¼æ•°æ–¹å‘æ˜¯å¹³è¡Œ**çš„ï¼Œå³
  $$
  \nabla_xf(x)=\lambda \nabla g(x)
  $$
  ä¸Šå¼ç­‰ä»·äº
  $$
  \nabla_x(f(x)+\lambda g(x))=0
  $$
  åŒæ—¶ï¼Œæå€¼ç‚¹è¿˜éœ€è¦æ»¡è¶³åœ¨ $g(x)$ ä¸Šï¼Œæœ‰ $g(x)=0$ ï¼Œæ°å¥½æ˜¯ $f(x)+\lambda g(x)$ å¯¹ $x$ å’Œ $\lambda$ çš„å¯¼æ•°ç­‰äº0çš„æƒ…å†µã€‚å› æ­¤ï¼Œä¸¤è€…æ˜¯ç­‰ä»·çš„ã€‚

  [çŸ¥ä¹-å¦‚ä½•ç†è§£æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ï¼Ÿ](https://www.zhihu.com/question/38586401/answer/457058079) 

- KKTæ¡ä»¶

  å¯¹äºæœ‰çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼š
  $$
  \begin{align}
  \min_x\ & f(x)\\
  \text{s.t. }\ & c_i(x)\le0 ,   &\ i=1,2,\cdots,k\\
  &h_j(x)=0, &\ j=1,2,\cdots,l
  \end{align}
  $$
  å¼•å…¥æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š
  $$
  L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)
  $$
  å…¶ä¸­ï¼Œ$\alpha_i\ge0$ ã€‚åˆ™KKTæ¡ä»¶ä¸ºï¼š
  $$
  \begin{align}
  \nabla_xL(x^*,\alpha^*,\beta^*) =0\\
  a_i^*c_i(x^*)=0\\
  c_i(x^*)\le0\\
  \alpha_i^*\ge0\\
  h_j(x^*)=0
  \end{align}
  $$
  å¯¹äºç­‰å¼çº¦æŸ $h_j(x)$ ï¼Œæ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ä¸­å·²ç»è¿›è¡Œè¿‡è®¨è®ºã€‚

  å¯¹äº**ä¸ç­‰å¼çº¦æŸ**ï¼Œå…ˆå®šä¹‰å¯è¡ŒåŸŸ $K=\{x\in\R^n| c_i(x)\le0\}$ ï¼Œå¹¶å‡è®¾æœ€ä¼˜è§£ä¸º $x^*$ ï¼Œæœ‰ä¸¤ç§æƒ…å†µï¼š

  1. $c_i(x)<0$ ï¼Œæœ€ä¼˜è§£ $x^*$ ä¸ºäº  $K$ å†…éƒ¨ï¼Œç§°ä¸ºå†…éƒ¨è§£ï¼Œæ­¤æ—¶çº¦æŸæ¡ä»¶æ˜¯æ— æ•ˆçš„ï¼Œå³ $\alpha_i=0$  ã€‚
  2. $c_i(x)=0$ ï¼Œæœ€ä¼˜è§£ $x^*$ ä¸ºäº  $K$ è¾¹ç•Œï¼Œç§°ä¸ºè¾¹ç•Œè§£ï¼Œæ­¤æ—¶çº¦æŸæ¡ä»¶æ˜¯æœ‰æ•ˆçš„ã€‚

  å› æ­¤ï¼Œæ— è®ºå†…éƒ¨è§£è¿˜æ˜¯è¾¹ç•Œè§£ $\alpha_ic_i(x)=0$ æ’æˆç«‹ï¼Œç§°ä¸º**äº’è¡¥æ¾å¼›æ€§**ã€‚

  [çŸ¥ä¹-Karush-Kuhn-Tucker (KKT)æ¡ä»¶](https://zhuanlan.zhihu.com/p/38163970) 

- æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§

  å…¬å¼å¤ªå¤šï¼Œå°±ä¸æŠ„äº†ï¼Œå…·ä½“å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹æˆ–è€…çŸ¥ä¹ã€‚

  [çŸ¥ä¹-æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§](https://zhuanlan.zhihu.com/p/38182879) 
  
- SVMçš„KKTæ¡ä»¶
  $$
  \min_{w,b} ||w||^2+\sum_{i=1}^n \lambda_i(1-(w^Tx_i+b)\cdot y_i)  \\
  s.t.  \ \ \lambda_i(1-(w^Tx_i+b)\cdot y_i)=0 \\
  1-(w^Tx_i+b)\cdot y_i \le0  \\
  \lambda_i\ge0
  $$

## 4.3 çº¿æ€§ä¸å¯åˆ†SVM

- çº¿æ€§SVMæ— æ³•è§£å†³çº¿æ€§ä¸å¯åˆ†çš„é—®é¢˜ï¼Œé¢å¯¹çº¿æ€§ä¸å¯åˆ†çš„æ•°æ®ï¼Œæœ‰ä¸¤ç§åŠæ³•ï¼š

  - æ”¹ç”¨éçº¿æ€§æ¨¡å‹ï¼Œå¦‚ç¥ç»ç½‘ç»œ
  - å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œåœ¨é«˜ç»´ç©ºé—´å­¦ä¹ ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ï¼ˆæ˜ å°„ä¹‹åï¼Œç®—æ³•å¤æ‚åº¦ä¼šå‡é«˜--> æ ¸å‡½æ•°ï¼‰

- ä¸ºä»€ä¹ˆè¦å°†åŸé—®é¢˜è½¬åŒ–ä¸ºå¯¹å¶é—®é¢˜ï¼š

  - åŸé—®é¢˜æ¯”è¾ƒéš¾è§£å†³
  - å¯¹å¶é—®é¢˜æœ‰ä¸€äº›æ¯”è¾ƒå¥½çš„ç‰¹æ€§ï¼Œå¦‚å¯ä»¥ä½¿ç”¨kernel trick

- SVMçš„prime-dual problem
  $$
  L=\frac12 ||w||^2+\sum_{i=1}^n \lambda_i(1-(w^Tx_i+b)\cdot y_i)  \\
  s.t.  \ \ \lambda_i(1-(w^Tx_i+b)\cdot y_i)=0 \\
  1-(w^Tx_i+b)\cdot y_i \le0  \\
  \lambda_i\ge0
  $$
  ä»¤ $\frac{\part L}{\part w}=0,\frac{\part L}{\part b}=0$ ï¼Œå¾—åˆ°ï¼š
  $$
  w=\sum_{i=1}^n \lambda_iy_ix_i\\
  \sum_{i=1}^n\lambda_iy_i=0
  $$
  å°†ä¸Šä¸¤å¼ä»£å…¥ $L$ å¾—åˆ°ï¼š
  $$
  L=-\frac12\sum_{i=1}^n\sum_{j=1}^n\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum_{i=1}^n\lambda_i
  $$
  ä¸Šå¼ç§°ä¸ºdual-problemã€‚

- å¯¹å¶é—®é¢˜çš„æ ‡å‡†å½¢å¼ä¸ºï¼š
  $$
  \min_\lambda\frac12\sum_{i=1}^n\sum_{j=1}^n\lambda_i\lambda_jy_iy_jx_i^Tx_j-\sum_{i=1}^n\lambda_i \\
  s.t. \sum_{i=1}^n \lambda_iy_i=0  \\
   \lambda_i\ge0 
  $$

- åŸé—®é¢˜å’Œå¯¹å¶é—®é¢˜ï¼Œè¦æ±‚æ¥çš„å‚æ•°æ˜¯ä¸åŒçš„ã€‚

- æ ¸æŠ€å·§ 

  è®¾ $\mathcal X$ æ˜¯è¾“å…¥ç©ºé—´ï¼Œåˆ $\mathcal H$ æ˜¯ç‰¹å¾ç©ºé—´ï¼Œå­˜åœ¨ä¸€ä¸ªä» $\mathcal X$ åˆ° $\mathcal H$ çš„æ˜ å°„
  $$
  \phi(x):\mathcal X\rightarrow\mathcal H
  $$
  åˆ™å¯¹æ‰€æœ‰çš„ $x,z\in \mathcal X$ ï¼Œå‡½æ•° $K(x,z)$ æ»¡è¶³æ¡ä»¶
  $$
  K(x,z)=\phi(x)\cdot\phi(z)
  $$
  åˆ™ç§° $K(x,z)$ æ˜¯æ ¸å‡½æ•°ï¼Œ $\phi(x)$ æ˜¯æ˜ å°„å‡½æ•°ï¼Œå…¶ä¸­ $\phi(x)\cdot\phi(z)$ æ˜¯ $\phi(x)$ å’Œ $\phi(z)$ çš„å†…ç§¯ã€‚

- å¸¸è§çš„æ ¸å‡½æ•°

  - çº¿æ€§æ ¸
    $$
    K(x,y) =x^Ty
    $$
    
  - å¤šé¡¹å¼æ ¸
  
  $$
  K(x,y)=(x^Ty+1)^d
  $$
  
  - é«˜æ–¯æ ¸
  $$
    K(x,y)=\exp{-\frac{||x-y||^2}{2\sigma^2}}
  $$

# ç–‘é—® 

- ä¸€ä¸ªçŸ¥ä¹æé—®
  1. GBDTä¸­çš„æ¢¯åº¦æ˜¯ä»€ä¹ˆå¯¹ä»€ä¹ˆçš„æ¢¯åº¦ï¼Ÿ
  2. ç»™ä¸€ä¸ªæœ‰mä¸ªæ ·æœ¬ï¼Œnç»´ç‰¹å¾çš„æ•°æ®é›†ï¼Œå¦‚æœç”¨LRç®—æ³•ï¼Œé‚£ä¹ˆæ¢¯åº¦æ˜¯å‡ ç»´? 
  3. åŒæ ·çš„m $\times$ næ•°æ®é›†ï¼Œå¦‚æœç”¨GBDTï¼Œé‚£ä¹ˆæ¢¯åº¦æ˜¯å‡ ç»´ï¼Ÿmç»´ï¼Ÿnç»´ï¼Ÿm $\times$ nç»´ï¼Ÿæˆ–è€…æ˜¯ä¸æ ‘çš„æ·±åº¦æœ‰å…³ï¼Ÿæˆ–è€…ä¸æ ‘çš„å¶å­èŠ‚ç‚¹çš„ä¸ªæ•°æœ‰å…³ï¼Ÿ

# Plan of next week

- 


