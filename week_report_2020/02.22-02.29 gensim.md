# 2.22-2.29 回顾

### 通过Kaggle的tutorial学习gensim的使用

- kaggle的一个sentiment analysis的项目Bag of Words Meets Bags of Popcorn来学习使用word2vec的基本使用。该项目是一个二分类的项目，标准好训练数据25000条，未标注的数据50000条，测试数据25000条。

- 方法：

  - 使用基础的bag of words，accuracy=0.84596
  - 通过训练word2vec，去句子中词的词向量的均值作为feature， accuracy=0.82600
  - 通过k_means将词向量聚类，然后将词映射到对应的centroid上，再做bag of words模型，accuracy=0.84348
  - 为了对比测试，也用textcnn进行了预测，accuracy=0.87632

- 这个tutorial熟悉的API

  - 预处理相关

    ```python
    from bs4 import BeautifulSoup
    import re
    from nltk.corpus import stopwords
    import nltk.data
    
    # 去除文本中的html标签
    text = BeautifulSoup(raw_review,features="html.parser").get_text()
    # 去除非字母的文本
    letters_only = re.sub("[^a-zA-Z]"," ",text)
    # 去停用词
    stopwords_english = set(stopwords.words("english"))
    # 切分文本中的句子
    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
    raw_sentences = tokenizer.tokenize(review.strip())
    
    ```

  - gensim相关

    ```python
    from gensim.models import Word2Vec,KeyedVectors
    num_features = 300
    min_word_count = 40
    num_workers = 4
    context = 10
    downsampling = 1e-3
    
    print("Training model...")
    logging.basicConfig(format='%(asctime)s:%(levelname)s:(message)s',level=logging.INFO)
    # 训练一个模型
    model = word2vec.Word2Vec(sentences,workers=num_workers,
    		size=num_features,min_count=min_word_count,window=context,
    		sample=downsampling)
    
    # 只保留归一化的word vectors，无法继续训练，但节省空间
    model.init_sims(replace=True)
    model_name = "300features_40minwords_10context"
    model.save(model_name)
    
    # 基础的函数
    print(model.doesnt_match("man woman child kitchen".split()))
    print(model.most_similar('queen'))
    
    # 只保留词向量
    word_vectors = model.wv
    del model
    
    wv = KeyedVectors.load("model.wv", mmap='r')
    vocab = set(wv.vocab.keys())
    vocab = wv.index2word
    array = wv.vectors # array of [vocab_size, num_feature]
    print(wv['feature'])
    
    
    ```

  - 其他

    ```python
    # bag of words(sum of one-hot)
    from sklearn.feature_extraction.text import CountVectorizer
    vectorizer = CountVectorizer(max_features = 5000)
    train_features = vectorizer.fit_transform(train['clean_review'])
    train_features = train_features.toarray()
    
    # train a random forest classifier
    forest = RandomForestClassifier(n_estimators=100)
    forest = forest.fit(train_features, train['sentiment'])
    ```

- 收获和不足
  - 熟悉了gensim中word2vec的基础API，可以写一些简单的代码
  - 不足：对于word2vec的原理忘光了，当时学的时候比较浅，且后续没有复习



## Plan of next week

- word2vec

  - CS224n笔记复习
  - 论文：[Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)
  - 论文：[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) 
  - 动手实现word2vec

- GloVe

  - [GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/pubs/glove.pdf)

- 其他和word embedding相关的文章

  - [Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016)

  - [Evaluation methods for unsupervised word embeddings](http://www.aclweb.org/anthology/D15-1036)

  - [A Latent Variable Model Approach to PMI-based Word Embeddings](http://aclweb.org/anthology/Q16-1028)

  - [Linear Algebraic Structure of Word Senses, with Applications to Polysemy](https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320)

  - [On the Dimensionality of Word Embedding.](https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf)

  - [From Word Embeddings To Document Distances](http://proceedings.mlr.press/v37/kusnerb15.pdf)

  - [Optimizing Chinese Word Segmentation for Machine Translation Performance](https://www.aclweb.org/anthology/W08-0336.pdf)

    

