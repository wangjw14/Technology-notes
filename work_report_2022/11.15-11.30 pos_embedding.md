# 11.15-11.30

## 位置编码

### 1. 绝对位置编码

#### 1.1 训练式

- **将位置编码当作可训练参数**，比如最大长度为512，编码维度为768，就初始化一个 512×768 的矩阵作为位置向量
- 被BERT、GPT采用。在输入的第k个向量 $x_k$ 中加入位置向量 $p_k$ 变为 $x_k+p_k$，其中 $p_k$ 只依赖于位置编号k。
- 缺点
  - 没有外推性，即如果预训练最大长度为512的话，那么最多就只能处理长度为512的句子
  - 改进：通过层次分解的方式，可以使得位置编码能外推到足够长的范围，同时保持还不错的效果
  - 参考文献：[层次分解位置编码，让BERT可以处理超长文本](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247515573&idx=1&sn=2d719108244ada7db3a535a435631210&chksm=96ea6235a19deb23babde5eaac484d69e4c2f53bab72d2e350f75bed18323eea3cf9be30615b#rd)

#### 1.2 三角式

- 一般也称为**Sinusoidal位置编码**，是Google的论文[《Attention is All You Need》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762)所提出来的一个显式解
  $$
  \begin{aligned}
  P E_{(p o s, 2 i)} &=\sin \left(\operatorname{pos} / 10000^{2 i / d_{\text {model }}}\right) \\
  P E_{(p o s, 2 i+1)} &=\cos \left(\text { pos } / 10000^{2 i / d_{\text {model }}}\right)
  \end{aligned}
  $$

- 现在我们很少能看到直接使用这种形式的绝对位置编码的工作，原因不详。

#### 1.3 递归式

- 如果在输入后面先接一层RNN，然后再接Transformer，那么理论上就不需要加位置编码了。
- 同理，我们也可以用RNN模型来学习一种绝对位置编码，比如从一个向量 $p_0$ 出发，通过递归格式 $p_{k+1}=f(p_k)$ 来得到各个位置的编码向量。
- FLOATER：提出了用微分方程（ODE）$dp_t/dt=h(p_t,t)$ 的方式来建模位置编码，函数 $h(p_t,t)$ 可以通过神经网络来建模，因此这种微分方程也称为神经微分方程。ICML 2020的论文[《Learning to Encode Position for Transformer with Continuous Dynamical Model》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2003.09229) 
- 基于递归模型的位置编码也具有比较好的外推性，同时它也比三角函数式的位置编码有更好的灵活性（比如容易证明三角函数式的位置编码就是FLOATER的某个特解）。但是很明显，递归形式的位置编码牺牲了一定的并行性，可能会带速度瓶颈。

#### 1.4 相乘式

- 用逐位相乘代替相加：如 $x_k⊗p_k$ 代替 $x_k+p_k$ 
- 一个实验参考：[《中文语言模型研究：(1) 乘性位置编码》](https://zhuanlan.zhihu.com/p/183234823)



### 2. 相对位置编码

- 不再完整建模每个输入的位置信息，而是在算Attention的时候考虑当前位置与被Attention的位置的相对距离

#### 2.1 经典式

- 相对位置编码起源于Google的论文[《Self-Attention with Relative Position Representations》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.02155)

- 华为开源的NEZHA模型也用到了这种位置编码

- 原始的绝对位置编码
  $$
  \left\{\begin{aligned}
  \boldsymbol{q}_i &=\left(\boldsymbol{x}_i+\boldsymbol{p}_i\right) \boldsymbol{W}_Q \\
  \boldsymbol{k}_j &=\left(\boldsymbol{x}_j+\boldsymbol{p}_j\right) \boldsymbol{W}_K \\
  \boldsymbol{v}_j &=\left(\boldsymbol{x}_j+\boldsymbol{p}_j\right) \boldsymbol{W}_V \\
  a_{i, j} &=\operatorname{softmax}\left(\boldsymbol{q}_i \boldsymbol{k}_j^{\top}\right) \\
  \boldsymbol{o}_i &=\sum_j a_{i, j} \boldsymbol{v}_j
  \end{aligned}\right.
  $$
  其中
  $$
  \boldsymbol{q}_i \boldsymbol{k}_j^{\top}=\left(\boldsymbol{x}_i+\boldsymbol{p}_i\right) \boldsymbol{W}_Q \boldsymbol{W}_K^{\top}\left(\boldsymbol{x}_j+\boldsymbol{p}_j\right)^{\top}=\left(\boldsymbol{x}_i \boldsymbol{W}_Q+\boldsymbol{p}_i \boldsymbol{W}_Q\right)\left(\boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{W}_K^{\top} \boldsymbol{p}_j^{\top}\right)
  $$
  Google把第一项位置去掉，第二项 $\boldsymbol{p}_j\boldsymbol{W}_K$ 改为二元位置向量 ${\boldsymbol{R}_{i, j}^K}$，变成：
  $$
  a_{i, j}=\operatorname{softmax}\left(\boldsymbol{x}_i \boldsymbol{W}_Q\left(\boldsymbol{x}_j \boldsymbol{W}_K+\textcolor{green}{\boldsymbol{R}_{i, j}^K}\right)^{\top}\right)
  $$
  以及 $\boldsymbol{o}_i=\sum_j a_{i, j} \boldsymbol{v}_j=\sum_j a_{i, j}\left(\boldsymbol{x}_j \boldsymbol{W}_V+\boldsymbol{p}_j \boldsymbol{W}_V\right)$ 中的 $\boldsymbol{p}_j \boldsymbol{W}_V$ 换成 $\boldsymbol{R}_{i, j}^V$ :
  $$
  \boldsymbol{o}_i=\sum_j a_{i, j}\left(\boldsymbol{x}_j \boldsymbol{W}_V+\textcolor{green}{\boldsymbol{R}_{i, j}^V}\right)
  $$
  所谓相对位置，就是将原本依赖二元坐标的向量${\boldsymbol{R}_{i, j}^K}, {\boldsymbol{R}_{i, j}^V}$，改为只依赖相对距离 $i-j$， 并且通常会进行截断
  $$
  \begin{aligned}
  &\boldsymbol{R}_{i, j}^K=\boldsymbol{p}_K\left[\operatorname{clip}\left(i-j, p_{\min }, p_{\max }\right)\right] \\
  &\boldsymbol{R}_{i, j}^V=\boldsymbol{p}_V\left[\operatorname{clip}\left(i-j, p_{\min }, p_{\text {max }}\right)\right]
  \end{aligned}
  $$

#### 2.2 XLNET式

- 源自Transformer-XL的论文[《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.02860) 

- XLNET式位置编码源于对上述 $\boldsymbol{q}_i \boldsymbol{k}_j^{\top}$ 的完全展开:
  $$
  \boldsymbol{q}_i \boldsymbol{k}_j^{\top}=\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{p}_j^{\top}+\boldsymbol{p}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{p}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{p}_j^{\top}
  $$
  Transformer-XL的做法很简单, 直接将 $\boldsymbol{p}_j$ 替换为相对位置向量 $\boldsymbol{R}_{i-j}$，至于两个 $\boldsymbol{p}_i$， 则干脆替换 为两个可训练的向量 $\boldsymbol{u}, \boldsymbol{v}$ :
  $$
  \boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \textcolor{green}{\boldsymbol{R}_{i-j}^{\top}}+\textcolor{red}{\boldsymbol{u} }\boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\textcolor{red}{\boldsymbol{v}} \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \textcolor{green}{\boldsymbol{R}_{i-j}^{\top}}
  $$
  此外， $\boldsymbol{v}_j$ 上的位置偏置就直接去掉了, 即直接令 $\boldsymbol{o}_i=\sum_j a_{i, j} \boldsymbol{x}_j \boldsymbol{W}_V$ 。

- 似乎从这个工作开始，后面的相对位置编码都只加到Attention矩阵上去，而不加到 $\boldsymbol{v}_j$ 上去了。

#### 2.3 T5式

- 出自文章[《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1910.10683) 

- 包含同样的思想的还有微软在ICLR 2021的论文[《Rethinking Positional Encoding in Language Pre-training》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2006.15595)中提出的TUPE位置编码。

- 对 $\boldsymbol{q}_i \boldsymbol{k}_j^{\top}$ 的完全展开:
  $$
  \boldsymbol{q}_i \boldsymbol{k}_j^{\top}=\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{p}_j^{\top}+\boldsymbol{p}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{p}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{p}_j^{\top}
  $$
  T5去除了第2、3项，第4项 $\boldsymbol{p}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{p}_j^{\top} $ 只依赖于位置${i,j}$，因此可以将其作为参数直接进行训练，即可以简化为：
  $$
  \boldsymbol{q}_i \boldsymbol{k}_j^{\top}=\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top} +\textcolor{green}{\boldsymbol\beta_{i,j}}
  $$

- 比较“别致”的是，不同于常规位置编码对将 $\boldsymbolβ_{i,j}$ 视为 $i-j$ 的函数并进行截断的做法，T5对相对位置进行了一个“**分桶**”处理，即相对位置是$i-j$ 的位置实际上对应的是$f(i-j)$ 位置，映射关系如下：
  $$
  \begin{array}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
  \hline i-j & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\
  \hline f(i-j) & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 8 & 8 & 8 & 9 & 9 & 9 & 9 \\
  \hline i-j & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & \cdots \\
  \hline f(i-j) & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 11 & 11 & 11 & 11 & 11 & 11 & 11 & 11 & \cdots \\
  \hline
  \end{array}
  $$
  具体参见代码。

  

#### 2.4 DeBERTa式

- 论文为[《DeBERTa: Decoding-enhanced BERT with Disentangled Attention》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2006.03654) 

- 对 $\boldsymbol{q}_i \boldsymbol{k}_j^{\top}$ 的完全展开:
  $$
  \boldsymbol{q}_i \boldsymbol{k}_j^{\top}=\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{p}_j^{\top}+\boldsymbol{p}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{p}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{p}_j^{\top}
  $$
  扔掉了第4项，保留第2、3项并且替换为相对位置编码
  $$
  \boldsymbol{q}_i \boldsymbol{k}_j^{\top}=\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}+\boldsymbol{x}_i \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \textcolor{green}{\boldsymbol{R}_{i, j}^{\top}}+ \textcolor{green}{\boldsymbol{R}_{j, i}} \boldsymbol{W}_Q \boldsymbol{W}_K^{\top} \boldsymbol{x}_j^{\top}
  $$

- DeBERTa比较有意思的地方，是提供了**使用相对位置和绝对位置编码的一个新视角**，

  - NLP的大多数任务可能都只需要相对位置信息，但确实有些场景下绝对位置信息更有帮助，于是它将整个模型分为两部分来理解。
  - 以Base版的MLM预训练模型为例，它一共有13层，前11层只是用相对位置编码，这部分称为Encoder，后面2层加入绝对位置信息，这部分它称之为Decoder，还弄了个简称EMD（Enhanced Mask Decoder）
  - 至于下游任务的微调截断，则是使用前11层的Encoder加上1层的Decoder来进行。



### 3. 其他位置编码

#### 3.1 CNN式



#### 3.2 复数式



#### 3.3 融合式



### 参考资料

- [让研究人员绞尽脑汁的Transformer位置编码](https://zhuanlan.zhihu.com/p/352898810) 





- Self-attention的变种，用于降低时间复杂度
  - [线性Attention的探索：Attention必须有个Softmax吗？](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247508324&idx=1&sn=0238864a68d94e7057574bee65315c6c&chksm=96ea7ee4a19df7f2332408475299041a876ff87ed0431bbb56736755ff3fc9fd5fbed29d6aa2&scene=21#wechat_redirect) 