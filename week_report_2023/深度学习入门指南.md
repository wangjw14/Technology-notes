# 深度学习入门指南

- 机器学习基础
  - loss函数
  - 梯度下降
- NLP入门：
  - 模型
    - onehot（分词不用看）
    - word2vec（最起源的模型，现在不用了）
    - textcnn、fasttext（简单任务可以用）
    - BERT（直接下载code，roBERTa、Ernie3 base、MacBert）
    - GPT
    - transformers
  - 任务类型
    - 原则：越难的任务，需要越大模型
    - 分类
    - 序列标注（CRF）
  - 相似度计算
    - 文本生成（LLM）
  - 推荐阅读：
    - 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史 - 张俊林的文章 - 知乎 https://zhuanlan.zhihu.com/p/49271699
    - 放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较 - 张俊林的文章 - 知乎 https://zhuanlan.zhihu.com/p/54743941
    - 通向AGI之路：大型语言模型（LLM）技术精要 - 张俊林的文章 - 知乎 https://zhuanlan.zhihu.com/p/597586623
    - 部分笔记：https://github.com/wangjw14/Technology-notes/blob/main/week_report_2023/LLM.md
  - 代码库
    - https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch
    - https://github.com/lonePatient/BERT-NER-Pytorch
- 了解做一个机器学习项目的流程
  - 吴恩达的[Machine Learning Yearning](https://nessie.ilab.sztaki.hu/~kornai/2020/AdvancedMachineLearning/Ng_MachineLearningYearning.pdf) 
  - 笔记在此：https://github.com/wangjw14/Technology-notes/blob/main/books/Machine%20Learning%20Yearning.md
- 进阶
  - 改loss
  - 加continue pretraining
  - 对抗训练
- 基础
  - normalization

