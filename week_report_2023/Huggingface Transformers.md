# Huggingface Transformers

## 1ã€Trainer

- https://huggingface.co/docs/transformers/main/main_classes/trainer

- https://zhuanlan.zhihu.com/p/363670628

- æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒå’Œamp

- ä¸€äº›å¯ä»¥ç»§æ‰¿æˆ–è€…æ”¹å†™çš„æ–¹æ³•

  ```python
  get_train_dataloader()
  get_eval_dataloader()
  get_test_dataloader()
  log()
  create_optimizer_and_scheduler()
  create_optimizer()
  create_scheduler()
  compute_loss()
  training_step()
  prediction_step()
  evaluate()
  predict()
  ```

- ä¸€äº›é‡è¦çš„å±æ€§

  ```python
  model
  model_wrapped 
  is_model_parallel
  place_model_on_device
  is_in_train
  ```

- Logging

  - `log_level` - for the main process

  - `log_level_replica` - for the replicas

  - ä¸åŒçš„é…ç½®æ–¹æ³•

    ```sh
    # åªæƒ³çœ‹main nodeçš„logä¿¡æ¯
    my_app.py ... --log_level warning --log_level_replica error
    # å¤šæœºçš„æƒ…å†µï¼Œåªçœ‹ä¸€å°æœºå™¨çš„main node
    my_app.py ... --log_level warning --log_level_replica error --log_on_each_node 0
    # æ‰€æœ‰èŠ‚ç‚¹éƒ½ä¿æŒå®‰é™
    my_app.py ... --log_level error --log_level_replica error --log_on_each_node 0
    ```

- å¤šGPUé€‰æ‹©

  ```sh
  CUDA_VISIBLE_DEVICES=0,2 python -m torch.distributed.launch trainer-program.py ...
  
  # è®¾ç½®GPUçš„é¡ºåº
  export CUDA_DEVICE_ORDER=PCI_BUS_ID
  export CUDA_DEVICE_ORDER=FASTEST_FIRST
  ```

- cuda ç›¸å…³

  - cudaçš„é»˜è®¤å®‰è£…è·¯å¾„ï¼š`/usr/local/cuda-10.2` 

  - æŸ¥çœ‹cudaçš„ä½ç½®ï¼š`which nvcc` 

  - é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šcudaçš„ç‰ˆæœ¬ï¼š

    ```sh
    export PATH=/usr/local/cuda-10.2/bin:$PATH
    export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH
    ```

- PyTorch Fully Sharded Data parallel

  - ä½¿ç”¨æ–¹æ³•ï¼š

    ```sh
    -m torch.distributed.launch --nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE
    ```

  - åˆ†ç‰‡ç­–ç•¥

    - `FULL_SHARD`: Zero 3  `--fsdp full_shard`
    - `SHARD_GRAD_OP`: Zero 2  `--fsdp shard_grad_op` 
    - `NO_SHARD`:  ä¸åˆ†ç‰‡ `-fsdp no_shard` 

  - ä½¿ç”¨CPU offload å’Œ auto wrap

    ```sh
    --fsdp "full_shard offload" or --fsdp "shard_grad_op offload"
    --fsdp "full_shard auto_wrap" or --fsdp "shard_grad_op auto_wrap"
    --fsdp "full_shard offload auto_wrap" or --fsdp "shard_grad_op offload auto_wrap"
    ```

  - æŒ‡å®šconfig

    ```sh
    # å‚æ•°å¯ä»¥æ˜¯configçš„è·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªdict
    --fsdp_config <path_to_fsdp_config.json>
    ```

  - fsdp_backward_prefetch or fsdp_forward_prefetch

  - ä¸æ”¯æŒç”Ÿæˆï¼Œä¸æ”¯æŒ--predict_with_generate



## 2ã€FSDP

- https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/

- https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html

- ä»PyTorch 1.11å¼€å§‹æ”¯æŒFSDP

- æ€§èƒ½æµ‹è¯•ï¼š84TFLOPS per A100 GPU for GPT 1T, and 159TFLOPS per A100 GPU for GPT 175B on AWS cluster

- PyTorchä¸­æœ‰2ç§æ–¹æ³•ä½¿ç”¨FSDPï¼šauto wrappingå’Œmanual wrapping

- Auto wrapping

  - `fsdp_auto_wrap_policy` å¯ä»¥æŒ‡å®šä¸€ä¸ªå‡½æ•°ï¼Œé€’å½’ FSDP wrap layers

  - `default_auto_wrap_policy` å¯ä»¥é€’å½’FSDP wrapæ‰€æœ‰å‚æ•°å¤šäº100Mçš„å±‚

  - `cpu_offload` å¯ä»¥offloadå‚æ•°åˆ°CPU

    ```python
    from torch.distributed.fsdp import (
       FullyShardedDataParallel,
       CPUOffload,
    )
    from torch.distributed.fsdp.wrap import (
       default_auto_wrap_policy,
    )
    import torch.nn as nn
     
    class model(nn.Module):
       def __init__(self):
           super().__init__()
           self.layer1 = nn.Linear(8, 4)
           self.layer2 = nn.Linear(4, 16)
           self.layer3 = nn.Linear(16, 4)
     
    model = DistributedDataParallel(model())
    fsdp_model = FullyShardedDataParallel(
       model(),
       fsdp_auto_wrap_policy=default_auto_wrap_policy,
       cpu_offload=CPUOffload(offload_params=True),
    )
    ```

- Manual wrapping

  - é€šè¿‡é€‰æ‹©æ¨¡å‹çš„ä¸€éƒ¨åˆ†è¿›è¡Œwrapï¼Œå®ç°æ›´å¤æ‚çš„åˆ†ç‰‡ç­–ç•¥

    ```python
    from torch.distributed.fsdp import (
       FullyShardedDataParallel,
       CPUOffload,
    )
    from torch.distributed.fsdp.wrap import (
       enable_wrap,
       wrap,
    )
    import torch.nn as nn
    from typing import Dict
     
     
    class model(nn.Module):
       def __init__(self):
           super().__init__()
           self.layer1 = wrap(nn.Linear(8, 4))
           self.layer2 = nn.Linear(4, 16)
           self.layer3 = wrap(nn.Linear(16, 4))
     
    wrapper_kwargs = Dict(cpu_offload=CPUOffload(offload_params=True))
    with enable_wrap(wrapper_cls=FullyShardedDataParallel, **wrapper_kwargs):
       fsdp_model = wrap(model())
    ```

- ç»è¿‡FSDPçš„wrapä¹‹åï¼Œå¯ä»¥åƒè®­ç»ƒæœ¬åœ°æ¨¡å‹ä¸€æ ·è®­ç»ƒFSDP model

  ```python
  optim = torch.optim.Adam(fsdp_model.parameters(), lr=0.0001)
  for sample, label in next_batch():
    out = fsdp_model(input)
    loss = criterion(out, label)
    loss.backward()
    optim.step()
  ```

- Benchmark

  - è®¾å¤‡ï¼š8 [NVIDIA A100-SXM4-40GB](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf) GPUsï¼ŒèŠ‚ç‚¹å†…éƒ¨ï¼ŒGPUé€šè¿‡AWS Elastic Fabric Adapter (EFA)è¿æ¥ï¼Œå¸¦å®½400 Gbps
  - æ¨¡å‹ï¼š [minGPT](https://github.com/karpathy/minGPT)ï¼Œ50K vocabulary size, fp16 precision and [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer
  - GPT 175Bï¼š159 TFLOPS per A100 GPUï¼ˆ51% A100çš„å³°å€¼æ€§èƒ½ï¼Œ312TFLOPSï¼‰ï¼Œbatch size 20ï¼Œseq len 512ï¼Œon 128 GPUs
  - GPT 1Tï¼š84 TFLOPS per A100 GPUï¼ˆ27% A100çš„å³°å€¼æ€§èƒ½ï¼Œ312TFLOPSï¼‰ï¼Œbatch size 4ï¼Œseq len 2048ï¼Œon 128 GPUs
  - å†å¢åŠ GPUä¸å†å¢åŠ æ€§èƒ½ï¼ŒGPUä¹‹é—´çš„é€šè®¯ä¸æ˜¯ç“¶é¢ˆï¼Œcudaåº•å±‚çš„ç¼“å­˜æˆä¸ºç“¶é¢ˆã€‚éœ€è¦å¢åŠ GPUæ˜¾å­˜



## 3ã€DeepSpeed

- https://huggingface.co/docs/transformers/main/main_classes/deepspeed

- DeepSpeedç›®å‰æ”¯æŒçš„åŠŸèƒ½

  1. Optimizer state partitioning (ZeRO stage 1)
  2. Gradient partitioning (ZeRO stage 2)
  3. Parameter partitioning (ZeRO stage 3)
  4. Custom mixed precision training handling
  5. A range of fast CUDA-extension-based optimizers
  6. ZeRO-Offload to CPU and NVMe

- deepspeedçš„ä½¿ç”¨

  ```sh
  # å•å¡çš„ä½¿ç”¨æ–¹æ³•
  deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py ...
  # å•å¡ï¼Œå¹¶æŒ‡å®šå¯¹åº”çš„GPU
  deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py ...
  
  # å¤šGPUçš„ä½¿ç”¨æ–¹æ³•1
  torch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json
  # å¤šGPUçš„ä½¿ç”¨æ–¹æ³•2
  deepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json
  
  # å¤šèŠ‚ç‚¹å¤šå¡æ–¹æ³•1ï¼Œéœ€è¦åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šæ‰‹åŠ¨å¯åŠ¨
  python -m torch.distributed.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 --master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json
  # å¤šèŠ‚ç‚¹å¤šå¡æ–¹æ³•2ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ª hostfile æ–‡ä»¶ï¼Œåªéœ€åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šå¯åŠ¨
  hostname1 slots=8
  hostname2 slots=8
  # ç„¶åè¿è¡Œ
  deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json
  
  # åœ¨SLURMä¸Šè¿è¡Œï¼Œå…ˆåˆ›å»º launch.slurm æ–‡ä»¶
  #SBATCH --job-name=test-nodes        # name
  #SBATCH --nodes=2                    # nodes
  #SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
  #SBATCH --cpus-per-task=10           # number of cores per tasks
  #SBATCH --gres=gpu:8                 # number of gpus
  #SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)
  #SBATCH --output=%x-%j.out           # output file name
  
  export GPUS_PER_NODE=8
  export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
  export MASTER_PORT=9901
  
  srun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \
   --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \
   --master_addr $MASTER_ADDR --master_port $MASTER_PORT \
  your_program.py <normal cl args> --deepspeed ds_config.json'
  # ç„¶åè¿è¡Œ
  sbatch launch.slurm
  
  # åœ¨jupyterä¸­è¿è¡Œï¼Œç•¥ï¼Œå‚è§åŸå§‹æ–‡æ¡£
  
  ```

  - ä¸ºä»€ä¹ˆå•å¡çš„æƒ…å†µï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨deepspeed
    - ä½¿ç”¨ZeRO-offloadï¼Œå°†éƒ¨åˆ†æ•°æ®offloadåˆ°CPUï¼Œé™ä½å¯¹æ˜¾å­˜çš„éœ€æ±‚
    - æä¾›äº†å¯¹æ˜¾å­˜çš„ç®¡ç†ï¼Œå‡å°‘æ˜¾å­˜ä¸­çš„ç¢ç‰‡

- ä¼ é€’å‚æ•°

  ```python
  TrainingArguments(..., deepspeed="/path/to/ds_config.json")
  
  # or
  ds_config_dict = dict(scheduler=scheduler_params, optimizer=optimizer_params)
  TrainingArguments(..., deepspeed=ds_config_dict)
  ```

- ZeROä¸­çš„é…ç½®

  - ZeRO-2

    - `overlap_comm`ï¼šè¿™ä¸ªå‚æ•°æ§åˆ¶æ˜¯å¦å¯ç”¨é€šä¿¡ä¸è®¡ç®—çš„é‡å ã€‚å½“è®¾ç½®ä¸º`True`æ—¶ï¼ŒDeepSpeedå°†å°è¯•åœ¨è¿›è¡Œæ¢¯åº¦è®¡ç®—æ—¶å¹¶è¡Œæ‰§è¡Œæ¢¯åº¦é€šä¿¡ã€‚è¿™å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘é€šä¿¡æ—¶é—´ï¼Œä»è€ŒåŠ é€Ÿæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚
    - `allgather_bucket_size`ï¼šè¿™ä¸ªå‚æ•°ç”¨äºæ§åˆ¶Allgatheræ“ä½œçš„åˆ†æ¡¶å¤§å°ã€‚Allgatheræ“ä½œæ˜¯æŒ‡åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ¯ä¸ªè¿›ç¨‹æ”¶é›†å…¶ä»–æ‰€æœ‰è¿›ç¨‹çš„æŸä¸ªå¼ é‡ï¼Œå¹¶å°†è¿™äº›å¼ é‡æŒ‰ç…§é¡ºåºæ‹¼æ¥èµ·æ¥ã€‚é€šè¿‡å°†å¼ é‡åˆ’åˆ†ä¸ºè¾ƒå°çš„æ¡¶ï¼ˆbucketsï¼‰ï¼Œå¯ä»¥åœ¨é€šä¿¡è¿‡ç¨‹ä¸­æ›´é«˜æ•ˆåœ°ä¼ è¾“æ•°æ®ã€‚`allgather_bucket_size`å€¼è¶Šå¤§ï¼Œæ¯ä¸ªæ¡¶çš„å¤§å°è¶Šå¤§ï¼Œé€šä¿¡æ“ä½œå¯èƒ½ä¼šå˜å¾—æ›´å¿«ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„å†…å­˜æ¥å­˜å‚¨ä¸­é—´ç»“æœã€‚åˆé€‚çš„æ¡¶å¤§å°éœ€è¦æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚
    - `reduce_bucket_size`ï¼šç±»ä¼¼äº`allgather_bucket_size`ï¼Œè¿™ä¸ªå‚æ•°ç”¨äºæ§åˆ¶Allreduceæ“ä½œçš„åˆ†æ¡¶å¤§å°ã€‚Allreduceæ“ä½œæ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œå°†æ‰€æœ‰è¿›ç¨‹çš„æŸä¸ªå¼ é‡è¿›è¡Œè§„çº¦ï¼ˆä¾‹å¦‚æ±‚å’Œï¼‰ï¼Œå¹¶å°†ç»“æœå¹¿æ’­å›æ‰€æœ‰è¿›ç¨‹ã€‚é€šè¿‡å°†å¼ é‡åˆ’åˆ†ä¸ºè¾ƒå°çš„æ¡¶ï¼Œå¯ä»¥æ›´é«˜æ•ˆåœ°ä¼ è¾“æ•°æ®ã€‚`reduce_bucket_size`å€¼è¶Šå¤§ï¼Œæ¯ä¸ªæ¡¶çš„å¤§å°è¶Šå¤§ï¼Œé€šä¿¡æ“ä½œå¯èƒ½ä¼šå˜å¾—æ›´å¿«ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„å†…å­˜æ¥å­˜å‚¨ä¸­é—´ç»“æœã€‚åˆé€‚çš„æ¡¶å¤§å°éœ€è¦æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚
    - `overlap_comm`ä½¿ç”¨çš„æ˜¯`allgather_bucket_size`å’Œ`reduce_bucket_size`å€¼çš„4.5å€ã€‚å› æ­¤ï¼Œå¦‚æœå®ƒä»¬è¢«è®¾ç½®ä¸º5e8ï¼Œè¿™å°†éœ€è¦9GBçš„å†…å­˜å ç”¨ï¼ˆ5e8 x 2Bytes x 2 x 4.5ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ª8GBæˆ–æ›´å°å†…å­˜çš„GPUï¼Œä¸ºäº†é¿å…OOMé”™è¯¯ï¼Œä½ éœ€è¦å°†è¿™äº›å‚æ•°å‡å°‘åˆ°å¤§çº¦2e8ï¼Œè¿™å°†éœ€è¦3.6GBçš„å†…å­˜ã€‚å¦‚æœä½ åœ¨å¤§å®¹é‡GPUä¸Šä¹Ÿå¼€å§‹å‡ºç°OOMé”™è¯¯ï¼Œä½ ä¹Ÿéœ€è¦åšåŒæ ·çš„è°ƒæ•´ã€‚
    - åœ¨deepspeed==0.4.4ä¸­æ–°å¢äº† `round_robin_gradients` é€‰é¡¹ï¼Œå¯ä»¥å¹¶è¡ŒåŒ–CPUçš„offloadã€‚å½“æ¢¯åº¦ç´¯ç§¯çš„æ­¥æ•°å¢åŠ ï¼Œæˆ–è€…GPUæ•°é‡å¢åŠ æ—¶ï¼Œä¼šæœ‰æ›´å¥½çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

  - ZeRO-3

    - `stage3_max_live_parameters` æ˜¯æ‚¨å¸Œæœ›åœ¨ä»»ä½•ç»™å®šæ—¶é—´ä¿ç•™åœ¨ GPU ä¸Šçš„å®Œæ•´å‚æ•°æ•°é‡çš„ä¸Šé™ã€‚ 

    - ` stage3_max_reuse_distance` æ˜¯ç”¨æ¥ç¡®å®šå°†æ¥ä½•æ—¶å†æ¬¡ä½¿ç”¨å‚æ•°çš„æŒ‡æ ‡ï¼Œä»è€Œå†³å®šæ˜¯ä¸¢å¼ƒå‚æ•°è¿˜æ˜¯ä¿ç•™å‚æ•°ã€‚ å¦‚æœä¸€ä¸ªå‚æ•°å°†åœ¨ä¸ä¹…çš„å°†æ¥å†æ¬¡ä½¿ç”¨ï¼ˆå°äº stage3_max_reuse_distanceï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¿ç•™å®ƒä»¥å‡å°‘é€šä¿¡å¼€é”€ã€‚ å½“æ‚¨å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹æ—¶ï¼Œè¿™éå¸¸æœ‰ç”¨ï¼Œæˆ‘ä»¬è¿›è¡Œå‰å‘é‡æ–°è®¡ç®—å¹¶å‘åä¼ é€’å•å±‚ç²’åº¦ï¼Œå¹¶å¸Œæœ›å°†å‚æ•°ä¿ç•™åœ¨å‰å‘é‡æ–°è®¡ç®—ä¸­ç›´åˆ°åå‘è®¡ç®—

    - å¦‚æœé‡åˆ° OOMï¼Œè¯·å‡å°‘ `stage3_max_live_parameters` å’Œ `stage3_max_reuse_distance`ã€‚ é™¤éæ‚¨æ­£åœ¨æ‰§è¡Œæ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œå¦åˆ™å®ƒä»¬å¯¹æ€§èƒ½çš„å½±å“åº”è¯¥å¾ˆå°ã€‚ 1e9 ä¼šæ¶ˆè€— ~2GBã€‚ å†…å­˜ç”± `stage3_max_live_parameters` å’Œ `stage3_max_reuse_distance` å…±äº«ï¼Œæ‰€ä»¥ä¸æ˜¯ç›¸åŠ çš„ï¼Œæ€»å…±æ‰ 2GBã€‚

    - å‚æ•°è®¡ç®—å…¬å¼ï¼ˆä½¿ç”¨autoæ—¶ï¼ŒTrainerä¼šè‡ªåŠ¨è®¡ç®—è¿™äº›å‚æ•°ï¼‰

      ```python
      reduce_bucket_size: hidden_size*hidden_size
      stage3_prefetch_bucket_size: 0.9 * hidden_size * hidden_size
      stage3_param_persistence_threshold: 10 * hidden_size
      ```

    - `stage3_gather_16bit_weights_on_model_save` åœ¨ä¿å­˜æ¨¡å‹æ—¶å¯ç”¨æ¨¡å‹ fp16 æƒé‡åˆå¹¶ã€‚ å¯¹äºå¤§å‹æ¨¡å‹å’Œå¤šä¸ª GPUï¼Œè¿™åœ¨å†…å­˜å’Œé€Ÿåº¦æ–¹é¢éƒ½æ˜¯ä¸€é¡¹æ˜‚è´µçš„æ“ä½œã€‚ å¦‚æœæ‚¨æ‰“ç®—æ¢å¤åŸ¹è®­ï¼Œç›®å‰éœ€è¦å®ƒã€‚ è¯·æ³¨æ„æœªæ¥çš„æ›´æ–°ï¼Œè¿™äº›æ›´æ–°å°†æ¶ˆé™¤æ­¤é™åˆ¶å¹¶ä½¿äº‹æƒ…å˜å¾—æ›´åŠ çµæ´»ã€‚

    - `sub_group_size` æ§åˆ¶åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¸­æ›´æ–°å‚æ•°çš„ç²’åº¦ã€‚ å‚æ•°è¢«åˆ†ç»„åˆ° `sub_group_size` çš„æ¡¶ä¸­ï¼Œæ¯ä¸ªæ¡¶ä¸€æ¬¡æ›´æ–°ä¸€ä¸ªã€‚ å› æ­¤ï¼Œå½“ä¸ ZeRO-Infinity ä¸­çš„ NVMe å¸è½½ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œ`sub_group_size` æ§åˆ¶æ¨¡å‹çŠ¶æ€åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´ä» NVMe ç§»å…¥å’Œç§»å‡º CPU å†…å­˜çš„ç²’åº¦ã€‚ è¿™å¯ä»¥é˜²æ­¢è¶…å¤§æ¨¡å‹è€—å°½ CPU å†…å­˜ã€‚ä¸ä½¿ç”¨NVMe offloadæ—¶ï¼Œå¯ä»¥ä½¿å…¶ä¿æŒé»˜è®¤å€¼ã€‚å½“å‡ºç°OOMæ—¶ï¼Œå‡å°`sub_group_size`ã€‚å½“ä¼˜åŒ–å™¨è¿­ä»£å¾ˆæ…¢æ—¶ï¼Œå¯ä»¥å¢å¤§`sub_group_size` ã€‚

    - ZeRO-3 ä¸­æœªä½¿ç”¨ `allgather_partitions`ã€`allgather_bucket_size` å’Œ `reduce_scatter` é…ç½®å‚æ•°

  - ZeRO-0

    - stage 0ä¼šç¦ç”¨æ‰€æœ‰çš„åˆ†ç‰‡ï¼Œç„¶åæŠŠDeepSpeedå½“ä½œæ—¶DDPæ¥ä½¿ç”¨ã€‚

  - ZeRO-1

    - åªå¯¹ä¼˜åŒ–å™¨å‚æ•°è¿›è¡Œåˆ†ç‰‡ï¼Œå¯ä»¥åŠ é€Ÿä¸€ä¸¢ä¸¢

- NVMe

  - ZeRO-Infinity éœ€è¦ä½¿ç”¨ ZeRO-3
  - ZeRO-3 ä¼šæ¯” ZeRO-2 æ…¢å¾ˆå¤šã€‚ä½¿ç”¨ä»¥ä¸‹ç­–ç•¥ï¼Œå¯ä»¥ä½¿å¾—ZeRO-3 çš„é€Ÿåº¦æ›´æ¥è¿‘ZeRO-2
    - å°†`stage3_param_persistence_threshold`å‚æ•°è®¾ç½®çš„å¾ˆå¤§ï¼Œæ¯”å¦‚`6 * hidden_size * hidden_size`
    - å°†`offload_params`å‚æ•°å…³é—­ï¼ˆå¯ä»¥æå¤§æ”¹å–„æ€§èƒ½ï¼‰

- å¦‚ä½•é€‰æ‹©ä¸åŒçš„Zero stageå’Œoffload

  - ä»å·¦åˆ°å³ï¼Œè¶Šæ¥è¶Šæ…¢

    ```sh
    Stage 0 (DDP) > Stage 1 > Stage 2 > Stage 2 + offload > Stage 3 > Stage 3 + offloads
    ```

  - ä»å·¦åˆ°å³ï¼Œæ‰€éœ€GPUæ˜¾å­˜è¶Šæ¥è¶Šå°‘

    ```sh
    Stage 0 (DDP) < Stage 1 < Stage 2 < Stage 2 + offload < Stage 3 < Stage 3 + offloads
    ```

  - è°ƒå‚æ­¥éª¤

    1. å°†`batch_size`è®¾ç½®ä¸º1ï¼Œé€šè¿‡æ¢¯åº¦ç´¯ç§¯å®ç°ä»»æ„çš„æœ‰æ•ˆ`batch_size` ï¼Œå¦‚æœOOMåˆ™
    2. è®¾ç½®`--gradient_checkpointing 1`  (HF Trainer)ï¼Œæˆ–è€… `model.gradient_checkpointing_enable()` ï¼Œå¦‚æœOOMåˆ™
    3. å°è¯•ZeRO stage 2ï¼Œå¦‚æœOOMåˆ™
    4. å°è¯•ZeRO stage 2 + `offload_optimizer` ï¼Œå¦‚æœOOMåˆ™
    5. å°è¯•ZeRO stage 3ï¼Œå¦‚æœOOMåˆ™
    6. å°è¯•offload_paramåˆ°CPUï¼Œå¦‚æœOOMåˆ™
    7. å°è¯•offload_optimizeråˆ°CPUï¼Œå¦‚æœOOMåˆ™
    8. å°è¯•é™ä½ä¸€äº›é»˜è®¤å‚æ•°ã€‚æ¯”å¦‚ä½¿ç”¨generateæ—¶ï¼Œå‡å°beam searchçš„æœç´¢èŒƒå›´
    9. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œåœ¨Ampereçš„GPUä¸Šä½¿ç”¨bf16ï¼Œåœ¨æ—§ç‰ˆæœ¬GPUä¸Šä½¿ç”¨fp16
    10. å¦‚æœä»ç„¶OOMï¼Œåˆ™ä½¿ç”¨ZeRO-Infinity ï¼Œä½¿ç”¨offload_paramå’Œoffload_optimizeråˆ°nvme
    11. ä¸€æ—¦ä½¿ç”¨batch_size=1æ—¶ï¼Œæ²¡æœ‰å¯¼è‡´OOMï¼Œæµ‹é‡æ­¤æ—¶çš„æœ‰æ•ˆååé‡ï¼Œç„¶åå°½å¯èƒ½å¢å¤§batch_size
    12. å¼€å§‹ä¼˜åŒ–å‚æ•°ï¼Œå¯ä»¥å…³é—­offloadå‚æ•°ï¼Œæˆ–è€…é™ä½ZeRO stageï¼Œç„¶åè°ƒæ•´batch_sizeï¼Œç„¶åç»§ç»­æµ‹é‡ååé‡ï¼Œç›´åˆ°æ€§èƒ½æ¯”è¾ƒæ»¡æ„ï¼ˆè°ƒå‚å¯ä»¥å¢åŠ 66%çš„æ€§èƒ½ï¼‰

  - ä¸€äº›å…¶ä»–å»ºè®®

    - å¦‚æœè®­æ¨¡å‹from scratchï¼Œhidden sizeæœ€å¥½å¯ä»¥è¢«16æ•´é™¤
    - batch sizeæœ€å¥½å¯ä»¥è¢«2æ•´é™¤

- ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨

  - å½“ä¸ä½¿ç”¨`offload_optimizer` æ—¶ï¼Œå¯ä»¥æŒ‰ç…§ä¸‹è¡¨ï¼Œæ··åˆä½¿ç”¨HFå’ŒDSçš„ä¼˜åŒ–å™¨å’Œè¿­ä»£å™¨ï¼Œé™¤äº†HF Schedulerå’ŒDS Optimizerè¿™ä¸€ç§æƒ…å†µã€‚

    | Combos       | HF Scheduler | DS Scheduler |
    | ------------ | ------------ | ------------ |
    | HF Optimizer | Yes          | Yes          |
    | DS Optimizer | No           | Yes          |

- ä¼˜åŒ–å™¨

  - å¯ç”¨ offload_optimizer æ—¶å¯ä»¥ä½¿ç”¨é DeepSpeed ä¼˜åŒ–å™¨ï¼Œåªè¦å®ƒåŒæ—¶å…·æœ‰ CPU å’Œ GPU å®ç°ï¼ˆLAMB é™¤å¤–ï¼‰ã€‚

  - DeepSpeed çš„ä¸»è¦ä¼˜åŒ–å™¨æ˜¯ Adamã€AdamWã€OneBitAdam å’Œ Lambã€‚ è¿™äº›å·²é€šè¿‡ ZeRO è¿›è¡Œäº†å½»åº•æµ‹è¯•ï¼Œå› æ­¤å»ºè®®ä½¿ç”¨ã€‚ 

  - å¦‚æœæ²¡æœ‰åœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®ä¼˜åŒ–å™¨æ¡ç›®ï¼ŒTrainer å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º AdamWï¼Œå¹¶å°†ä½¿ç”¨æä¾›çš„å€¼æˆ–ä»¥ä¸‹å‘½ä»¤è¡Œå‚æ•°çš„é»˜è®¤å€¼ï¼š--learning_rateã€--adam_beta1ã€--adam_beta2ã€ --adam_epsilon å’Œ --weight_decayã€‚

  - ä¸ AdamW ç±»ä¼¼ï¼Œæ‚¨å¯ä»¥é…ç½®å…¶ä»–å®˜æ–¹æ”¯æŒçš„ä¼˜åŒ–å™¨ã€‚ è¯·è®°ä½ï¼Œå®ƒä»¬å¯èƒ½å…·æœ‰ä¸åŒçš„é…ç½®å€¼ã€‚ ä¾‹å¦‚ å¯¹äº Adamï¼Œæ‚¨éœ€è¦å°† weight_decay è®¾ç½®ä¸º 0.01 å·¦å³ã€‚

  - æ­¤å¤–ï¼Œoffloadåœ¨ä¸ Deepspeed çš„ CPU Adam ä¼˜åŒ–å™¨ä¸€èµ·ä½¿ç”¨æ—¶æ•ˆæœæœ€ä½³ã€‚ å¦‚æœä½ æƒ³å¯¹å¸è½½ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨ï¼Œå› ä¸º deepspeed==0.8.3 ä½ è¿˜éœ€è¦æ·»åŠ ï¼š

    ```json
    {
       "zero_force_ds_cpu_optimizer": false
    }
    ```

- è°ƒåº¦å™¨

  - DeepSpeed æ”¯æŒ LRRangeTestã€OneCycleã€WarmupLR å’Œ WarmupDecayLR å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

  - Transformerså’ŒDeepSpeedä¸­è°ƒåº¦å™¨çš„overlap

    ```
    WarmupLR via --lr_scheduler_type constant_with_warmup
    WarmupDecayLR via --lr_scheduler_type linear
    ```

- fp32ç²¾åº¦

  - ç”±äº fp16 æ··åˆç²¾åº¦å¤§å¤§å‡å°‘äº†å†…å­˜éœ€æ±‚å’Œæ›´å¿«çš„é€Ÿåº¦ï¼Œå› æ­¤å”¯ä¸€ä¸æƒ³ä½¿ç”¨å®ƒçš„æ—¶é—´æ˜¯æ‚¨ä½¿ç”¨çš„æ¨¡å‹åœ¨æ­¤è®­ç»ƒæ¨¡å¼ä¸‹è¡¨ç°ä¸ä½³ã€‚ é€šå¸¸ï¼Œå½“æ¨¡å‹æœªåœ¨ fp16 æ··åˆç²¾åº¦ä¸­è¿›è¡Œé¢„è®­ç»ƒæ—¶ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼ˆä¾‹å¦‚ï¼Œè¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ bf16 é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼‰ã€‚ è¿™æ ·çš„æ¨¡å‹å¯èƒ½ä¼šæº¢å‡ºæˆ–ä¸‹æº¢ï¼Œå¯¼è‡´ NaN æŸå¤±ã€‚ å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œé‚£ä¹ˆæ‚¨å°†å¸Œæœ›ä½¿ç”¨å®Œæ•´çš„ fp32 æ¨¡å¼ã€‚
  - å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯åŸºäº Ampere æ¶æ„çš„ GPUï¼Œpytorch 1.7 åŠæ›´é«˜ç‰ˆæœ¬å°†è‡ªåŠ¨åˆ‡æ¢ä¸ºä½¿ç”¨æ›´é«˜æ•ˆçš„ tf32 æ ¼å¼è¿›è¡ŒæŸäº›æ“ä½œï¼Œä½†ç»“æœä»å°†é‡‡ç”¨ fp32ã€‚ 
  - ä½¿ç”¨ ğŸ¤— Trainerï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ --tf32 å¯ç”¨å®ƒï¼Œæˆ–ä½¿ç”¨ --tf32 0 æˆ– --no_tf32 ç¦ç”¨å®ƒã€‚ PyTorch é»˜è®¤å€¼æ˜¯ä½¿ç”¨tf32ã€‚

- è‡ªåŠ¨æ··åˆç²¾åº¦

  - fp16
    - å¯ä»¥ä½¿ç”¨ pytorch-like AMP æ–¹å¼æˆ–è€… apex-like æ–¹å¼
    - ä½¿ç”¨ `--fp16` `--fp16_backend amp` æˆ– `--fp16_full_eval` å‘½ä»¤è¡Œå‚æ•°æ—¶å¯ç”¨æ­¤æ¨¡å¼
  - bf16
    - ä½¿ç”¨`--bf16` or `--bf16_full_eval` å‘½ä»¤è¡Œå‚æ•°æ—¶å¯ç”¨æ­¤æ¨¡å¼

- NCCL

  - é€šè®¯ä¼šé‡‡ç”¨ä¸€ç§å•ç‹¬çš„æ•°æ®ç±»å‹

  - é»˜è®¤æƒ…å†µä¸‹ï¼ŒåŠç²¾åº¦è®­ç»ƒä½¿ç”¨ fp16 ä½œä¸ºreductionæ“ä½œçš„é»˜è®¤å€¼

  - å¯ä»¥å¢åŠ ä¸€ä¸ªå°çš„å¼€é”€å¹¶ç¡®ä¿reductionå°†ä½¿ç”¨ fp32 ä½œä¸ºç´¯ç§¯æ•°æ®ç±»å‹

    ```json
    {
        "communication_data_type": "fp32"
    }
    ```

- apex

  - ä½¿ç”¨`--fp16`ã€ `--fp16_backend apex`ã€ `--fp16_opt_level 01` å‘½ä»¤è¡Œå‚æ•°æ—¶å¯ç”¨æ­¤æ¨¡å¼

    ```json
    "amp": {
        "enabled": "auto",
        "opt_level": "auto"
    }
    ```

  - Apex æ˜¯ä¸€ä¸ªåœ¨ PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸‹ç”¨äºåŠ é€Ÿè®­ç»ƒå’Œæé«˜æ€§èƒ½çš„åº“ã€‚å®ƒæ˜¯è‹±ä¼Ÿè¾¾ï¼ˆNVIDIAï¼‰å¼€å‘çš„ï¼Œä¸»è¦é’ˆå¯¹å…·æœ‰ GPU åŠ é€ŸåŠŸèƒ½çš„æ·±åº¦å­¦ä¹ è®­ç»ƒã€‚Apex æä¾›äº†æ··åˆç²¾åº¦è®­ç»ƒã€åˆ†å¸ƒå¼è®­ç»ƒå’Œå†…å­˜ä¼˜åŒ–ç­‰åŠŸèƒ½ï¼Œå¸®åŠ©ç”¨æˆ·æé«˜è®­ç»ƒé€Ÿåº¦ã€æ‰©å±•è®­ç»ƒè§„æ¨¡ä»¥åŠä¼˜åŒ– GPU èµ„æºåˆ©ç”¨ç‡ã€‚

-  è·å–æ¨¡å‹å‚æ•°

  - deepspeedä¼šåœ¨ä¼˜åŒ–å™¨å‚æ•°ä¸­å­˜å‚¨æ¨¡å‹çš„ä¸»å‚æ•°ï¼Œå­˜å‚¨åœ¨`global_step*/*optim_states.pt` æ–‡ä»¶ä¸­ï¼Œæ•°æ®ç±»å‹ä¸ºfp32ã€‚å› æ­¤ï¼Œæƒ³è¦ä»checkpointä¸­æ¢å¤è®­ç»ƒï¼Œåˆ™ä¿æŒé»˜è®¤å³å¯

  - å¦‚æœæ¨¡å‹æ˜¯åœ¨ZeRO-2æ¨¡å¼ä¸‹ä¿å­˜çš„ï¼Œæ¨¡å‹å‚æ•°ä¼šä»¥fp16çš„å½¢å¼å­˜å‚¨åœ¨`pytorch_model.bin`ä¸­

  - å¦‚æœæ¨¡å‹æ˜¯åœ¨ZeRO-3æ¨¡å¼ä¸‹ä¿å­˜çš„ï¼Œéœ€è¦å¦‚ä¸‹æ‰€ç¤ºè®¾ç½®å‚æ•°ï¼Œå¦åˆ™`pytorch_model.bin`å°†ä¸ä¼šè¢«åˆ›å»º

    ```json
    {
        "zero_optimization": {
            "stage3_gather_16bit_weights_on_model_save": true
        }
    }
    ```

  - åœ¨çº¿fp32æƒé‡æ¢å¤ï¼ˆéœ€è¦å¾ˆå¤šçš„RAMï¼‰ç•¥

  - ç¦»çº¿è·å–fp32æƒé‡

    ```sh
    python zero_to_fp32.py . pytorch_model.bin
    ```

- ZeRO-3 and Infinity Nuances

  - æ„é€ è¶…å¤§æ¨¡å‹ï¼ˆç•¥ï¼‰
  - æœé›†å‚æ•°ï¼ˆç•¥ï¼‰

- ZeRO inference

  - åªæœ‰ZeRO-3æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºå¯ä»¥å°†å‚æ•°åˆ†ç‰‡

    ```sh
    deepspeed --num_gpus=2 your_program.py <normal cl args> --do_eval --deepspeed ds_config.json
    ```

- ä¼°ç®—éœ€è¦çš„æ˜¾å­˜

  ```sh
  $ python -c 'from transformers import AutoModel; \
  from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
  model = AutoModel.from_pretrained("bigscience/T0_3B"); \
  estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=2, num_nodes=1)'
  [...]
  Estimated memory needed for params, optim states and gradients for a:
  HW: Setup with 1 node, 2 GPUs per node.
  SW: Model with 2783M total params, 65M largest layer params.
    per CPU  |  per GPU |   Options
     70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1
     70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0
     62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=1
     62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=0
      0.74GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=1
     31.11GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=0
  ```

- å¯èƒ½é‡åˆ°çš„é—®é¢˜

  - å¯åŠ¨æ—¶ï¼Œè¿›ç¨‹è¢«æ€æ­»ï¼Œå¹¶ä¸”æ²¡æœ‰æ‰“å°å‡ºtracebackï¼šCPUæ˜¾å­˜ä¸å¤Ÿ
  - lossæ˜¯NaNï¼šè®­ç»ƒæ—¶ç”¨çš„æ˜¯bf16ï¼Œä½¿ç”¨æ—¶æ˜¯fp16ã€‚å¸¸å¸¸å‘ç”Ÿäºgoogleåœ¨TPUä¸Štrainçš„æ¨¡å‹ï¼Œå¦‚T5ã€‚æ­¤æ—¶éœ€è¦ä½¿ç”¨fp32æˆ–è€…bf16ã€‚











overlap_comm

Zero_int





## 4ã€HfArgumentParser

- https://zhuanlan.zhihu.com/p/296535876

- ä¸€ä¸ªç¤ºä¾‹

  ```python
  from transformers import HfArgumentParser
  from dataclasses import dataclass, field
  
  @dataclass()
  class A():
      a: str = field()
  
  @dataclass()
  class B():
      b: str = field(
          default="hahah"
      )
  
  parser = HfArgumentParser((A, B))
  a_args, b_args = parser.parse_args_into_dataclasses()
  def main():
      print(a_args.a)
      print(b_args.b)
  
  main()
  
  ```

  è¿è¡Œä»£ç ï¼š

  ```sh
  python test.py --a="hello world"
  ```

## 5ã€dataset

```python
# ä¸‹è½½å¹¶ä¿å­˜åˆ°æœ¬åœ°
import datasets
data = datasets.load_dataset('wikitext', 'wikitext-2-v1')
data.save_to_disk('./wikitext_local')

# åŠ è½½
data = load_from_disk('./wikitext_local')
traindata =  data["train"]
testdata =  data["test"]

# ä¸‹è½½å¹¶ä¿å­˜åˆ°æœ¬åœ°
from datasets import load_dataset
traindata = load_dataset('allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train', use_auth_token=False)
valdata = load_dataset('allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation', use_auth_token=False)

traindata.save_to_disk('./c4_train')
valdata.save_to_disk('./c4_val')

```

- ä¸€ä¸ªå·¥å…·ä¸‹è½½
  - https://huggingface.co/docs/huggingface_hub/v0.17.1/en/guides/download

- ä¸‹è½½å·¥å…·

  ```python
  from huggingface_hub import snapshot_download
  import argparse
  import os
  
  parser = argparse.ArgumentParser()
  parser.add_argument('--repo_id', type=str)
  parser.add_argument('--save_dir', type=str, default='/ssd2/llm_zoo/')
  args = parser.parse_args()
  save_path = os.path.join(args.save_dir, args.repo_id.split('/')[-1])
  
  snapshot_download(repo_id=args.repo_id, local_dir=save_path, local_dir_use_symlinks=False)
  ```

  