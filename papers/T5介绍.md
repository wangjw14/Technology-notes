## 2、T5介绍

- T5是一个将所有的NLP任务，都抽象成一个text to text任务的transformer模型。
- 论文中，同时提出了C4数据集，有大约750G的英文数据和其他语言的数据。
- 数据集中，包含的多语言数据比较少，因此T5在翻译任务上表现较差。

![img](https://picx.zhimg.com/80/v2-9bc1a529196895375371a42e185125d0_1440w.png)

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

在T5的论文中，作者提到，当计算量扩大4倍，无论将这些计算量变成更多的训练数据，训练epoch，还是模型参数，都不重要，模型在log perplexity上都会线性下降。这一点和Scaling Laws结论一致。T5-11B，当时在除翻译任务外的其他任务上，都达到了SOTA。

2020年，T5的参数量达到了11B。2020年，GPT3参数量达到了175B。2022年，PaLM的参数量达到了540B。那么，Scaling的极限是什么？