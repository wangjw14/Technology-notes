# 腾讯广告大赛2020

## 前十选手的答辩 

- Target encode：首先统计每个广告的用户群体的target encode(对于age，我们将其 onehot以后的十维属性分别进行统计)，然后对用户下所有广告(忽略低频广告)的target encode求mean作为用户的target encode特征。
- 时间步上的Dropout
- Transformer

- 对于过长的序列进行截断
- 混合不同的embedding
- 计数特征
- 通过EDA发现实际数据的时间、节假日等信息
- 数据增强：序列shuffle
- 

### 关于模型选择的思路

- **主体富信息**：

  - 比赛中，广告是一个实体，而不仅仅是一个属性，在推荐系统中，一个实体所能代表的信息，要远远超过很多的标签。比如，推荐物品时，利用用户点击的上一个物品来进行推荐，要远比100个用户标签推荐，有更好的效果。因此，如何很好的用模型表征一个实体，就显得非常重要（如何训练一个embedding）。这也是NLP的相关方法，能够完胜树模型的原因。而同时采用NN的模型的时候，使用更好的预训练的embedding，也会带来更大的效果提升。
  - 主体富信息的实现有一个限制条件，就是实体要出现一定的次数才行，否则如果数据太稀疏，得到的结果无法泛化。

- 关于数据的insight：

  - 对于一个用户的广告点击序列，存在信息的冗余，比如，10个广告id的序列，就可以判断一个用户的信息，但是实际序列可能有30个广告id。剩余的广告id就是冗余的信息。（**样本信息过载**）
  - 广告出现的顺序对于最后的预测也没有影响。（**时间弱相关**）
  - 前50%的准确率要大于后50%的准确率，前期冷启动，后期加入了用户行为信息。

- 针对以上的特点：可以对数据进行shuffle，并使用滑窗的方式，实现数据增强。

- 数据的稀疏性问题：

  - 很多广告的出现次数都很少，在分布较为稀疏时， 往往基于低频特征无法很好的学习到id的具体信息。所以需要稠密化转化。
  - 要让每一条数据都发挥其最大的功能，实现更好的泛化。一个经典场景如下：
    - 一个序列中，可能有`ad1, ad2, ... , ad10` 十个id，但是`ad1` 和 `ad10` 具有强标签相关信息，其他节点若相关或者无关。因此更多的信息（权重）注入到这两个节点  ，形成信息孤岛，其他的广告并没有发挥更多的作用，形成信息空岛。此时，要将 maxpooling 转换为 drop max pooling，让除了`ad1` 和 `ad10` 的其他节点，学习和target的关系，最大化一条数据的作用。（低频词依赖周围环境的信息注入）

- 为什么BERT适合这个场景

  - 主体富信息（使用masked language model去建模）
  - 样本信息过载
  - 时间弱相关
  - w2v效果优异，embedding很适合这个场景
  - 低频词依赖周围环境的信息注入
  - 当下最牛的预训练模型

- BERT模型太大，跑不动怎么破？

  -  BERT 可以实现将词级别的完整信息注入，理想情况下可获得单个词的丰富的多维度信息，而针对当前场景，是否可以实现一种只将 **target** 紧密相关的信息注入的方法?从而大幅度降低模型规模。
  - Focus/target BERT

- 作者的总结：

  - 样本即特征，特征即标签: 

    1. 信息损失等较坏情况下具有良好预测能力。每条样本都应为此付出贡献。 
    2. 特征或特征之间应该具有相互备份容灾的能力，具有丢失情况下的恢复能力。这个过程中形成的 相互记忆的中间态，具有桥梁的作用，具有更强的泛化能力。

  - 谨防信息**孤岛\空岛**:

    1、稀疏实体富信息现象，容易形成信息孤岛、空岛。 如何将此类信息拆解分发或者注入。 是接下来研究的重点。从而实现由记忆到泛化的转变。

  - 特征是稀疏的，但是特征和特征之间的联系是稠密的。用户可以是稀疏的，但是用户和用户之间的联系是稠密的。利用这些连接，可以学到稠密的embedding ，有更好的泛化性。 





### 评委提问

- 数据稀疏如何处理
- 模型的选择的思路
- 用户的一些bais，比如有的用户会所有广告都点击，而有的人所有的广告都不点击。如何让模型对这一情况进行捕捉？
  - 选手回答：从数据质量的角度来评估。有些序列比较平滑（真实性高），有些序列不平滑，代表可能有更多的真实性差或信息量丰富。具体需要根据业务进行评估和后续处理。
- 是否可以根据CTR预测，正向和反向同时进行学习的和预测？



### EDA

- ads

  ```
  train:  2481135
  test:   2618159
  unique: 3412772
  ```

  




### 代码中的疑问

- train BERT的时候，LM的输入输出vocab有轻微的不同，是故意这样的吗？train_df和train_data，以及是否要满足最少5次的要求。
- 做5折交叉的时候，为什么要将 `user_id, creative_id` 进行去重。



### 测试word2vec的window参数

- 取50000 train和 50000 test，进行快速尝试。看window size对算法的影响

| window size | accuracy | 备注       |
| ----------- | -------- | ---------- |
| 8           | 1.3176   |            |
|             |          |            |
| 10          | 1.3268   | no shuffle |
| 10          | 1.3288   | 1 shuffle  |
| 10          | 1.3333   | 2 shuffle  |
|             |          |            |
| 20          | 1.3248   | no shuffle |
| 20          | 1.3384   | 1 shuffle  |
| 20          | 1.3352   | 2 shuffle  |
|             |          |            |
| 50          | 1.3376   | no shuffle |
| 50          | 1.3284   | 1 shuffle  |
| 50          | 1.338    | 2 shuffle  |

- 训练数据895000，测试数据5000。

| window size | shuffle | lr   | accuracy | 备注                    |
| ----------- | ------- | ---- | -------- | ----------------------- |
| 90          | 2       | 5e-4 | 1.429    | with target encoding    |
| 90          | 2       | 5e-4 | 1.4208   | without target encoding |

