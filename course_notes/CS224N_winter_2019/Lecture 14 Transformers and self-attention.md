# Lecture 14 Transformers and self-attention

- The start point, always ask the question: what is the structure of my dataset or what are the symmetries in my dataset. And is there a model that exists that has the inductive(归纳的) biases to model these properties that exist in my dataset.
- Deep learning is all about representation learning.
- Pandemoniums by Oliver Selfridge (recommend to read)
- Self Attention: re-express yourself in certain terms by a weighted combination of your entire neighborhood
- Add feed-forward layers to compute new features for you
- Transformer outperforms LSTM, but we are not sure that transformer is an architecture with better expressivity than a LSTM. Maybe all we did just build an architecture that was good for SGD. On the other hand, transformers build all pairwise connections that modeling very clear relationships between any two words.







