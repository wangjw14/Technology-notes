# 03.15-03.21 回顾

### 数据结构相关

- array(数组)的特点：
  1. 从0开始的索引
  2. 有固定长度
  3. 内存中连续的表示
  4. 边界检查

- Shuffle一个数组

  ```python
  import random
  def shuffle_array(arr):
    for i in range(len(arr)):
    	j = random.randint(0,i)
    	arr[i],arr[j] = arr[j],arr[i]
  ```

  一个长度为n的数组，shuffle之后，一共有n！种可能性，j在迭代的过程中，分别有1,2,...,n种可能性，最终一共产生n！种可能性，使得最后每一种组合出现的概率相等。

- n以内的所有素数

  ```python
  def count_primes(n):
  	is_primes = [True] * n
  	i = 2
  	while i*i<n:  # 确认边界条件
  		if is_primes[i]:
  			j = i*i
  			while(j<n):
  				is_primes[j] = False
  				j+=i
  		i += 1
  	primes = [i for i in range(2,n) if is_primes[i]]
  	print(primes)
  ```

  时间复杂度：$O(n\log\log n)$ 

  https://zhuanlan.zhihu.com/p/84523764

- 螺旋数组

  ```python
  class Solution:
      def spiralOrder(self, matrix: List[List[int]]) -> List[int]:
          if len(matrix)==0:
              return []
          up = 0
          down = len(matrix)-1
          left = 0
          right = len(matrix[0])-1
  
          results = []
          direction = 0
          while(up<=down and left<=right):
              if direction == 0:
                  for i in range(left, right+1):
                      results.append(matrix[up][i])
                  up += 1
              elif direction == 1:
                  for i in range(up, down+1):
                      results.append(matrix[i][right])
                  right -= 1
              elif direction == 2:
                  for i in range(right,left-1,-1):
                      results.append(matrix[down][i])
                  down -= 1
              else:
                  for i in range(down,up-1,-1):
                      results.append(matrix[i][left]) 
                  left += 1
              direction = (direction+1) % 4
          return results
  ```

- 杨辉三角

  ```python
  class Solution:
      def generate(self, numRows: int) -> List[List[int]]:
          result = []
          for i in range(numRows):
              temp = []
              for j in range(i+1):
                  if j==0 or j ==i:
                      temp.append(1)
                  else:
                      temp.append(result[i-1][j]+result[i-1][j-1])
              result.append(temp)
          return result
  ```

- 从n-1个不重复的位于1-n之间的数中找出缺失的那个数

  - 使用异或
  - 使用填位法

- 三门问题：要换，之前概率1/3，换之后概率2/3

- 新生儿问题：4/7

- 掷骰子喝酒问题：不会

### ELMo

- 为以后查阅方便，elmo相关部分和上周的elmo相关内容合并。见上周总结。

### Normalization

- 为什么需要normalization

  - Internal Covariate Shift的存在
    1. 上层参数需要不断适应新的数据分布。不同层之间存在耦合现象，降低学习速度。
    2. 下层的输入变化可能趋向于变大或变小，导致上层进入梯度饱和区，学习过早停止。
    3. 每层的更新都会影响到其它层，每层的参数更新策略需要更谨慎。
  - 解决方案，白化（whitening）—> 使得数据可以独立同分布（independent and identically distributed）

- Normalization的通用框架
  $$
  h=f(g\cdot{{x-\mu}\over\sigma} +b)
  $$

  - $\mu$是平移参数，$\sigma$是缩放参数，$b$是再平移参数，$g$是再缩放参数。

  - 再平移和再缩放的目的是为了保证模型的表达能力不因为normalization而下降。
  - 旧参数是$\mu$和$\sigma$，新参数$b$和$g$。旧参数由底层的神经网络的复杂关联决定，新参数通过梯度下降来学习，两者不会相互抵消，但同时解除了不同层时间的耦合，从而简化了神经网络的训练。

- Batch norm的计算方式，对于神经网络中的第$l$层，有
  $$
  \begin{aligned}
  &Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]}\\
  &\mu=\frac{1}{m} \sum_{i=1}^{m} Z^{[l(i)}\\
  &\sigma^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(Z^{[l](i)}-\mu\right)^{2}\\
  &\tilde{Z}^{[l]}=\gamma \cdot \frac{Z^{[l]}-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\beta\\
  &A^{[l]}=g^{[l]}\left(\tilde{Z}^{[l)}\right)
  \end{aligned}
  $$

  - 针对单个神经元进行normalization，利用mini-batch来计算均值和方差。
  - 加入$\gamma$和$\beta$的作用是减小batch norm对网络表达能力的影响

  - 在进行normalization的过程中，由于我们的规范化操作会对减去均值，因此，偏置项$b$可以被忽略掉或可以被置为0，即$BN(Wu+b)=BN(Wu)$ 
  - 适用场景：mini-batch比较大，数据分布比较接近。训练之前，数据要进行充分的shuffle。

  - test阶段，使用训练时不同batch上的均值和方差对全体数据的均值和方差进行无偏估计

  $$
  \begin{align}
  &\mu_{t e s t}=\mathbb{E}\left(\mu_{b a t c h}\right) \\
  &\sigma_{\text {test}}^{2}=\frac{m}{m-1} \mathbb{E}\left(\sigma_{\text {batch}}^{2}\right) \\
  
  &B N\left(X_{t e s t}\right)=\gamma \cdot \frac{X_{t e s t}-\mu_{t e s t}}{\sqrt{\sigma_{t e s t}^{2}+\epsilon}}+\beta
  \end{align}
  $$

  - batch norm的作用
    - BN使得每一层之间的数据分布相对稳定（解藕）,加速模型的学习
    - 使得模型对网络中的参数不那么敏感，简化调参过程，使得学习更加稳定
    - BN是的网络允许使用饱和性激活函数（sigmoid，tanh等），缓解梯度消失问题
    - 具有一定的正则化效果（使用mini-batch的mean/variance作为总体样本统计量估计，加入了随机噪声）

- Layer normalization

  - 考虑一层神经元所有维度的输入，计算该层的均值和方差。
  - LN针对单个训练样本进行，不依赖其他的数据，避免了BN中受mini-batch数据分布影响的问题，可用于小mini-batch的场景，动态网络和RNN。
  - LN不需要保存mini-batch的均值和方差，节省了额外的存储空间。
  - 缺点：LN针对一整层的神经元进行转换，使得所有输入都在一个区间范围内，如果不同的特征属于相似的类别（如颜色和大小），那么LN可能会降低模型的表达能力。

- Weight normalization

  针对参数进行normalization
  $$
  w=g\cdot \hat v=g\cdot\frac v{||v||}
  $$
  相当于$\sigma=||v||,\ \mu=0,\ b=0$。WN的规范化不直接使用输入数据进行计算，避免了BN过于依赖mini-batch的不足，以及LN每层唯一转换器的限制，同时也可以用于动态网络结构。

- Cosine Normalization

  - $f_w(x)=w\cdot x$，针对乘法进行规范化，改为以下形式
    $$
    f_w(x)=\cos\theta=\frac{w\cdot x}{||w||\cdot||x||}
    $$
    缺点：原始的内积计算，几何意义是输入向量在权重向量上的投影，既包含夹角信息，也包含向量的scale信息。去掉scale信息，会导致表达信息下降。

- Normalization的性质

  - 权重伸缩不变性
    $$
    Norm(W^\prime x)=Norm(Wx)
    $$
    其中，$W^\prime=\lambda W$ 

  - 数据伸缩不变性
    $$
    Norm(Wx^\prime)=Norm(Wx)
    $$
    其中，$x^\prime=\lambda x$，且仅对BN、LN、CN成立，对于WN不成立。

- 参考资料

  - [Batch Normalization原理与实战](https://zhuanlan.zhihu.com/p/34879333)

  - [详解深度学习中的Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)


## Plan of next week

- 本周完成内容较少，未达成既定目标。下周继续完成该目标。


