# 04.05-04.11 回顾

- 图模型的一个结构
  1. 生成式模型和判别式模型
  2. 概率图模型（Graphical Models）
  3. 朴素贝叶斯（NB）
  4. 隐马尔可夫模型（HMM）
  5. 最大熵模型（MEM）
  6. 最大熵马尔可夫模型（MEMM）
  7. 条件随机场（CRF）
  
- HMM、MEMM、CRF的对比总结

  [知乎-从HMM、MEMM到CRF](https://zhuanlan.zhihu.com/p/71190655) 

# EM 算法

- Jensen不等式
  $$
  f(\theta x_1+(1-\theta)x_2)\le \theta f(x_1)+(1-\theta)f(x_2)
  $$
  其中，$f(x)$是凸函数（若是凹函数，则是$\ge$）。将上述情况进行推广，则有
  $$
  f(\theta_1x_1+\cdots+\theta_kx_k)\le\theta_1f(x_1)+\cdots+\theta_kf(x_k)
  $$
  其中，$\theta_1,\cdots,\theta_k\ge0,\theta_1+\cdots+\theta_k=1$ 。

  将$\theta_1,\cdots,\theta_k$ 当作概率，则有
  $$
  f(E[x])\le E [f(x)]
  $$
  连续的情况：
  $$
  f\left(\int_Sp(x)xdx \right)\le\int_Sp(x)f(x)dx
  $$
  其中，$p(x)\ge0 \  \text{on}\ S\subseteq \text{dom} \ f,\int_Sp(x)dx=1$

- EM算法

  - 输入：观测变量$Y$，隐变量$Z$，联合分布$P(Y,Z|\theta)$，条件分布$P(Z|Y,\theta)$ 
  - 输出：模型参数$\theta$

  1. 选择初始参数$\theta^{(0)}$，开始迭代

  2. E step：第$i$次迭代的参数为$\theta^{(i)}$ ，则在第$i+1$步，计算
     $$
     Q(\theta,\theta^{(i)}) = E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]
     =\sum_Z\log P(Y,Z|\theta)P(Z|Y,\theta^{(i)})
     $$
     也就是求给定参数$\theta^{(i)}$和观测变量$Y$的情况下，隐变量$Z$的条件概率分布的期望。

  3. M step：求使$Q(\theta,\theta^{(i)})$最大化的$\theta$，作为第$i+1$次迭代的参数
     $$
     \theta^{(i+1)} = \arg\max_{\theta}Q(\theta,\theta^{(i)})
     $$

  4. 重复2、3步，直到收敛。

- EM 算法的导出

  - 对于一个含有隐变量的概率模型，要极大化针对观测数据$Y$的对数似然函数
    $$
    L(\theta)=\log P(Y|\theta)=\log \sum_ZP(Y,Z|\theta)=\log \sum_Z P(Y|Z,\theta)P(Z|\theta)
    $$
    第$i$次迭代的参数$\theta^{(i)}$ 
    $$
    \begin{align}
    L(\theta)&=\log \sum_ZP(Y,Z|\theta) \\
    &=\log\sum_Z P(Z|Y,\theta^{(i)})\frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})} \\
    &\ge\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})}
    \end{align}
    $$
    则迭代之后参数的值为
    $$
    \begin{align}
    \theta^{(i+1)} &=\arg \max_{\theta} L(\theta)\\
    &=\arg \max_{\theta}\left( \sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})} \right)\\
    &=\arg \max_{\theta}\left(\sum_ZP(Z|Y,\theta^{(i)})\log P(Y,Z|\theta) \right)\\
    &=\arg \max_{\theta}\ Q(\theta,\theta^{(i)})
    \end{align}
    $$
    分母中的部分可以去掉的依据是，和$\theta$无关的常数项，且在log函数中，求导时是0。

  - 疑问：《统计学习方法》为什么优化$L(\theta)-L(\theta^{(i)})$ 而不是$L(\theta)$ 。只从结果看，结论是一样的。

- EM 算法的特点：

  - 不能保证全局最优，只能保证局部最优。初始值敏感，实际中会多次运行取最优解。
  - 迭代过程是严格递增的，必然会收敛。

# HMM

- HMM的基本假设
  1. 齐次马尔可夫假设：任意 $t$ 时刻的状态只和前一时刻 $t-1$ 的状态有关。
  2. 观测独立性假设：任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。
- HMM的3类问题
  - 概率计算问题：计算 $P(O|\lambda)$ 
  - 参数估计问题：计算 $\lambda=(A,B,\pi)$ 
  - 预测问题：已知参数 $\lambda$ 和观测序列 $O$ ，求状态序列 $I$

- 概率计算问题

  - 直接计算法：
    $$
    \begin{align}
    P(O|\lambda)&=\sum_IP(I|\lambda)P(O|I,\lambda) \\
    &=\sum_I \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdots a_{i_{T-1}i_T}b_{i_T}(o_T)
    \end{align}
    $$
    要计算所有的 $I$ 的可能性，若状态一共有N种，则共有 $N^T$ 种可能性，算法的复杂度为 $O(TN^T)$ ，复杂度太高。

  - 前向算法

    - 定义前向概率
      $$
      \alpha_t(i)=P(i_t=q_t,o_1,o_2,\cdots,o_t|\lambda)
      $$
      则有：

      1. 初始值
         $$
         \alpha_1(i)=\pi_ib_i(o_1)
         $$

      2. 递推
         $$
         \alpha_{t+1}(i)  = \left[\sum_j \alpha_t(j)a_{ji}\right]b_i(o_{t+1})
         $$

      3. 终止
         $$
         P(O|\lambda)=\sum_i\alpha_T(i)
         $$

    - 复杂度：$O(TN^2)$ ，复杂度降低的原因在于引用了前一时刻的结果，避免了重复计算。

  - 后向算法

    - 后向概率
      $$
      \beta_t(i)=P(o_{t+1},o_{t+2},\cdots,o_T|i_t=q_t,\lambda)
      $$

      1. 初始态
         $$
         \beta_T(i)= 1
         $$

      2. 递推
         $$
         \beta_t(i)=\sum_j a_{ij}b_j(o_{t+1})\beta_{t+1}(j)
         $$

      3. 终止
         $$
         P(O|\lambda)=\sum_i \pi_ib_i(o_1)\beta_1(i)
         $$

  - 前向-后向算法
    $$
    P(O|\lambda)=\sum_i\sum_j \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)
    $$

    - 给定 $\lambda$ 和 $O$ ，在时刻 $t$ 处于状态 $q_i$ 的概率
      $$
      \gamma_t(i)=P(i_t=q_i|O,\lambda) = \frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}\\
      =\frac{\alpha_t(i)\beta_t(i)}{\sum_j\alpha_t(j)\beta_t(j)}
      $$

    - 给定 $\lambda$ 和 $O$ ，在时刻 $t$ 处于状态 $q_i$ ，且在 $t+1$ 处于状态 $q_j$ 的概率
      $$
      \xi_t(i,j)=P(i_t=q_i,i_{t+1}=q_j|O,\lambda)=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{P(O|\lambda)} \\
      =\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_i\sum_j\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}
      $$

    - 观测 $O$ 下状态 $i$ 出现的期望值
      $$
      \sum_{t=1}^T \gamma_t(i)
      $$

    - 观测 $O$ 下由状态 $i$ 转移出的期望值 
      $$
      \sum_{t=1}^{T-1} \gamma_t(i)
      $$

    - 观测 $O$ 下由状态 $i$ 转移到状态 $j$ 的期望值 
      $$
      \sum_{t=1}^{T-1}\xi_t(i,j)
      $$

- 参数估计问题

  - 完全数据下参数估计

    1. 转移概率的估算
       $$
       \hat a_{ij}= \frac{A_{ij}}{\sum_jA_{ij}}
       $$
       其中 $A_{ij}$ 是从状态 $i$ 转移到状态 $j$ 的频数。

    2. 观测概率的估算
       $$
       \hat b_j(k)=\frac{B_{jk}}{\sum_kB_{jk}}
       $$
       其中 $B_{jk}$ 是从状态 $j$ 到观测 $k$ 的频数。

    3. 初始概率的估算
       $$
       \hat\pi_i = \frac {\#(q_i) }{\sum_j\# (q_j)}
       $$

  - 不完全数据的Baum-Welch算法

    - 输入：观测数据 $O=(o_1,o_2,\cdots,o_T)$

      1. 初始化：对 $n=0$ ，选取 $\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})$ .

      2. 递推，针对 $n=1,2,\cdots$ 
         $$
         \begin{align}
         a_{ij}^{(n+1)}&=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)} \\
         b_j(k)^{(n+1)}&=\frac{\sum_{t=1,o_t=k}^T\gamma_t(j)}{\sum_{t=1}^T\gamma_t(j)}\\
         \pi_i^{(n+1)}&=\gamma_i(i)
         \end{align}
         $$

      3. 终止，得到参数$\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})$

- 预测问题（Viterbi算法）

  - 定义时刻 $t$ 状态为 $i$ 的所有路径中的概率最大值为：
    $$
    \delta_t(i)=\max_{i_1,i_2,\cdots,i_{t-1}}P(i_t=i,i_{t-1},\cdots,i_1,o_t,\cdots,o_1|\lambda)
    $$
    则递推公式为：
    $$ {align}
    \begin{align}
    \delta_{t+1}(i)&=\max_{i_1,i_2,\cdots,i_{t}}P(i_{t+1}=i,i_{t},\cdots,i_1,o_{t+1},\cdots,o_1|\lambda)  \\
    &= \max_j[\delta_t(j)a_{ji}]b_i(o_{t+1})
    \end{align}
    $$ {align}

  - 定义时刻 $t$ 状态为 $i$ 的所有路径中，概率最大的路径的第 $t-1$ 个结点为
    $$
    \psi_t(i)=\arg\max_j[\delta_{t-1}(j)a_{ji}]
    $$

  - Viterbi算法

    - 输入： $\lambda=(A,B,\pi)$ 和观测 $O=(o_1,o_2,\cdots,o_T)$ 

    - 输出： 最优路径 $I^*=(i_1^*,i_2^*,\cdots,i_T^*)$ 

      1. 初始化
         $$
         \delta_1(i)=\pi_ib_i(o_1)\\
         \psi_1(i)=0
         $$

      2. 递推
         $$
         \delta_{t}=\max_j[\delta_{t-1}(j)a_{ji}]b_i(o_{t}) \\
         \psi_t=\arg\max_j[\delta_{t-1}(j)a_{ji}]
         $$

      3. 终止
         $$
         P^*=\max_i\delta_T(i)\\
         i_T^*=\arg\max_i \delta_T(i)
         $$

      4. 最优路径回溯
         $$
         i_t^*=\psi_{t+1}(i^*_{t+1})
         $$

    - 求得最优路径 $I^*=(i_1^*,i_2^*,\cdots,i_T^*)$ 



# CRF

- 生成模型和判别模型

  - 生成模型：$p(x,y)$ 联合概率
  - 判别模型： $p(y|x)$  条件概率

- Log-linear model 

  - 包括：(1) Logistic Regression    (2) Conditional Random Field

  - 一般形式
    $$
    p(y|x;w)=\frac{\exp\sum_{j=1}^Jw_jF_j(x,y)}{Z(x,w)}
    $$
    其中，$F_j(x,y)$ 是feature function，$J$ 是特征的个数。

- Multinomial Logistic Regression

  $$
  F_j(x,y)=x_i\cdot I(y=c)
  $$
  其中，$x\in\R^d$ ，$I(y=c)$ 是indicator function。经过一些变换可以得到Multinomial Logistic Regression，也就是softmax。

- 概率无向图
  - 概率无向图的定义：

    设有联合概率分布 $P(Y)$ ，由无向图 $G=(V,E)$ 表示，图中结点表示随机变量，边表示随机变量之间的依赖关系。如果 $P(Y)$ 满足成对、局部或全局马尔可夫性，则称此联合分布为***概率无向图模型***，或者***马尔可夫随机场***。（就是说，结点所代表的随机变量，只和和它有边相连的结点相关，其他结点无关。）

  - 概率无向图的因子分解：

    无向图中任何两个结点都有边连接的结点子集叫做***团***（clique）。如果一个团，没发加入任何一个结点形成更大的团，则成为最大***团***（maximal clique）。

  - 概率无向图的联合概率分布可以表示为以下形式：
    $$
    p(Y)=\frac1Z\prod_C\Psi_C(Y_C)\\
    Z=\sum_Y\prod_C\Psi_C(Y_C)\\
    \Psi_C(Y_C)=\exp\{-E(Y_C)\}
    $$
    其中，$C$ 是无向图的最大团，$Y_C$ 是 $C$ 对应结点的随机变量，$Z$ 是规范化因子，$\Psi_C(Y_C)$ 是势函数，（势函数要求是严格正函数）。乘积是在无向图所有的最大团上进行的。

- 条件随机场

  - 条件随机场

    设 $X$ 和 $Y$ 是随机变量，$P(Y|X)$ 是给定X的条件下Y的条件概率分布。若 $Y$ 构成的无向图 $G=(V,E)$ 表示的马尔可夫随机场，即
    $$
    P(Y_v|X,Y_w,w\ne v)=P(Y_v|X,Y_w,w\sim  v)
    $$
    对任意结点成立，则称条件概率分布$P(Y|X)$ 为条件随机场。$w\sim  v$ 表示所有和 $v$ 有连接的结点，$w\ne v$ 表示除 $v$ 以外的所有结点。

    - 线性链条件随机场

      设 $X=(X_1,X_2,\cdots,X_n)$ 和 $Y=(Y_1,Y_2,\cdots,Y_n)$ 均是线性链表示的随机变量序列，若条件概率分布 $P(Y|X)$ 满足马尔可夫性
      $$
      P(Y_i|X,Y_1,\cdots,Y_{i-1},Y_{i+1},\cdots,Y_n)=P(Y_i|X,Y_{i-1},Y_{i+1})
      $$
      则称 $P(Y|X)$ 为线性链条件随机场。（上式中，当 $i=1,n$ 时，只考虑单边。）

- 线性链CRF的参数化表示
  $$
  p(y|x)=\frac1{Z(x)}\exp\left(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i) \right)\\
  {Z(x)}=\sum_y\exp\left(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i) \right)
  $$
  其中，$t_k$ 和 $s_l$ 是特征函数，且分别是转移特征和状态特征。特征函数一般是indicator function。特征函数都依赖于位置，都是局部特征函数。 $\lambda_k$ 和 $\mu_k$ 是权值，$Z(x)$ 是规范化因子。
  
- 简化版表示
  
    - 将 $t_k$ 和 $s_l$ 写成统一的形式 $f_k(y_{i-1},y_i,x,i)$  ， $\lambda_k$ 和 $\mu_k$ 写成统一的形式 $w_k$ ，则有：
      $$
      f_k(y,x)=\sum_if_k(y_{i-1},y_i,x,i) \\
      p(y|x)=\frac1{Z(x)}\exp\sum_kw_kf_k(y,x)\\
      Z(x)=\sum_y\exp\sum_kw_kf_k(y,x)
      $$
      
    - 进一步简化
      $$
      F(y,x)=(f_1(y,x),f_2(y,x),\cdots,f_k(y,x))\\
      w=(w_1,w_2,\cdots,w_k)^T\\
      p(y|x)=\frac{\exp( w\cdot  F(y,x))}{Z(x)} \\
      Z(x)=\sum_y\exp(w\cdot F(y,x))
      $$
  
- CRF的矩阵形式

  假设 $P(y|x)$ 是线性链CRF，引入特殊的起点和终点状态$y_0=\text{start},y_{n+1}=\text{stop}$ ，则有以下定义：
  $$
  W_i(y_{i-1},y_{i}|x)=\sum_{k=1}^K w_kf_k(y_{i-1},y_i,x,i)\\
  M_i(y_{i-1},y_{i}|x)=\exp\left(W_i(y_{i-1},y_{i}|x)\right)\\
  M_i(x)=[M_i(y_{i-1},y_i|x)]
  $$
  其中， $M_i(x)$ 是一个 $m$ 阶的矩阵。$M_i(y_{i-1},y_i|x)$ 是 $M_i(x)$ 中的一个元素。则，条件概率 $P(y|x)$ 可以写成：
  $$
  P(y|x)=\frac1{Z(x)}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)\\
  Z(x)= \sum_{y_{i-1},y_i}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)
  =(M_1(x)M_2(x)\cdots M_{n+1}(x))_{\text{start,stop}} 
  $$
  
- 概率计算问题

  - 前向算法

    1. 初始化
       $$
       \alpha_0(y_0|x)=\cases{1,\ \  y_0=\text{start}\\0,\ \ \text{otherwise}}
       $$

    2. 递推

    $$
    \alpha_i(y_i|x)=\alpha_{i-1}(y_{i-1}|x)M_i(y_{i-1},y_i|x)  \ \ \ \   (标量形式)\\
     \alpha_i^T(x)=\alpha_{i-1}^T(x)M_i(x)\ \ \ \  \  {(m维向量)}
    $$

  - 后向算法

    1. 初始化
       $$
       \beta_{n+1}(y_{n+1}|x)=\cases{1,\   \ y_{n+1}=\text{stop}\\0,\ \ \text{otherwise}}
       $$
    
    2. 递推

  $$
  \beta_i(y_i|x)=M_i(y_i,y_{i+1}|x)\beta_{i+1}(y_{i+1}|x)\ \ \ \   (标量形式)\\
    \beta_i(x)=M_{i+1}(x)\beta_{i+1}(x)\ \ \ \  \  {(m维向量)}
  $$

  - 前向-后向算法
    $$
    Z(x)=\alpha_n^T(x)\cdot \bold 1= \bold 1\cdot\beta_1(x)
    $$
    其中，$\bold 1$ 是元素都为1 的 $m$ 维向量。

  - 概率计算
    $$
    p(Y_i=y_i|x)=\frac{\alpha_i(y_i|x)\beta_i(y_i|x)}{Z(x)}\\
    p(Y_{i-1}=y_{i-1},Y_i=y_i)=\frac{\alpha_{i-1}(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}
    $$

  - 期望计算

    - 特征函数 $f_k$ 关于条件分布 $P(Y|X)$ 的期望
      $$
      \begin{align} 
      E_{P(Y|X)}(f_k)&=\sum_y P(y|x)f_k(y,x)\\
      &=\sum_i\sum_{y_{i-1},y_i}f_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}
      \end{align}
      $$
    
    - 特征函数 $f_k$ 关于联合分布 $P(X,Y)$ 的期望
      $$
      \begin{align}
      E_{P(X,Y)}(f_k)&=\sum_{x,y}P(x,y)f_k(y,x)\\
      &=\sum_x\tilde  P(X)\sum_yP(Y|X)\sum_if_k(y_{i-1},y_i,x,i)\\
      &=\sum_x\tilde P(X)\sum_i\sum_{y_{i-1},y_i}f_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}
      \end{align}
      $$


- 参数估计问题

  CRF用极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。能用在log-linear models上的求参方法都可以用过来。

- 预测算法

  - 给定CRF $P(Y|X)$ 和输入序列 $x$ ，求条件概率最大的输出 $y^*$ 
      $$
      \begin{align}
      y^*&=\arg\max_yP(y|x)\\
      &=\arg\max_y \frac{\exp(w\cdot F(y,x))}{Z(x)}\\
      &=\arg\max_y  \exp(w\cdot F(y,x))\\
      &=\arg\max_y  (w\cdot F(y,x))\\
      \end{align}
      $$

  - 作如下约定
      $$
      F_i(y_{i-1},y_i,x)=(f_1(y_{i-1},y_i,x,i),f_2(y_{i-1},y_i,x,i),\cdots,f_K(y_{i-1},y_i,x,i))^T
      $$

  - Viterbi算法

      输入：模型的特征向量 $F(y,x)$ 和权值向量 $w$ ，观测序列 $x=(x_1,x_2,\cdots,x_n)$ 

      输出：最优路径 $y^*=(y^*_1,y^*_2,\cdots,y^*_n)$ 。

      1. 初始化
         $$
         \delta_1(j)=w\cdot F_1(y_0=start,y_1=j,x)
         $$

      2. 递推
         $$
         \delta_i(l)=\max_j\left\{ \delta_{i-1}(j)+w\cdot F_i(y_{i-1}=j,y_i=l,x)\right\}\\
         \Psi_i(l)=\arg\max_j\left\{ \delta_{i-1}(j)+w\cdot F_i(y_{i-1}=j,y_i=l,x)\right\}
         $$

      3. 终止
         $$
         \max_y(w\cdot F(y,x))=\max_j\delta_n(j)\\
         y^*_n=\arg\max_j \delta_n(j)
         $$

      4. 最优路径回溯
         $$
         y^*_i=\Psi_{i+1}(y^*_{i+1})
         $$

      求得最优路径 $y^*=(y^*_1,y^*_2,\cdots,y^*_n)$ 。


## Plan of next week

- LSTM+CRF
- 分治 


