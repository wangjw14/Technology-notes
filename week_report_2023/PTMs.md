# PTMs

- 本文中的结论，都是截止2020-09-20。关于ernie 3.0的相关资料，见papers目录下的论文笔记文档。



- 我筛出了一批表现优秀的模型，包括：RoBERTa，Google T5，ALBERT，ELECTRA，XLNet，GPT3，BART，UNILM v2, StructBert，MacBert。这些模型要么在某个榜单前几名，要么论文实验结果显示效果非常好，二者占其一。这里面，GPT3是个纯生成模型，ELECTRA相对而言方法比较特殊，在后面我会单独说下它。需要说明的是，ERNIE和NEZHA模型，效果也是非常好的，能够排在某些榜单前列。
- 原始Bert模型的基础上，RoBERTa通过实验，证明了如下几点：
  1. 进一步增加预训练数据数量，能够改善模型效果；
  2. 延长预训练时间或增加预训练步数，能够改善模型效果；
  3. 急剧放大预训练的每个Batch的Batch Size，能够明显改善模型效果；
  4. 拿掉预训练任务中的Next Sentence Prediction子任务，它不必要存在；
  5. 输入文本的动态Masking策略有帮助；

- 促进模型性能快速提高的因素，主要包含下列几方面
  - 更高质量、更多数量的预训练数据。
    - 保证预训练数据质量的前提下，数据规模越大模型效果越好。
  - 增加模型容量及复杂度。
    - 所谓增加模型容量及复杂度，指的是增加Transformer模型的参数量，一般而言，模型容量越大，模型的表达能力越强。
  - 更充分地训练模型
    - 放大Batch Size、增加预训练步数，就是RoBERTa做的那两个事情。
  - 有难度的预训练任务
    - 对于单词级的Mask语言模型来说，Span类的预训练任务效果最好。
    - SOP（Sentence Order Prediction）是有效的句子级预测任务。

|            | 增加更多数据 | 提升模型容量 | 增大batch size | 增大训练步数 | 单词任务（span） | 句子任务（SOP） |
| ---------- | ------------ | ------------ | -------------- | ------------ | ---------------- | --------------- |
| RoBERTa    | ✔            |              | ✔              | ✔            |                  |                 |
| T5         | ✔            | ✔            | ✔              | ✔            | ✔                |                 |
| ALBERT     | ✔            | ✔            | ✔              | ✔            |                  | ✔               |
| GPT3       | ✔            | ✔            | ✔              | ✔            |                  |                 |
| XLNet      | ✔            |              | ✔              | ✔            | ✔                |                 |
| MacBERT    | ✔            |              | ✔              | ✔            | ✔                |                 |
| StructBERT | ✔            |              | ✔              | ✔            | ✔                | ✔               |
| BART       | ✔            |              | ✔              | ✔            | ✔                | ✔               |
| UniLM v2   | ✔            |              | ✔              | ✔            | ✔                |                 |



- 对于语言理解类任务来说，估计Google T5和ALBERT是效果最好的预训练模型；（截止2020-09-20 ）
- 而对于语言生成类的任务来说，估计GPT3是效果最好的模型。（截止2020-09-20 ）