# 04.26-05.02 回顾

- python程序调用的可视化网址：http://www.pythontutor.com/visualize.html#mode=edit

# 树

- 树的遍历

  - 先序遍历
  - 后续遍历
  - 二叉树的中序遍历（顺序遍历）

- 二叉树

  - 每个内部节点之多有两个孩子；节点的孩子是有序对
  - 满二叉树：深度为k的二叉树，有$2^n-1$ 个节点
  - 完全二叉树：完全二叉树的度为1的节点只有0个或1个（取决于最后一层节点数为奇数还是偶数）

- 二叉搜索树（顺序二叉树）：左子树的值均小于根节点的值，右子树的值均大雨根节点的值。（递归定义）

- 二叉搜索树的增insert

  ```python
  def insert(value):
    return __insert(root, value)
  def __insert(node, value):
    if node is None:
      return node(value)
    if value == node._item:
      pass
    else:
      if value < node._item:
        node._left = __insert(node.left, value)
       else:
        node._right = __insert(node.right, value)
    return node 
  ```

  时间复杂度$O(\log n)$ ，最差情况为$n$ 。

- 二叉搜索树的删delete

  ```python
  def remove(value):
    return __remove(root, value)
  def __remove(node, value):
    if node is None:
      return None
    if key < node._item:
      node._left = __remove(node._left, value)
    elif key > node._item:
      node._right = __remove(node._right, value)
    else:
      if node._left is None:
        node = node._right
      elif node._right is None:
        node = node._left
      else:
        node._item = __get_max(node._left)
        node._left = __remove(node._left, node._item)
    return node
  def __get_max(node):
    if node is None:
      return None
    while node._right is not None:
      node = node._right
    return node._item
  ```

- 二叉搜索树的改update

  查找之后修改，时间复杂度$O(\log n)$ ，最差情况为$n$ 。

- 二叉搜索树的查search

  类似二分查找，时间复杂度$O(\log n)$ ，最差情况为$n$ 。（递归模式、循环模式）

  ```python
  def get(value):
    return __get(root, value)
  def __get(node, value):
    if node is None:
      return None 
    if node._item == value:
      return node._item:
    elif value > node._item:
      return __get(node.right, value)
    else:
      return __get(node.left, value)
  ```

- 平衡二叉树：
  - 任何节点的两个子树的高度相差最多一个。
  - AVL树和红黑树都是平衡二叉树。
  - 平衡二叉树的增删改查时间内复杂度都是$O(\log N) $ 



# NLP相关

- 倒排索引 Inverted Index。建立一个映射

  ```python
  {word : [list of docs contian this word]}
  ```

  通过筛选出包含query词的docs，大幅减少了需要计算相似度的doc candidates数量，从而优化计算所需要的时间。

- Noisy Channel Model
  $$
  p (\text{ text | source}) \propto p(\text{ source | text })\ p(\text{text})
  $$
  形如上式的模型，可称为noisy channel model。可用于：语音识别、机器翻译、拼写纠错、OCR、密码破解等。

- 搜索技巧，如果想要了解question answering的最新消息：

  ```
  question answering survey
  question answering tutorial slides
  question answering tutorial slides ACL
  question answering tutorial slides EMNLP 
  ```

# 信息抽取技术

- **信息抽取**包括：**实体抽取**和**关系抽取**。
- 文本的**特征工程** 
  1. Bag-of-word features
     - 当前词，前后词，前面第二个词，后面第二个词
     - bigram，trigram
  2. 词性
     - 当前词词性，前后词词性，前面第二个词词性，后面第二个词词性
     - n-gram词性
  3. 前缀后缀
     - 当前词前后缀，前后词前后缀，前面第二个词前后缀，后面第二个词前后缀
  4. 当前词的特性
     - 词长，多少个大写字母，是否大写开头，是否包含'-'，是否包含数字，前面词是否包含大写...
  5. Stemming 之后，重复1中的特征
- 特征类型
  - 类别特征（categorical feature）--> one-hot encoding
  - 序数特征（ordinal feature）
  - 连续型特征（continuous feature）
    - A. 归一化（0-1之间，标准正态分布）之后使用
    - B. 离散化后使用
- 特征one-hot encoding时，如果某些特征特别稀疏，可以适当进行合并（建立在数据分析，可视化基础上）

### 命名实体识别NER

- English toolkits: NLTK NE, Spacy, Stanford Parser
- Chinese toolkits: HanNLP, HIT NLP, Fudan NLP
- NER的评估：preceison, recall, f1_score
- NER方法：
  - 基于规则的方法（Rule-based method）比如：正则
  - 投票模型（Majority Voting）Baseline
  - 利用分类模型：1. 非时序模型（LR, SVM）2. 时序模型（HMM, CRF, LSTM-CRF）
- 基于特征工程和非时序模型的NER



#### 实体消岐（Entity Disambiguiation）

- 同一个名称，可能表示不同的实体。如小米，苹果。（关于刘伟的一切）
- 建立一个实体库，在库中，详细记录每一个名称可能对应的多个实体。每个实体有相关的描述文字，并将其转换为tfidf向量（长度为 $|V|$ ）。将文本的上下文（和库中接近的长度）也转换为tfidf向量，计算相似度，取相似度最高的实体作为真正的实体。
- elmo、BERT等上下文相关的词向量。

#### 实体统一（Entity Resolution）

- 给定两个实体，判断是否指向同一个实体。
- 第一种方法：计算相似度 sim(str1, str2) ，如编辑距离。
- 第二种方法：基于规则的方法。将一个实体转换为原型（prototype），类似于stemming。
- 第三种方法：监督学习的方法。
  - 从实体的上下文中，截取一段文本，转换为特征向量（如tfidf），可以将两个特征向量拼接，再通过一个二分类模型，判断两者是否是同一实体。也可以计算两个特征向量的余弦相似度，再经过LR分类。也可以加入编辑距离作为一个特征。
- 基于图的实体统一
  - 第一类特征：个体特征，如age，salary，job等。
  - 第二类特征：基于图的特征，如节点的度，度为1的节点等。
  - 根据以上两类特征，计算相似度。

#### 指代消解（Co-reference Resolution） 

- 基于supervised learning的方法
  - 数据标注和搜集
  - 建立代词和实体的两两配对，作为正负样本。
  - 提取特征，类似tfidf的方法。
  - 用特征和标注训练分类器。

### 关系抽取

- Open Source Knowledge Base
  
  - FreeBase, WorldNet, Yago, Dpedia, KnowledgeVault
- 关系抽取的方法：
  - 基于规则
  - 监督学习
  - 半监督、无监督学习
    - Bootstrap, Snowball
    - Distant Supervision
    - 无监督学习

- 基于规则的方法：

  - 优点：比较准确、不需要训练数据
  - 缺点：low recall rate，人力成本高，规则本身难设计

- 监督学习的方法：

  - 作为分类问题处理：定义关系类型、定义实体类型、准备训练数据（标注好的实体和实体之间的关系）
  - 特征工程，然后训练分类模型，以概率最高的关系作为结果
    - 特征包括：1. 类似上述的特征；2. 位置相关特征，句法分析，依存分析相关特征
    - 需要增加一个“没有关系” 的类别。
    - 实现中，可以设计一个二级分类器。第一个确定是否又关系，第二个确定有什么关系。

- Bootstrap

  - 首先建立要提取关系的seed tuple，然后：

    1. 利用seed tuple生成规则
    2. 利用已有的规则生成新的tuple

    重复以上两步。

  - 缺点： Error Accumulation，误差会不断累积，到后期准确率很低。

- Snowball （Bootstrap改进版）

  - **规则** 定义为一个五元组：

    ```
    rule = <L, T1, M, T2, R>
    ```

    其中，`T1, T2` 是识别到的实体类型，`L, M, R` 分别是左侧、中间和右侧的文本，并将其表示为向量形式，如长度为$|V|$ 的 tf-idf 向量（需要经过归一化）。则有对于两个规则的相似度如下定义：
    $$
    Rule_1=<L_1,T_1,M_1,T_2,R_1>\\
    Rule_2=<L_2,T_1^\prime,M_2,T_2^\prime,R_2>\\
    \text{sim}(Rule_1,Rule_2)=\cases {\begin{align}& 0   & \text{if}\ T_1 \ne T_1^\prime \text{ or } T_2\ne T_2^\prime \\& \mu_1L_1\cdot L_2 +\mu_2M_1\cdot M_2+\mu_3 R_1\cdot R_2\ &\text{otherwise} \end{align}}
    $$
    其中，
    $$
    \mu_1=\mu_3=0.2 \\
    \mu_2=0.6\\
    \mu_1+\mu_2+\mu_3=1\\
    $$
    
  - 建立要提取关系的seed tuple，然后：

    1. 生成规则

       根据规则之间的相似度，进行聚类。将类似的规则（相似度>阈值），取平均值，得到的centroid作为规则。

    2. 生成tuple

       从数据中，发现新的tuple，以及相应的五元组，将该五元组和规则库的规则计算相似度，如果和某一条规则的相似度>阈值，则将该tuple加入加入库中。

    3. 评估规则准确率，过滤

       对于每一条规则 $R_i=<L,T_1,M,T_2,R>$ ，扫描一遍文本，找出由该规则提取到的tuple，将这些tuple和seed库中的tuple进行对比，将预测的precision作为该规则的confidence。>阈值的规则保留，否则过滤。

    4. 评估tuple准确率，过滤

       对于某一个tuple，找出可以生成该tuple的所有规则$R_i$，以及该规则的置信度 $p_i$ ，则该tuple的置信度为
       $$
       confidence(tuple)= 1-\prod_i (1-p_i)
       $$
       如果置信度>阈值，则该tuple保留，否则过滤。（对应的规则数量越多，单条规则的置信度越高，则tuple的置信度也越高。）

    重复以上步骤。并且和bootstrap不同，snowball不再使用精确的match，而是近似的match。

    定义了以下的相似度：`sim(rule1, rule2)` 和`sim(rule1, tuple1)` ，从而实现过滤功能。


# Plan of next week

- 


