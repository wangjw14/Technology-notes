# 100亿模型计划

## 1、机器组装

两台原型机，每台机器是双卡的RTX 3090Ti，采用的是水冷。每台5000美金，约合3.5w人民币。比4卡的特斯拉 V100机器还要好一些。

### 1.1 需求

- 满足日常的开发和研究的需求。在一些大家常见的数据集上，能够比较各个算法之间的好坏，以及测试一些新的想法，在常见数据集上的性能，也包括对一些啊模型的内存啊，计算性能上面做一些优化。
- 更大的一个目标是，想把多台这样子的机器，组装成一个集群，训练一个参数的规模在10亿到100亿之间的这样子的模型。
- 未来一两年之内，在产品上来讲，10亿到100亿之间的模型将会是一个主流
- 算法不是瓶颈，因为有很多开源的代码可以去用。数据也不再是一个瓶颈，对于绝大部分产品来讲，你都能收集到上百GB或者甚至是上TB的跟你产品相关的数据，因为不需要标号，所以成本是比较低的，所以现在主要的瓶颈，是计算的资源。
- 就算用我们这种高性价比的装机的方法，你也至少要准备150万人民币，才能够装到一个足够大的集群，使得你能够有效的训练这样子的模型。
- 预算150w

### 1.2 计算资源的选项

1. 购买比较高端的GPU的服务器
   - 一个代表性的服务器就是NVIDIA自己推出的DGX A100
     - 服务器里面装了8个NVIDIA A100的GPU 
     - 如果想训练百亿模型，需要5台这样的机器，至少要500万
     - 很贵，不建议实验室或者小公司买
2. 使用高端的游戏的显卡和游戏的主板，装一台高端的游戏机，既能打游戏，也能做深度学习
   - https://en.wikipedia.org/wiki/GeForce_30_series
   - 选择显卡：主要看风扇和价格，厂家反而不重要
     - 多卡机器，最好选择涡轮散热
     - 想要安静，就买水冷的散热
     - 注意卡的宽度，是几个pcie卡槽的宽度
     - 3090Ti支持的是PCIE 4.0，单通道2GB/s，一共16通道，可达32GB/s。对比DGX A100可达600GB/s
     - 使用NV Link，增大GPU之间的通讯带宽。支持4通道，可以达到200GB/s
   - 买卡网站：bestbuy、https://www.newegg.com/
3. 在云上面去租GPU的机器
   - AWS上DGX A100对应的机器叫EC2 P4，它也是有八个A100的GPU，价格是每小时32美金
     - 算下来价格大概是说，用3年的价格，等价于你就把这台机器买回来放在家里自己用
     - H100就要出来了，H100他的性能据说是比A100要快很多很多，据说是至少快个四倍以上，价格只贵2倍

- DGX A100服务器
  - https://www.nvidia.com/en-us/data-center/dgx-a100/
  - 8x A100 GPU，内存80G/卡，总内存640G
  - 12 NVLinks/GPU，GPU到另外一个GPU可以达到600GB/s的一个双向带宽
  - 6x NVIDIA NVSwitch，能够达到总共的带宽是4.8个TB
  - 10x NVIDIA Connectx-7，500GB/s的双向带宽
  - 这种类似的机器跑transformer，GPU之间的带宽也仍然是一个瓶颈

### 1.3 压力测试

- 环境

  - 系统：ubuntu 22
  - 安装驱动：`sudo apt-get install nvidia-driver-515`
  - 查看Nvidia link是否正常：`nvidia-smi topo -m`

- 测试GPU

  - 使用 gpu-burn的小程序

    ```sh
    cd gpu-burn
    ./gpu-burn --help
    ./gpu-burn -tc 1200   # 使用Tensor cores运行1200s
    ```

  - nvtop的插件，可视化gpu的运行情况

- 测试cpu

  - 使用cpu-burn的小程序



## 2、环境安装

- 3090Ti，在除去A100之外，要么比别的GPU性能好，要么同等性能，比别的GPU便宜
- 如何评测一个任务在实际硬件上的性能，以及测试性能和理论性能之间的差别
  - 可以帮助根据自己的任务来选择自己合适的硬件
  - 也可以帮助确认，哪方面是计算的瓶颈，可以通过调参，达到性能的大幅提升
- 三种方案安装需要的框架，以pytorch为例
  - CUDA, pip/conda pytorch
  - Nvidia driver, conda pytorch
  - Nvidia driver, Nvidia docker

### 2.1 CUDA, pip/conda pytorch

- 





## 3、性能测试

- https://github.com/mli/transformers-benchmarks/blob/main/micro_bench.ipynb
- https://github.com/mli/transformers-benchmarks/blob/main/transformers.ipynb

### 3.1 矩阵乘法

- 测试配置

  ```python
  import torch
  
  print('Pytorch version\t:', torch.__version__)
  print('CUDA version\t:', torch.version.cuda)
  print('GPU\t\t:',torch.cuda.get_device_name())
  
  # 输出
  # Pytorch version	: 1.13.0a0+08820cb
  # CUDA version	: 11.7
  # GPU		: NVIDIA GeForce RTX 3090 Ti
  ```

- 辅助函数

  ```python
  import inspect
  from collections import defaultdict
  import pandas as pd
  from torch.utils import benchmark 
  
  pd.options.display.precision = 3
  
  def var_dict(*args):
      callers_local_vars = inspect.currentframe().f_back.f_locals.items()
      return dict([(name, val) for name, val in callers_local_vars if val is arg][0] 
                  for arg in args)
  
  def walltime(stmt, arg_dict, duration=3):
      return benchmark.Timer(stmt=stmt, globals=arg_dict).blocked_autorange(
          min_run_time=duration).median
  ```

- 矩阵乘法测试

  - $A_{mn} \times B_{nl}$ 所需要的运算次数：$2mnl$ 

  - 实际硬件上乘法和加法可能是一个运算实现

  - 乘-累加算子：https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation

  - 不同数据类型、不同大小的矩阵计算时候，GPU的性能

    |               | n=128 |  n=512 | n=2048 | n=8192 |
    | ------------: | ----: | -----: | -----: | ------ |
    | torch.float32 | 0.250 | 21.784 | 31.282 |  42.056 |
    | torch.float16 | 0.402 | 25.857 | 71.505 |  81.314 |

    结论：

    - 矩阵太小，会使得计算打不满，因此要尽量让矩阵大一些
    - 半精度的计算，可以使得性能翻倍。除了本身16位计算快之外，还会调用cuBLAS中的tensor cores进行加速

- 按元素乘法测试

  - 不同大小的向量，进行乘法

    |        |   65536 |  262144 | 1048576 | 4194304 |
    | -----: | ------: | ------: | ------: | ------- |
    | TFLOPS |   0.015 |   0.060 |   0.116 |  0.107   |
    |   GB/s | 120.687 | 482.782 | 927.392 |  859.757 |
  
  - 被GPU的内存带宽限制住了。带宽几乎打满，但是计算是理论值的1/800

- cuBLAS没有开源，但是cutlass是开源了
  - https://github.com/NVIDIA/cutlass

 ### 3.2 transofrmer layer

- 子模块的计算复杂度
  - ffn：$16blh^2$ 
  - self- attention：$4bhl^2 + 8blh^2$ 
- 结论
  - 更大的batch size会有更好的计算性能
  - 激活函数，会拖慢整个ffn，增加gelu之后，TFLOPS从74.282降低到63.994。内存的带宽瓶颈
  - 加入残差连接之后，继续下降到59.793
  - attention 41.050
- BERT层比GPT层要快一些

### 3.3 单卡训练

- 基于hugging face可以加速的办法
  - 使用fp16或者bf16，使用半精度的计算。同时可以增大batch size。如果支持bf16，要用bf16
  - 使用apex中的adamw_apex_fused优化器，对于优化器进行了优化
  - 累加几个batch，然后再进行权重更新。微调不能用太大的batch size，预训练的时候可以用很大的
  - 内存实在不够的时候，使用不保存中间变量，用计算换空间
- 使用Megatron-LM
  - 对于内存的优化更好，可以用更大的batch size。
  - 性能提升将近一倍，主要的原因是底层的kernels有更好的实现。
  - 手写了layer norm、带掩码的softmax、Gelu、AdamW

### 3.4 多卡训练

- NVLink
  - 是否使用NVLink，会极大影响通讯的带宽，从而影响整体的训练速度
  - 不用NVLink，会使得通讯的时间，从10%提升到38%
  - 使用NVLink，再使用梯度累加，可以减少通讯的次数，使得通讯的占比降低（10% -> 3%），模型更新的占比也降低（7% -> 2%）

- 张量并行

  - 参数在不同的GPU上，使得batch size可以增大

  - 但是没法通过增加累加次数减少通讯的时间，因为计算和通讯是在一起做的

  - 因为参数做了切分，因此可以在双卡上训练 EleutherAI/gpt-neo-1.3B，单卡的batch size是1

    

### 3.5 总结

- 使用大的batch size的好处
  - 大的矩阵，可以使得GPU的计算性能发挥出来
  - 降低模型更新、通讯带来的额外的开销

- 如何增大batch size
  - 更大的内存
  - 使用更快的数据类型，如16位浮点数
  - 使用更好的实现，比如将更好的kernel可以将多个运算合并，降低中间变量的内存，从而增大batch size
  - 梯度累加
  - 扔掉中间变量，增加额外30%的计算开销
- 多卡并行
  - 数据并行，能用就用。通讯模式简单，通讯和计算有一定的并行度

