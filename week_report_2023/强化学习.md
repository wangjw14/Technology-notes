# 强化学习











## 一、强化学习基础概念

![RL_basic_conpect](pics/RL_basic_conpect.png)

- 强化学习一般包含三个要素：Actor、Env 和 Reward Function
- Actor
  - Policy $\pi$ 一般是指参数为 $\theta$ 的网络，输入是Env的一个state，输出是一个action的分布。
  - 比如，对于一个游戏而言，输入是显示器的画面，输出是要采取的行动，如“左移”、“右移”等。
  - 这几个名称是等价的。Actor $=$ policy $=\pi=$ network $=$ function
- Env
  - 一般是游戏的规则等。

- Actor 和 Env 之间的互动中，一条trajectory的概率

![RL-process](pics/RL-process.jpeg)

- Reward
  - 一条trajectory 的 total reward：$R(\tau)=\sum_{t=1}^T r_t$ 
  - 穷举所有的 trajectory，得到reward的期望值

$$
\bar{R}_\theta=\sum_\tau R(\tau) p_\theta(\tau)=E_{\tau \sim p_\theta(\tau)}[R(\tau)] \\\quad
$$



## 二、Policy Gradient

- 为了求的 $\bar{R}_\theta$ 的最大值，需要对其进行求导，然后进行梯度上升求解。

$$
\nabla \bar{R}_\theta=\sum_\tau R(\tau) \nabla p_\theta(\tau)=\sum_\tau R(\tau) p_\theta(\tau) \frac{\nabla p_\theta(\tau)}{p_\theta(\tau)} \\
\approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R\left(\tau^n\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)
$$

​		其中 $R(\tau)$ 不一定需要可微，可以是一个黑盒。

- 得到梯度之后，一次迭代过程如下：首先使用 policy $\pi _\theta$ ，采样很多的 trajectory；然后使用这些数据，更新参数 $\theta$ ，之后再重新采用数据，不断进行迭代。

  <img src="pics/policy_gradient.png" alt="policy_gradient" style="zoom:30%;" />

- RL 和一般的supervised learning的区别在于，一般的训练，模型参数更新之后，不会影响数据的标签。而在RL中，模型参数更新之后，Actor 和 Env互动的结果会随之改变，从而影响 label（或者 reward），使得每次更新参数之后，需要重新对数据进行采样。这样也训练成本很高。

- Policy Gradient的改进

  - 















