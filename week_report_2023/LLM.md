# LLM

- 梯队
  - 第一梯队：OpenAI
  - 第二梯队：Google：PaLM，Pathways



## 1、ChatGPT是否带来了NLP乃至AI领域的研究范式转换？带来了哪些影响？

### 1.1 第一次范式转换：从深度学习到两阶段预训练模型

- 时间范围：在深度学习引入NLP领域（2013年左右），到GPT 3.0出现之前（2020年5月左右）
- transformer的出现：
  - 特征抽取能力更强大
  - 可以将模型变得更大，数据量也可以变得更大
- 带来的影响
  - 部分NLP研究子领域的衰退乃至逐步消亡（典型的中间任务包括：中文分词、词性标注、NER、句法分析、指代消解、语义Parser等）
  - NLP不同子领域的技术方法和技术框架日趋统一

### 1.2 第二次范式转换：**从预训练模型走向通用人工智能** 

- 时间范围：在GPT3.0出现之后（20年6月左右），一直到目前为止，我们应该正处于这个范式转换过程中。
- 为什么LLM都是GPT式的自回归模型
  1. NLG可以兼容NLU，反之则不行
  2. 生成模型更容易做好zero shot/few shot prompting方式的任务，而Bert模式以这种方式做任务，是天然有劣势的（参考：On the Role of Bidirectionality in Language Model Pre-Training）
- 为什么我们要追求zero shot/few shot prompting这种方式来做任务呢？
  - LLM应该具备强大的自主学习能力，可以学习各种类型的知识，导致LLM必然是巨无霸
  - 当我们使用LLM解决某个具体领域问题的时候，应该用我们人类习惯的表达方式，就是说LLM应该理解人类的命令。
- 带来的影响
  - 让LLM适配人的新型交互接口，使用instruct
  - 很多NLP子领域不再具备独立研究价值
  - 更多NLP之外的研究领域（图像、多模态）将被纳入LLM技术体系，**这个方向方兴未艾，是具备高价值的研究主题**。



## 2、学习者：从无尽数据到海量知识

### 2.1 LLM从海量数据中学到了什么知识？

- 可以分为**语言类知识**和**世界知识**两大类。
  - 语言类知识：指的是词法、词性、句法、语义等有助于人类或机器理解自然语言的知识。
    - 浅层语言知识比如词法、词性、句法等知识存储在Transformer的低层和中层，而抽象的语言知识比如语义类知识，广泛分布在Transformer的中层和高层结构中。
  - 世界知识：指的是在这个世界上发生的一些真实事件（事实型知识，Factual Knowledge），以及一些常识性知识(Common Sense Knowledge)
    - LLM确实从训练数据中吸收了大量世界知识，而这类知识主要分布在Transformer的中层和高层，尤其聚集在中层。而且，随着Transformer模型层深增加，能够学习到的知识数量逐渐以指数级增加
    - 把LLM看作是一种以模型参数体现的隐式知识图谱
    - 对于Bert类型的语言模型来说，只用1000万到1亿单词的语料，就能学好句法语义等语言学知识，但是要学习事实类知识，则要更多的训练数据。（When Do You Need Billions of Words of Pre-training Data?）

### 2.2 LLM又是如何存取这些知识的？

- 多头注意力（MHA）部分占了大约参数总体的三分之一，三分之二的参数集中在FFN结构中。
- MHA主要用于计算单词或知识间的相关强度，并对全局信息进行集成，更可能是在建立知识之间的联系，大概率不会存储具体知识点，那么很容易推论出LLM模型的知识主体是存储在Transformer的FFN结构里。
- paper：“Transformer Feed-Forward Layers Are Key-Value Memories”给出了一个比较新颖的观察视角，它把Transformer的FFN看成存储大量具体知识的Key-Value存储器



### 2.3 如何修正LLM里存储的知识

- 从训练数据的源头来修正知识。
  - 对于指定的某条知识，可以定位到是哪些训练数据导致LLM学会了这条知识
  - 定位到其对应的数据源头，删除数据源，然后重新预训练整个LLM模型，这样即可达成删除LLM中相关知识的目的。
  - 比较适合那种对于某个特定类别数据的一次性大规模删除场合，不适合少量多次的常规知识修正场景，比如可能比较适合用来做去除偏见等去toxic内容的处理。
  - paper：Towards Tracing Factual Knowledge in Language Models Back to the Training Data

- 对LLM模型做一次fine-tuning来修正知识。
  - 我们可以根据要修正成的新知识来构建训练数据，然后让LLM模型在这个训练数据上做fine-tuning，这样指导LLM记住新的知识，遗忘旧的知识。
  - 问题：
    - 带来灾难遗忘问题，就是说除了忘掉该忘的知识，还忘掉了不该忘的知识，导致这么做了之后有些下游任务效果下降。
    - 目前的LLM模型规模非常大，即使是做fine-tuning，如果次数频繁，其实成本也相当高。
  - paper：Modifying Memories in Transformer Models

- 直接修改LLM里某些知识对应的模型参数来修正知识。
  - 如何在LLM参数空间中定位某条知识的具体存储位置；
  - 如何修正模型参数，来实现旧知识到新知识的修正。
  - 可以参考“Locating and Editing Factual Associations in GPT”和“Mass-Editing Memory in a Transformer”。理解这个修正LLM知识的过程，其实对于更深入理解LLM的内部运作机制是很有帮助的。



## 3、随着LLM规模逐步增大，会带来什么影响？

- **预训练阶段**的优化指标确实和**下游任务**表现出正相关关系，但是并非完全正相关。也就是说，只看预训练阶段的指标，来判断一个LLM模型是否够好，这是不够的。
- 我们分头来看在这两个不同阶段，随着LLM模型增大，有什么影响。

### 3.1 预训练阶段

- “伸缩法则”（scaling law）：当我们独立增加**训练数据量**、**模型参数规模**或者**延长模型训练时间**（比如从1个Epoch到2个Epoch），预训练模型在测试集上的Loss都会单调降低，也就是说模型效果越来越好。
- 如何分配算力
  - OpenAI
    - 选择了同时增加训练数据量和模型参数，但是采用早停策略(early stopping)来减少训练步数的方案。
    - 对于训练数据量和模型参数这两个要素，如果只单独增加其中某一个，这不是最好的选择，最好能按照一定比例同时增加两者，它的结论是优先增加模型参数，然后才是训练数据量。
    - 假设用于训练LLM的算力总预算增加了10倍，那么应该增加5.5倍的模型参数量，1.8倍的训练数据量，此时模型效果最佳。
    - paper：Scaling Laws for Neural Language Models
  - DeepMind
    - 基本结论和OpenAI的结论差不多，比如确实需要同时增加训练数据量和模型参数，模型效果才会更好。
    - 修正了两者的比例关系，认为训练数据量和模型参数是同等重要的，也就是说，假设用于训练LLM的算力总预算增加了10倍，那么应该增加3.3倍的模型参数量，3.3倍的训练数据量，这样模型效果才最好。
    - 可以选择放大训练数据，并同比例地减少LLM模型参数，以达到在不降低模型效果的前提下，极大缩小模型规模的目的。
    - paper：Training Compute-Optimal Large Language Models

### 3.2 下游任务

- 随着模型规模增大，不同类型的任务有不同的表现，具体而言，有以下三类情况。
- 第一类任务：完美体现了LLM模型的scaling law，就是说随着模型规模逐步放大，任务的表现越来越好
  - 这类任务通常符合如下共性：它们往往都是知识密集型任务，也就是说如果LLM模型包含的知识量越多，这类任务表现越好。
- 第二类任务：展现出LLM具备某种“涌现能力（Emergent Ability）“
  - 当模型参数规模未能达到某个阀值时，模型基本不具备解决此类任务的任何能力，体现为其性能和随机选择答案效果相当，但是当模型规模跨过阀值，LLM模型对此类任务的效果就出现突然的性能增长。
  - “涌现能力”的任务也有一些共性：这些任务一般由多步骤构成，要解决这些任务，往往需要先解决多个中间步骤，而逻辑推理能力在最终解决这类任务中发挥重要作用。
  - 为什么会出现这种“涌现能力”现象呢？
    - 有些任务的评价指标不够平滑
    - 有些任务由若干中间步骤构成，随着模型规模增大，解决每个步骤的能力也在逐步增强，但是只要有一个中间步骤是错的，最终答案就是错的，于是也会导致这种表面的“涌现能力”现象。
    - paper：Emergent Abilities of Large Language Models
  - paper：Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
- 第三类任务：随着模型规模增长，任务的效果曲线展现出U形特性
  - 随着模型规模逐渐变大，任务效果逐渐变差，但是当模型规模进一步增长，则效果开始越来越好，呈现出U形增长趋势
  - 内部其实隐含了两种不同类型的子任务，一种是真正的任务，另外一种是“干扰任务（distractor task）”。
  - 当模型规模小的时候，无法识别任意一种子任务，所以模型的表现跟随机选择答案差不多，当模型增长到中等规模的时候，主要执行的是干扰任务，所以对真正的任务效果有负面影响，体现为真正任务效果的下降，而当进一步增加模型规模，则LLM可以忽略干扰任务，执行真正的任务，体现为效果开始增长。



## 4、人机接口：从In Context Learning到Instruct理解

### 4.1 什么是In Context Learning?

- few shot learning

- Fine-tuning vs In Context Learning
  - Fine-tuning拿这些例子当作训练数据，利用反向传播去修正LLM的模型参数，而修正模型参数这个动作，确实体现了LLM从这些例子学习的过程。
  - In Context Learning只是拿出例子让LLM看了一眼，并没有根据例子，用反向传播去修正LLM模型参数的动作，就要求它去预测新例子。
- In Context Learning并未学习映射函数，但是输入和输出的分布很重要，这两个不能乱改。
  - 真正对In Context Learning影响比较大的是：$x$ 和 $y$ 的分布，也就是输入文本 $x$  的分布和候选答案 $y$ 有哪些，如果你改变这两个分布，比如把 $y$ 替换成候选答案之外的内容，则In Context Learning效果急剧下降。

### 4.2 它和Instruct又是什么关系？

- zero shot learning
- 偏学术研究的Instruct vs 人类真实需求描述的Instruct
  - 偏学术研究的Instruct
    - Model：FLAN
    - 能够有效增加LLM模型Instruct泛化能力的因素包括：增加多任务的任务数量、增加LLM模型大小、提供CoT Prompting， 以及增加任务的多样性。
    - paper：“Scaling Instruction-Fine-tuned Language Models”／“Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks”
  - 人类真实需求描述的Instruct
    - Model：ChatGPT
  - 从用户数据中收集真实需求，这事情是很重要的。
    - 在GPT3上用FLAN提到的任务、数据以及Prompt模版进行微调，来在GPT 3上复现FLAN方法，然后和InstructGPT进行比较，因为InstructGPT的基础模型也是GPT3，所以只有数据和方法的差别，两者可比，结果发现FLAN方法的效果，距离InstructGPT有很大的差距。

- 我们是否能够提供给LLM完成某个任务的若干具体示例，让LLM找出其对应的自然语言描述的Instruct命令？
  - 答案是可以的
  - 给LLM一些示例，让LLM自动生成能够描述这项任务的自然语言命令，然后它再用LLM生成的任务描述去测试任务效果。它使用的基础模型是GPT 3和InstructGPT，经过这项技术加持后，LLM生成的Instruct的效果相比未采用这项技术的GPT 3 以及InstuctGPT来说，指标有极大地提升，而且在一些任务上超过人类的表现。
  - paper：Large Language Models Are Human-Level Prompt Engineers



## 5、LLM具备推理能力吗？

- 当模型规模足够大的时候，LLM本身是具备推理能力的，在简单推理问题上，LLM已经达到了很好的能力，但是复杂推理问题上，还需要更多深入的研究。
- 提升推理能力的方法
  - 使用更好的prompt：通过合适的提示语或提示样本，更好地激发出LLM本身就具备的推理能力，Google在这个方向做了大量很有成效的工作。
  - 训练更好的推理能力的模型：预训练过程中引入程序代码，和文本一起参与预训练，以此进一步增强LLM的推理能力，这应该是OpenAI实践出的思路。

### 5.1 基于prompt的方法

1. 直接在问题上追加辅助推理Prompt
   - 也被称为zero-shot CoT
   - 第一阶段在提问的问题上追加“Let’s think step by step”这句提示语，LLM会输出具体的推理过程；
   - 第二阶段，在第一阶段的问题后，拼接LLM输出的具体推理过程，并再追加Prompt=“Therefore, the answer (arabic numerals) is”，
   - 在数学推理测试集GSM8K上，加上提示语后，推理准确率直接从原先的10.4%提升到了40.4%
2. 基于示例的思维链（few-shot CoT,Chain of Thought）Prompting
   - few-shot CoT
   - 给出一些人工写好的推理示例，示例里把得到最终答案前，一步步的具体推理步骤说清楚，而这些人工写的详细推理过程，就是思维链Prompting
   - Self-Consistency：要求LLM输出多个不同的推理过程和答案，然后采用投票的方式选出最佳答案
3. 分治算法的思想
   - 对于一个复杂的推理问题，我们把它分解成若干容易解决的子问题，一一解决掉子问题后，我们再从子问题的答案推导复杂问题的答案。



### 5.2 代码预训练增强LLM推理能力

- 从纯文本预训练模型切换到文本和Code混合预训练模型，在几乎所有测试数据集合上，模型推理能力都得到了巨大的效果提升，比如我们以“Self Consistency”方法为例，在大多数据集合上的性能提升，都直接超过了20到50个百分点

  

## 6、未来之路：LLM研究趋势及值得研究的重点方向

1. **探索LLM模型的规模天花板**
   - 做超大规模的LLM模型，对技术团队的工程实现能力要求是非常高的，也不能认为这事情缺乏技术含量。
   - 继续推大LLM模型规模，有什么研究意义呢？
     - 很多推理类型的有难度的任务，加上CoT Prompting后，其效果也呈现出遵循Scaling law的趋向。那么，很自然的一个问题就是：对于这些任务，LLM的规模效应，能将这些任务解决到何种程度？
     - 其次，考虑到LLM具备的神奇的“涌现能力”，如果我们继续增加模型规模，它会解锁哪些让我们意想不到的新能力呢？
   - 这种事情也就只能说说，对99.99%的从业者来说，是没有机会和能力做这个事情的。要做这个事情，对研究机构的财力及投入意愿、工程能力、技术热情，都有极高的要求，缺一不可。能做这事情的机构，粗估下来，国外不超过5家，国内不超过3家。

2. **增强LLM的复杂推理能力**
   - 前LLM能够解决得比较好的推理问题，往往都相对简单，LLM的复杂推理能力仍然薄弱，比如即使是简单的字符拷贝推理或者加减乘除运算，当字符串或者数字非常长的时候，LLM推理能力会极速下降，再比如行为规划能力等复杂推理能力很弱。总而言之，加强LLM的复杂推理能力，应该是LLM未来研究中最重要的环节之一。
3. **LLM纳入NLP之外更多其它研究领域**
   - 作为通向AGI的重要种子选手，将图像、视频、音频等图像与多模态集成进入LLM，乃至AI for Science、机器人控制等更多、差异化更明显的其它领域逐步纳入LLM，是LLM通往AGI的必经之路。
4. **更易用的人和LLM的交互接口**
   - 目前的技术并不完美，肯定还有很多命令LLM理解不了。所以，沿着这个方向，寻找更好的技术，来让人类使用自己习惯的命令表达方式，而LLM又能听懂，这是个新的，且非常有前景的技术方向。
5. **建设高难度的综合任务评测数据集**
   - 目前行业应出现了一些新的测试集，有代表性的包括BIGBench、OPT-IML等。这些测试集合体现出一些特性，比如相对LLM现有技术具备一定的难度、综合了各种各样多种类型的任务等。
   - 受到ChatGPT的启发，我觉得除此外应纳入另一考虑因素：体现真实用户需求。
6. **高质量数据工程**
   - 关于数据，需要考虑两个方面：数据的质量和数量。
   - 而根据T5的对比实验，我们可以得出结论：在数量和质量两个因素里，质量优先，正确的道路应该是在保证数据质量的前提下，再去增大数据规模。
   - 数据质量：多样化的数据赋予了LLM更好解决更多不同类型任务的能力，这可能是数据质量里最关键的标准。
   - 数据数量：2026年左右，高质量的NLP数据将会用光，低质量NLP数据会在2030到2050年用光，而低质量图像数据会在2030到2060年用光。
   - paper：Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning
7. **超大LLM模型Transformer的稀疏化**
   - 现有研究表明，标准的Dense Transformer在训练和推理时，它本身也是稀疏激活的，就是说只有部分参数会被激活，大部分参数没有参与训练和推理过程。
   - Sparse模型存在训练不稳定、容易过拟合等问题，不太容易训练好。所以，如何修正稀疏模型面临的问题，设计出更容易训练的稀疏模型，是很重要的未来研究方向。
   - paper：Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers



## 7、复刻chatGPT需要注意什么

1. 在预训练模式上，选择GPT这种自回归语言模型
2. 强大的推理能力是让用户认可LLM的重要心理基础，而如果希望LLM能够具备强大的推理能力，根据目前经验，最好在做预训练的时候，要引入大量代码和文本一起进行LLM训练。
3. 如果希望模型参数规模不要那么巨大，但又希望效果仍然足够好，此时有两个技术选项可做配置：
   - 要么增强高质量数据收集、挖掘、清理等方面的工作，意思是我模型参数可以是ChatGPT/GPT 4的一半，但是要想达到类似的效果，那么高质量训练数据的数量就需要是ChatGPT/GPT 4模型的一倍（Chinchilla的路子）
   - 另外一个可以有效减小模型规模的路线是采取文本检索（Retrieval based）模型+LLM的路线
   - 这两个技术选型不互斥，反而是互补的
4. 超级大模型因为模型规模大，所以训练成本过高，LLM模型Sparse化是一个应该考虑的选项
5. ChatGPT是目前最接近理想LLM的技术方案，而理想中的LLM应该是以一个几乎无所不能的基础通用大模型作为依托，来支持各种各样的上层任务类型。应该重视通过增加数据多样性来增加LLM新能力的思路。
6. 易用的人机操作接口。
   - 要从最终用户那里收集任务表述方式，而不能靠研发人员自己的臆想或猜测。ChatGPT给我最大的启发其实是这一点，至于是否用增强学习我倒觉得不重要，其它替代技术应该也能做类似的事情。



## 8、ChatGPT:为什么是OpenAI

- 从GPT 1.0开始，基本就坚定地把LLM看做是通往AGI的一条必由之路。
- Bert证明了：双向语言模型对于很多NLP理解类任务，效果比自回归这种单向语言模型效果更好。尽管如此，GPT 2.0并没有因此切换到双向语言模型这条路上，仍然走文本生成的路，而且开始尝试零示例（zero shot）prompt和少量示例（few shot）prompt。
- GPT 3.0已经展示出了比较强大的zero shot/few shot prompt能力，这时候OpenAI心目中的AGI已经完全漏出水面，轮廓清晰，而且它的效果也证明了这条路，是有较大可能走得通的。
- OpenAI通过ChatGPT证明了一点；虽然我们距离真正的AGI，可能还有很长的路要走，但是通过超大LLM走向AGI这条路，目前看是可行的。



附录：

| 模型   | 参数 | 备注      |
| ------ | ---- | --------- |
| GPT 3  | 175B | OpenAI    |
| LaMDA  | 137B | Google    |
| PaLM   | 540B | Google    |
| Gogher | 280B | DeepMind  |
| GLM    | 130B | 清华&智谱 |
| 盘古   | 200B | 华为      |
| 文心   | 260B | 百度      |
| 源1.0  | 245B | 浪潮      |