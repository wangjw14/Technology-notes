# 10.25-10.31 回顾

## 互联网新闻情感分析

- 比赛网址：https://www.datafountain.cn/competitions/350/
- 方案网址：https://github.com/cxy229/BDCI2019-SENTIMENT-CLASSIFICATION
- RoBERTa的微调，使用多层的hidden state，后面接一些其他网络
- 数据量少的时候，数据增强的使用（伪标签）
- 模型集成



## 互联网金融新实体发现

- 比赛网址：https://www.datafountain.cn/competitions/361

#### Rank2：

- 方案网址：https://github.com/ChileWang0228/Deep-Learning-With-Python/tree/master/chapter8
- 方案思路：
  - BERT + BILSTM + CRF
  - BERT + IDCNN + CRF
  - 动态权重BERT + IDCNN + CRF
  - 动态权重BERT + BILSTM + CRF
  - 上面提到的四种异构单模分别搭载其他预训练模型{BERT_WWM, ROBERTA, ERNIE}得到多种异构单模。
- 命名实体识别NER论文调研 - 阿力阿哩哩的文章 - 知乎 https://zhuanlan.zhihu.com/p/100863101

#### Rank9：

- 方案网址：https://github.com/gaozhanfire/CCF-BDCI2019_Entity_Discovery
- 提分点：
  - 最初使用了初赛和复赛全量训练数据
  - 效果非常不好，便尝试只使用复赛数据（提升了3百分点）
  - 尝试使用***回标***数据标注方式，即将训练集中所有的实体（即所有文章对应的实体）收集到一个set里，然后用这个set中的实体对每篇文章进行标注。（提升了3个百分点）！！！！
  - 我们认为复赛数据更加注重语义（比如词出现在文中的位置（比如分号隔开的实体））所以我们将一些知名实体（比如京东、阿里）也加入到上面提到的那个set集合中进行，与原数据中的实体一起对训练集文章进行标注，对结果进行后处理时将知名实体进行去除。（带来了5个千左右的提升）







# Plan of next week

- 


