# T5

- T5使用了简化的相对位置embeding，即每个位置对应一个数值而不是向量，将相对位置的数值加在attention softmax之前的logits上，每个head的有自己的PE，所有的层共享一套PE。个人认为这种方式更好一点，直接在计算attention weight的时候加入位置信息，而且每一层都加一次，让模型对位置更加敏感。

- https://zhuanlan.zhihu.com/p/88363572