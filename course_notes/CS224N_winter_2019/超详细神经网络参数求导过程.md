# 超详细神经网络参数求导过程

### 一个最简单的神经网络

$$
\begin{align}
\pmb x &= \text{input} \\
\pmb z &= \pmb{Wx}+\pmb b_1\\
\pmb h &= \text{ReLU}(\pmb z)\\
\pmb \theta &= \pmb{Uh}+\pmb b_2 \\
\pmb{\hat y}&=\text {softmax}(\pmb \theta)\\
J& = \text{CrossEntropy}(\pmb y,\pmb {\hat y})
\end{align}
$$


$$
\pmb x\in \Bbb R^{D_x\times 1}\qquad \pmb W\in \Bbb R^{D_h\times D_x}\qquad \pmb b_1\in \Bbb R^{D_h\times 1}\qquad\pmb U\in \Bbb R^{N_c\times D_h}\qquad\pmb b_2\in \Bbb R^{N_c\times 1}\qquad
$$
其中 $D_x$ 是输入的维数，$D_h$是隐层的维数，$N_c$是类别的个数。

### ReLU函数

$$
\begin{align}
\text{ReLU}(x)&=\max(x,0)\\
\text{ReLU}^\prime(x)&=\cases
{1 \qquad\text{if } x>0 \\
0 \qquad\text{otherwise}}=\text{sgn}(\text{ReLU}(x))
\end{align}
$$

### softmax函数

softmax函数中，第$i$个分量为
$$
\pmb {\hat y}_i=\frac{e^{\pmb\theta_i}}{\sum_{j=1}^{N_c}{e^{\pmb\theta_j}}},i=1,2...N_c
$$

### Cross Entropy函数

$$
J= -\sum_{k=1}^{N_c}\pmb y_k\ln\pmb {\hat y}_k
$$

### 求导超详细过程

为方便描述，做下面的定义
$$
\pmb\delta_1 = \frac{\part J}{\part \pmb\theta}\qquad \pmb \delta_2=\frac{\part J}{\part \pmb z}
$$
根据链式法制，可以进一步分解为
$$
\begin{align}
&\pmb\delta_1 = \frac{\part J}{\part \pmb\theta}=\frac{\part J}{\part \pmb{\hat y}}\frac{\part \pmb{\hat y}}{\part \pmb\theta}\\
&\pmb \delta_2=\frac{\part J}{\part \pmb z}=\frac{\part J}{\part \pmb\theta}\frac{\part \pmb\theta}{\part \pmb h}\frac{\part \pmb h}{\part \pmb z} = \pmb\delta_1\frac{\part \pmb\theta}{\part \pmb h}\frac{\part \pmb h}{\part \pmb z}
\end{align}
$$
先计算$\pmb\delta_1$，其中$\frac{\part J}{\part \pmb{\hat y}_k}$ 
$$
\begin{align}
\frac{\part J}{\part \pmb{\hat y}_k} &=\frac{\part(-\sum_{k=1}^{N_c}\pmb y_k\ln\pmb {\hat y}_k)}{\part \pmb{\hat y}_k} =-\frac{\pmb y_k}{\pmb {\hat y}_k}
\end{align}
$$


而$\frac{\part \pmb{\hat y}_k}{\part \pmb\theta_i}$需要分两种情况计算，当$k\neq i$时
$$
\begin{align}
\frac{\part \pmb{\hat y}_k}{\part \pmb\theta_i} &=\frac{\part(\frac{e^{\pmb\theta_k}}{\sum_{j=1}^{N_c}{e^{\pmb\theta_j}}})}{\part \pmb\theta_i}\\
&= -e^{\pmb\theta_k}\cdot\frac{1}{(\sum_{j=1}^{N_c}{e^{\pmb\theta_j}})^2}\cdot e^{\pmb\theta_i} \\
&=-\frac{e^{\pmb\theta_k}}{\sum_{j=1}^{N_c}{e^{\pmb\theta_j}}}\frac{e^{\pmb\theta_i}}{\sum_{j=1}^{N_c}{e^{\pmb\theta_j}}} \\
&=-\pmb{\hat y}_k\pmb{\hat y}_i
\end{align}
$$
当$k=i$ 时
$$
\begin{align}
\frac{\part \pmb{\hat y}_k}{\part \pmb\theta_i} &=\frac{\part(\frac{e^{\pmb\theta_i}}{\sum_{j=1}^{N_c}{e^{\pmb\theta_j}}})}{\part \pmb\theta_i}\\
&= \frac{e^{\pmb\theta_i}(\sum_{j=1}^{N_c}{e^{\pmb\theta_j}})-(e^{\pmb\theta_i})^2}{(\sum_{j=1}^{N_c}{e^{\pmb\theta_j}})^2}\\
&=\frac{e^{\pmb\theta_i}}{\sum_{j=1}^{N_c}{e^{\pmb\theta_j}}}\cdot\frac{\sum_{j=1}^{N_c}{e^{\pmb\theta_j}}-e^{\pmb\theta_i}}{\sum_{j=1}^{N_c}{e^{\pmb\theta_j}}} \\
&=\pmb{\hat y}_i(1-\pmb{\hat y}_i)
\end{align}
$$
将上面的两部分带入
$$
\begin{align}
\frac{\part J}{\part \pmb\theta_i} &= \sum_{k=1}^{N_c}\frac{\part J}{\hat{\pmb y}_k}\frac{\part\hat{\pmb y}_k}{\pmb{\theta}_i} \\
&=\sum_{k=1}^{N_c}-\frac{\pmb y_k}{\pmb {\hat y}_k}\frac{\part\hat{\pmb y}_k}{\pmb{\theta}_i} \\
&=-\frac{\pmb y_i}{\pmb {\hat y}_i}\frac{\part\hat{\pmb y}_i}{\pmb{\theta}_i} + \sum_{k=1,k\neq i}^{N_c}-\frac{\pmb y_k}{\pmb {\hat y}_k}\frac{\part\hat{\pmb y}_k}{\pmb{\theta}_i}\\
&= -\frac{\pmb y_i}{\pmb {\hat y}_i}\pmb{\hat y}_i(1-\pmb{\hat y}_i) + \sum_{k=1,k\neq i}^{N_c}-\frac{\pmb y_k}{\pmb {\hat y}_k}\cdot-\pmb{\hat y}_k\pmb{\hat y}_i \\
&=-{\pmb y_i}(1-\pmb{\hat y}_i) + \sum_{k=1,k\neq i}^{N_c}{\pmb y_k}\pmb{\hat y}_i \\
&=-{\pmb y_i}+\sum_{k=1}^{N_c}{\pmb y_k}\pmb{\hat y}_i \\
&=\pmb{\hat y}_i-{\pmb y_i}
\end{align}
$$
写出向量形式，采用分母布局，那么
$$
\pmb\delta_1 =\frac{\part J}{\part \pmb\theta}={(\pmb{\hat y}-{\pmb y})}^T
$$
再来计算$\pmb \delta_2$
$$
\begin{align}
\pmb \delta_2&=\pmb\delta_1\frac{\part \pmb\theta}{\part \pmb h}\frac{\part \pmb h}{\part \pmb z} \\
&=\pmb\delta_1\pmb U\frac{\part \pmb h}{\part \pmb z} \\
&=\pmb\delta_1\pmb U\circ\text{ReLU}^\prime(\pmb z) \\
&=\pmb\delta_1\pmb U\circ\text{sgn}(\pmb h)
\end{align}
$$

### 求参数的梯度

$$
\begin{align}
\frac{\part J}{\part \pmb U}&=\frac{\part J}{\part \pmb\theta}\frac{\part \pmb\theta}{\part \pmb U} = \pmb\delta_1\frac{\part \pmb\theta}{\part \pmb U}= \pmb\delta_1^T\pmb h^T \\
\frac{\part J}{\part \pmb b_2}&=\frac{\part J}{\part \pmb\theta}\frac{\part \pmb\theta}{\part \pmb b_2} = \pmb\delta_1\frac{\part \pmb\theta}{\part \pmb b_2}= \pmb\delta_1^T\ \\
\frac{\part J}{\part \pmb W}&=\frac{\part J}{\part \pmb z}\frac{\part \pmb z}{\part \pmb W} = \pmb\delta_2\frac{\part \pmb z}{\part \pmb W}= \pmb\delta_2^T\pmb x^T \\
\frac{\part J}{\part \pmb b_1}&=\frac{\part J}{\part \pmb z}\frac{\part \pmb z}{\part \pmb b_1} = \pmb\delta_2\frac{\part \pmb z}{\part \pmb b_2}= \pmb\delta_2^T \\
\frac{\part J}{\part \pmb x}&=\frac{\part J}{\part \pmb z}\frac{\part \pmb z}{\part \pmb x} = \pmb\delta_2\frac{\part \pmb z}{\part \pmb x}= (\pmb\delta_2 W)^T \\

\end{align}
$$

将上述求出的结论代入，即可得到最终结果。



### 参考资料

http://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf

https://www.cnblogs.com/pinard/p/10750718.html

https://www.cnblogs.com/pinard/p/10825264.html

