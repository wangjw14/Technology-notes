# Anthropic LLM

## 摘要

- 我们使用了perference modeling和RLHF，来微调语言模型
- aligning training几乎可以提升所有NLP评估任务上的性能
- 方法上使用了迭代的在线训练，每个星期会使用新的人工标注，训练新的perference model和RL策略
- RL的奖励和原始模型与RL调过的模型的KL散度的平方根成线性关系



## 导言

- 本文主要做有用性（helpful）和无害性（harmless）

- 针对有用性和无害性，分别做了2个数据集

- 本文的贡献

  - 收集了一个偏好的对话数据集（对话是多轮的）
  - 和人工标注进行align，有很多好处。参数量小的时候，RLHF会降低性能（对齐税），当模型参数量变大之后，基本没有影响。zero-shot甚至可以更好。
  - 有用性和无害性两个任务之间会有一定的冲突，需要显式地建模，告诉模型不作恶
  - 使用OOD，避免说出有害的答案
  - RL的稳定性问题，和迭代的在线学习

- 相关工作

  - LaMDA：标注了一些数据，进行了微调

  - WebGPT or GopherCite：在网络上搜索，将搜到的结果返回。和微软bing的相关性更高

  - InstructGPT vs Anthropic LLM

    |            | InstructGPT        | Anthropic LLM                                                |
    | ---------- | ------------------ | ------------------------------------------------------------ |
    | 训练流程   | 先做微调，然后RL   | 做了上下文的提炼（一种prompt enginering）<br>代价小一点，效果差一点 |
    | 无害性训练 | 无                 | 有                                                           |
    | 奖励模型   | 6B                 | 最大的模型                                                   |
    | RL的过程   | 加入了原始的LM任务 | 没有加入原始LM任务                                           |
    | 是否在线RL | 否                 | 是                                                           |



## 数据搜集

- 背景
  - 人类有复杂的直觉，很容易表达，但很难被公式量化或者自动化。
  - 对话是一种非常通用的任务，任何基于文字的任务，都可以通过对话来表达。

- 方法

  - 人工提出问题，机器给出2个回答，然后去看是哪个回答更好一些（一致性只有63%）
  - 根据标注人员的写作，去判断其标注质量
  - 让标注人员根据直觉去判断是否有用和无害，从而增加样本多样性，长期来看是好的

- 两个数据集

  - 一个是有用，一个是无害
  - 这两者之间会有冲突，会导致模型分裂

- 使用了3类模型，不同的模型同时部署

  - HHH 上下文蒸馏出来的LM，52B。类似于instructGPT里的微调，但实际上没有微调。
  - RS，52B。选k个样本，一般k=16
  - RLHF后的模型。继续在线微调

- 生成的数据也分成3块

  - 基础数据集，44k有用数据集，42k无害数据集
  - RS数据集，52k有用数据集，2k有害数据集
  - 在线数据集，22k有用数据集，没有有害数据集

- 比较模型好坏的Elo分数

  - 没有搞的特别懂，以后再看
    $$
    Win Fraction =\frac{1}{1+10^{\frac{\Delta(\text { Elo Score })}{400}}} \\ 
    \Delta (Elo Score ) \approx 174 * \Delta( PM Score )
    $$
    

## 偏好模型











